[
  {
    "objectID": "patterns.html",
    "href": "patterns.html",
    "title": "Design Patterns",
    "section": "",
    "text": "class Nanobrick(Protocol):\n    async def invoke(self, input: T) -&gt; U: ...\n    def batch(self, inputs: List[T]) -&gt; List[U]: ...\n    async def stream(self, input: T) -&gt; AsyncIterator[U]: ...\nWhy it works: Consistent interface across all components enables seamless composition.\n\n\n\n# Intuitive chaining\npipeline = validator | transformer | storage\n\n# Equivalent to\npipeline = Pipeline([validator, transformer, storage])\nWhy it works: Familiar syntax from Unix pipes and R’s magrittr.\n\n\n\n\n\n\nclass ValidatorData(Nanobrick[dict, dict, ValidationRules]):\n    async def invoke(self, input: dict, *, deps: ValidationRules) -&gt; dict:\n        # Use injected dependencies\n        return deps.validate(input)\nWhy it works: Testable, flexible, and explicit dependencies.\n\n\n\nT = TypeVar('T')\nU = TypeVar('U')\n\nclass Transformer(Nanobrick[T, U]):\n    # Type-safe transformations\n    pass\nWhy it works: Catches errors at development time.",
    "crumbs": [
      "Home",
      "Building Systems",
      "Design Patterns"
    ]
  },
  {
    "objectID": "patterns.html#core-patterns-from-successful-frameworks",
    "href": "patterns.html#core-patterns-from-successful-frameworks",
    "title": "Design Patterns",
    "section": "",
    "text": "class Nanobrick(Protocol):\n    async def invoke(self, input: T) -&gt; U: ...\n    def batch(self, inputs: List[T]) -&gt; List[U]: ...\n    async def stream(self, input: T) -&gt; AsyncIterator[U]: ...\nWhy it works: Consistent interface across all components enables seamless composition.\n\n\n\n# Intuitive chaining\npipeline = validator | transformer | storage\n\n# Equivalent to\npipeline = Pipeline([validator, transformer, storage])\nWhy it works: Familiar syntax from Unix pipes and R’s magrittr.\n\n\n\n\n\n\nclass ValidatorData(Nanobrick[dict, dict, ValidationRules]):\n    async def invoke(self, input: dict, *, deps: ValidationRules) -&gt; dict:\n        # Use injected dependencies\n        return deps.validate(input)\nWhy it works: Testable, flexible, and explicit dependencies.\n\n\n\nT = TypeVar('T')\nU = TypeVar('U')\n\nclass Transformer(Nanobrick[T, U]):\n    # Type-safe transformations\n    pass\nWhy it works: Catches errors at development time.",
    "crumbs": [
      "Home",
      "Building Systems",
      "Design Patterns"
    ]
  },
  {
    "objectID": "patterns.html#nanobricks-specific-patterns",
    "href": "patterns.html#nanobricks-specific-patterns",
    "title": "Design Patterns",
    "section": "Nanobricks-Specific Patterns",
    "text": "Nanobricks-Specific Patterns\n\n5. Skill Pattern\n@nanobrick\nclass BasicValidator:\n    def invoke(self, data): ...\n\n# Add capabilities without modifying core\nenhanced = BasicValidator().with_skill(SkillAPI())\nWhy it works: Separation of concerns, optional complexity.\n\n\n6. Dual Nature Pattern\n# Use as library\nresult = validator.invoke(data)\n\n# Use as service\napp = validator.as_api()\ncli = validator.as_cli()\nWhy it works: Same logic, multiple interfaces.\n\n\n7. Progressive Enhancement\n# Start simple\nclass ValidatorEmail(Nanobrick):\n    def invoke(self, email: str) -&gt; bool:\n        return \"@\" in email\n\n# Enhance later\nclass ValidatorEmailSmart(ValidatorEmail):\n    def __init__(self):\n        self.add_skill(SkillAI())\n    \n    async def invoke(self, email: str) -&gt; bool:\n        # Basic check\n        if not super().invoke(email):\n            return False\n        \n        # AI enhancement\n        if self.has_ai:\n            return await self.ai_verify(email)\n        \n        return True\nWhy it works: Start simple, grow as needed.",
    "crumbs": [
      "Home",
      "Building Systems",
      "Design Patterns"
    ]
  },
  {
    "objectID": "patterns.html#composition-patterns",
    "href": "patterns.html#composition-patterns",
    "title": "Design Patterns",
    "section": "Composition Patterns",
    "text": "Composition Patterns\n\nSequential Composition\n# Data flows through each step\npipeline = input | step1 | step2 | output\n\n\nParallel Composition\n# Execute concurrently, combine results\nresult = await (validator & enricher & scorer).invoke(data)\n\n\nConditional Composition\n# Branch based on conditions\npipeline = input | validator | (transformer if condition else passthrough) | output\n\n\nRecursive Composition\n# Nanobricks containing nanobricks\nclass Workflow(Nanobrick):\n    def __init__(self, steps: List[Nanobrick]):\n        self.steps = steps\n    \n    async def invoke(self, input):\n        result = input\n        for step in self.steps:\n            result = await step.invoke(result)\n        return result",
    "crumbs": [
      "Home",
      "Building Systems",
      "Design Patterns"
    ]
  },
  {
    "objectID": "patterns.html#anti-patterns-to-avoid",
    "href": "patterns.html#anti-patterns-to-avoid",
    "title": "Design Patterns",
    "section": "Anti-Patterns to Avoid",
    "text": "Anti-Patterns to Avoid\n\n1. Deep Inheritance\n❌ Don’t do this:\nclass ValidatorBase(Nanobrick):\n    class ValidatorString(ValidatorBase):\n        class ValidatorEmail(ValidatorString):\n            class ValidatorEmailCorporate(ValidatorEmail):\n                # Too deep!\n✅ Do this instead:\n# Compose behaviors\nvalidator = StringValidator() | EmailPattern() | CorporateDomain()\n\n\n2. Hidden State\n❌ Don’t do this:\nclass Counter(Nanobrick):\n    def __init__(self):\n        self._count = 0  # Hidden mutable state\n    \n    def invoke(self, input):\n        self._count += 1  # Side effect!\n        return input\n✅ Do this instead:\nclass Counter(StatefulNanobrick):\n    # Explicit state management\n    state: CounterState\n\n\n3. Tight Coupling\n❌ Don’t do this:\nclass ProcessorData(Nanobrick):\n    def invoke(self, input):\n        # Directly calling another service\n        db_result = DatabaseService.query(input)\n        api_result = ExternalAPI.fetch(input)\n✅ Do this instead:\nclass ProcessorData(Nanobrick[Input, Output, Dependencies]):\n    async def invoke(self, input: Input, *, deps: Dependencies):\n        # Use injected dependencies\n        db_result = await deps.database.query(input)\n        api_result = await deps.api.fetch(input)",
    "crumbs": [
      "Home",
      "Building Systems",
      "Design Patterns"
    ]
  },
  {
    "objectID": "patterns.html#best-practices",
    "href": "patterns.html#best-practices",
    "title": "Design Patterns",
    "section": "Best Practices",
    "text": "Best Practices\n\nKeep It Simple: Start with the minimal interface\nExplicit Over Implicit: Clear data flow and dependencies\nComposition Over Configuration: Build complexity through combination\nType Safety: Use generics and type hints\nFail Fast: Validate at boundaries\nDocument Intent: Clear docstrings and examples",
    "crumbs": [
      "Home",
      "Building Systems",
      "Design Patterns"
    ]
  },
  {
    "objectID": "type-safety.html",
    "href": "type-safety.html",
    "title": "Type Safety in Nanobricks",
    "section": "",
    "text": "When composing nanobricks with the pipe operator, we need to ensure type safety:\nvalidator: Nanobrick[dict, dict]\ntransformer: Nanobrick[dict, list]\nstorage: Nanobrick[list, None]\n\n# This should type-check correctly\npipeline = validator | transformer | storage",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Type Safety in Nanobricks"
    ]
  },
  {
    "objectID": "type-safety.html#the-type-composition-challenge",
    "href": "type-safety.html#the-type-composition-challenge",
    "title": "Type Safety in Nanobricks",
    "section": "",
    "text": "When composing nanobricks with the pipe operator, we need to ensure type safety:\nvalidator: Nanobrick[dict, dict]\ntransformer: Nanobrick[dict, list]\nstorage: Nanobrick[list, None]\n\n# This should type-check correctly\npipeline = validator | transformer | storage",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Type Safety in Nanobricks"
    ]
  },
  {
    "objectID": "type-safety.html#solutions",
    "href": "type-safety.html#solutions",
    "title": "Type Safety in Nanobricks",
    "section": "Solutions",
    "text": "Solutions\n\n1. Beartype (Runtime Type Checking)\nBeartype offers O(1) runtime type checking:\nfrom beartype import beartype\n\n@beartype\nclass ValidatorData(Nanobrick[dict, dict]):\n    async def invoke(self, input: dict) -&gt; dict:\n        # Beartype ensures types at runtime\n        return validated_data\nAdvantages: - Fast runtime validation - Works with existing type hints - Catches errors static analysis might miss\n\n\n2. Mypy (Static Type Checking)\nTraditional static analysis with mypy plugins:\n# mypy will check this at analysis time\npipeline: Nanobrick[dict, None] = validator | transformer | storage\n\n\n3. Hybrid Approach (Recommended)\nUse both static and runtime checking:\nfrom typing import TypeVar, Generic\nfrom beartype import beartype\n\nT1 = TypeVar('T1')\nT2 = TypeVar('T2')\nT3 = TypeVar('T3')\n\n@beartype\nclass Pipeline(Generic[T1, T2]):\n    \"\"\"Type-safe pipeline composition\"\"\"\n    \n    def __or__(self, other: Nanobrick[T2, T3]) -&gt; Pipeline[T1, T3]:\n        # Type-safe composition\n        return Pipeline(self, other)",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Type Safety in Nanobricks"
    ]
  },
  {
    "objectID": "type-safety.html#implementation-strategy",
    "href": "type-safety.html#implementation-strategy",
    "title": "Type Safety in Nanobricks",
    "section": "Implementation Strategy",
    "text": "Implementation Strategy\n\nPhase 1: Basic Type Safety\n\nUse standard Python generics\nRely on mypy for static checking\nDocument type constraints\n\n\n\nPhase 2: Enhanced Runtime Checking\n\nIntegrate beartype decorators\nAdd composition validators\nCreate type-safe operators\n\n\n\nPhase 3: Advanced Patterns\n\nCustom mypy plugins\nPipeline type inference\nAutomatic type adaptation",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Type Safety in Nanobricks"
    ]
  },
  {
    "objectID": "type-safety.html#alternative-libraries",
    "href": "type-safety.html#alternative-libraries",
    "title": "Type Safety in Nanobricks",
    "section": "Alternative Libraries",
    "text": "Alternative Libraries\n\nPipeFunc\nPipeFunc provides type-safe pipelines: - Automatic dependency resolution - Type checking built-in - Visualization support\n\n\nPipe Package\nSimple pipe operations with partial type support:\nfrom pipe import select, where, take\n\nresult = data | select(lambda x: x.value) | where(lambda x: x &gt; 0) | take(5)",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Type Safety in Nanobricks"
    ]
  },
  {
    "objectID": "type-safety.html#best-practices",
    "href": "type-safety.html#best-practices",
    "title": "Type Safety in Nanobricks",
    "section": "Best Practices",
    "text": "Best Practices\n\nExplicit Type Annotations\nclass MyBrick(Nanobrick[InputType, OutputType]):\n    ...\nType Guards at Boundaries\ndef compose_safely(a: Nanobrick[A, B], b: Nanobrick[B, C]) -&gt; Nanobrick[A, C]:\n    # Validate type compatibility\n    ...\nProgressive Enhancement\n\nStart with basic typing\nAdd runtime checks where needed\nOptimize critical paths",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Type Safety in Nanobricks"
    ]
  },
  {
    "objectID": "type-safety.html#open-questions",
    "href": "type-safety.html#open-questions",
    "title": "Type Safety in Nanobricks",
    "section": "Open Questions",
    "text": "Open Questions\n\nShould we enforce strict type checking or allow dynamic typing?\nHow do we handle union types in pipelines?\nWhat about optional/nullable types in compositions?",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Type Safety in Nanobricks"
    ]
  },
  {
    "objectID": "type-safety.html#resources",
    "href": "type-safety.html#resources",
    "title": "Type Safety in Nanobricks",
    "section": "Resources",
    "text": "Resources\n\nBeartype Documentation\nMypy Documentation\nPython Typing PEPs",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Type Safety in Nanobricks"
    ]
  },
  {
    "objectID": "quickstart.html",
    "href": "quickstart.html",
    "title": "Quickstart Guide",
    "section": "",
    "text": "pip install nanobricks\nOr with development dependencies:\npip install \"nanobricks[dev]\"",
    "crumbs": [
      "Home",
      "Getting Started",
      "Quickstart Guide"
    ]
  },
  {
    "objectID": "quickstart.html#installation",
    "href": "quickstart.html#installation",
    "title": "Quickstart Guide",
    "section": "",
    "text": "pip install nanobricks\nOr with development dependencies:\npip install \"nanobricks[dev]\"",
    "crumbs": [
      "Home",
      "Getting Started",
      "Quickstart Guide"
    ]
  },
  {
    "objectID": "quickstart.html#your-first-nanobrick",
    "href": "quickstart.html#your-first-nanobrick",
    "title": "Quickstart Guide",
    "section": "Your First Nanobrick",
    "text": "Your First Nanobrick\nEvery nanobrick is a self-contained unit of functionality:\n\n# Note: This example uses async/await syntax. \n# In a Jupyter notebook, you can run this directly with 'await'.\n# In a Python script, wrap in asyncio.run() or use invoke_sync().\n\nfrom nanobricks import NanobrickSimple\n\nclass GreeterBrick(NanobrickSimple[str, str]):\n    \"\"\"Says hello to someone.\"\"\"\n    \n    async def invoke(self, name: str, *, deps=None) -&gt; str:\n        return f\"Hello, {name}!\"\n\n# Use it\ngreeter = GreeterBrick()\n\n# In Jupyter/async context:\nmessage = await greeter.invoke(\"World\")\n\n# In regular Python script:\n# import asyncio\n# message = asyncio.run(greeter.invoke(\"World\"))\n# OR\n# message = greeter.invoke_sync(\"World\")\n\nprint(message)  # \"Hello, World!\"",
    "crumbs": [
      "Home",
      "Getting Started",
      "Quickstart Guide"
    ]
  },
  {
    "objectID": "quickstart.html#composing-nanobricks",
    "href": "quickstart.html#composing-nanobricks",
    "title": "Quickstart Guide",
    "section": "Composing Nanobricks",
    "text": "Composing Nanobricks\nThe real power comes from composition using the pipe operator:\n\nfrom nanobricks import NanobrickSimple\n\nclass UppercaseBrick(NanobrickSimple[str, str]):\n    async def invoke(self, text: str, *, deps=None) -&gt; str:\n        return text.upper()\n\nclass ExclamationBrick(NanobrickSimple[str, str]):\n    async def invoke(self, text: str, *, deps=None) -&gt; str:\n        return f\"{text}!!!\"\n\n# Compose them!\npipeline = GreeterBrick() | UppercaseBrick() | ExclamationBrick()\n\n# Produces: \"HELLO, ALICE!!!\"\nresult = await pipeline.invoke(\"Alice\")",
    "crumbs": [
      "Home",
      "Getting Started",
      "Quickstart Guide"
    ]
  },
  {
    "objectID": "quickstart.html#adding-skills",
    "href": "quickstart.html#adding-skills",
    "title": "Quickstart Guide",
    "section": "Adding Skills",
    "text": "Adding Skills\nSkills add capabilities without changing your brick’s core logic:\n\n# Add logging\nlogged_greeter = greeter.with_skill(\"logging\")\n\n# Add API endpoint\napi_greeter = greeter.with_skill(\"api\", port=8000)\n\n# Add CLI interface\ncli_greeter = greeter.with_skill(\"cli\")\n\n# Chain multiple skills\nproduction_greeter = (\n    greeter\n    .with_skill(\"logging\")\n    .with_skill(\"api\")\n    .with_skill(\"observability\")\n)",
    "crumbs": [
      "Home",
      "Getting Started",
      "Quickstart Guide"
    ]
  },
  {
    "objectID": "quickstart.html#building-real-applications",
    "href": "quickstart.html#building-real-applications",
    "title": "Quickstart Guide",
    "section": "Building Real Applications",
    "text": "Building Real Applications\nHere’s a complete data validation service in under 20 lines:\n\nfrom nanobricks import NanobrickSimple, skill\nfrom nanobricks.validators import EmailValidator, TypeValidator\n\n@skill(\"api\", port=8080)\n@skill(\"logging\")\nclass UserValidationService(NanobrickSimple[dict, dict]):\n    \"\"\"Validates user registration data.\"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        self.pipeline = (\n            TypeValidator({\"email\": str, \"age\": int})\n            | EmailValidator(field=\"email\")\n            | self.enrich_user\n        )\n    \n    async def invoke(self, user_data: dict, *, deps=None) -&gt; dict:\n        validated = await self.pipeline.invoke(user_data)\n        return {\"valid\": True, \"user\": validated}\n    \n    async def enrich_user(self, data: dict, *, deps=None) -&gt; dict:\n        data[\"created_at\"] = \"2024-01-01\"\n        return data\n\n# Run it!\nservice = UserValidationService()\nservice.start_server()  # Now available at http://localhost:8080",
    "crumbs": [
      "Home",
      "Getting Started",
      "Quickstart Guide"
    ]
  },
  {
    "objectID": "quickstart.html#whats-next",
    "href": "quickstart.html#whats-next",
    "title": "Quickstart Guide",
    "section": "What’s Next?",
    "text": "What’s Next?\n\nLearn the Concepts\n\nTutorial - Step-by-step guide\nCore Concepts - Understand the architecture\n\n\n\nBuild Systems\n\nSDK Guide - Build larger applications\nPatterns - Best practices and design patterns\n\n\n\nDeploy to Production\n\nProduction Guide - Deploy and scale\nAPI Reference - Complete reference",
    "crumbs": [
      "Home",
      "Getting Started",
      "Quickstart Guide"
    ]
  },
  {
    "objectID": "quickstart.html#quick-examples",
    "href": "quickstart.html#quick-examples",
    "title": "Quickstart Guide",
    "section": "Quick Examples",
    "text": "Quick Examples\n\nData Pipeline\n\nfrom nanobricks.transformers import CSVParser, TypeConverter\nfrom nanobricks.validators import SchemaValidator\n\netl_pipeline = (\n    CSVParser()\n    | SchemaValidator(schema)\n    | TypeConverter(types={\"age\": int, \"salary\": float})\n    | DatabaseWriter()\n).with_skill(\"retry\", max_attempts=3)\n\n\n\nREST API Gateway\n\nfrom nanobricks.skills import create_api_gateway\n\napi = create_api_gateway(\n    routes={\n        \"/users\": UserService(),\n        \"/orders\": OrderService(),\n        \"/products\": ProductService(),\n    },\n    middleware=[\n        RateLimiter(100),\n        Authenticator(),\n        RequestLogger(),\n    ]\n)\n\n\n\nMicroservice\n\nfrom nanobricks import create_microservice\n\nservice = create_microservice(\n    name=\"payment-processor\",\n    bricks=[\n        ValidatePayment(),\n        CheckFraud(),\n        ProcessPayment(),\n        SendReceipt(),\n    ],\n    skills=[\"api\", \"cli\", \"docker\", \"kubernetes\", \"monitoring\"]\n)",
    "crumbs": [
      "Home",
      "Getting Started",
      "Quickstart Guide"
    ]
  },
  {
    "objectID": "quickstart.html#join-the-community",
    "href": "quickstart.html#join-the-community",
    "title": "Quickstart Guide",
    "section": "Join the Community",
    "text": "Join the Community\n\nGitHub\nDiscord\nExamples Repository\n\n\nReady to build? Check out the Tutorial for a deeper dive!",
    "crumbs": [
      "Home",
      "Getting Started",
      "Quickstart Guide"
    ]
  },
  {
    "objectID": "sdk-guide.html",
    "href": "sdk-guide.html",
    "title": "Nanobricks SDK Guide",
    "section": "",
    "text": "This guide shows how to use Nanobricks as an SDK for building larger, production-ready Python systems. We’ll cover architecture patterns, real-world examples, and best practices.",
    "crumbs": [
      "Home",
      "Building Systems",
      "Nanobricks SDK Guide"
    ]
  },
  {
    "objectID": "sdk-guide.html#introduction",
    "href": "sdk-guide.html#introduction",
    "title": "Nanobricks SDK Guide",
    "section": "",
    "text": "This guide shows how to use Nanobricks as an SDK for building larger, production-ready Python systems. We’ll cover architecture patterns, real-world examples, and best practices.",
    "crumbs": [
      "Home",
      "Building Systems",
      "Nanobricks SDK Guide"
    ]
  },
  {
    "objectID": "sdk-guide.html#why-nanobricks-as-an-sdk",
    "href": "sdk-guide.html#why-nanobricks-as-an-sdk",
    "title": "Nanobricks SDK Guide",
    "section": "Why Nanobricks as an SDK?",
    "text": "Why Nanobricks as an SDK?\nTraditional approaches to building Python systems often lead to: - Monolithic code that’s hard to test and maintain - Tight coupling between components - Difficulty adding features without breaking existing code - Complex deployment and scaling challenges\nNanobricks solves these problems by providing: - Atomic Components: Self-contained units that do one thing well - Standardized Interfaces: Every component follows the same pattern - Composition Over Inheritance: Build complex systems from simple parts - Progressive Enhancement: Add capabilities as needed through skills - Production-Ready: Built-in support for monitoring, deployment, and scaling",
    "crumbs": [
      "Home",
      "Building Systems",
      "Nanobricks SDK Guide"
    ]
  },
  {
    "objectID": "sdk-guide.html#architecture-patterns",
    "href": "sdk-guide.html#architecture-patterns",
    "title": "Nanobricks SDK Guide",
    "section": "Architecture Patterns",
    "text": "Architecture Patterns\n\nRepository Pattern\nBuild a complete data access layer:\n\nfrom nanobricks import NanobrickBase, NanobrickSimple, Pipeline\nfrom typing import Optional, List, TypedDict\n\nclass DatabaseDeps(TypedDict):\n    db_connection: DatabaseConnection\n    cache: Optional[CacheClient]\n\nclass UserRepository:\n    \"\"\"Repository for user data access.\"\"\"\n    \n    def __init__(self, deps: DatabaseDeps):\n        self.deps = deps\n        \n        # Build pipelines for each operation\n        self.get_user = Pipeline(\n            ValidateUserId(),\n            BuildSelectQuery(),\n            ExecuteQuery(deps),\n            MapToUser(),\n            CacheResult(deps)\n        )\n        \n        self.create_user = Pipeline(\n            ValidateUserData(),\n            SanitizeInput(),\n            BuildInsertQuery(),\n            ExecuteQuery(deps),\n            MapToUser()\n        )\n        \n        self.update_user = Pipeline(\n            ValidateUserId(),\n            ValidateUserData(),\n            BuildUpdateQuery(),\n            ExecuteQuery(deps),\n            InvalidateCache(deps),\n            MapToUser()\n        )\n    \n    async def get(self, user_id: str) -&gt; User:\n        return await self.get_user.invoke(user_id)\n    \n    async def create(self, user_data: dict) -&gt; User:\n        return await self.create_user.invoke(user_data)\n    \n    async def update(self, user_id: str, user_data: dict) -&gt; User:\n        return await self.update_user.invoke({\"id\": user_id, **user_data})\n\n# Supporting bricks\nclass ValidateUserId(NanobrickSimple[str, str]):\n    async def invoke(self, user_id: str, *, deps=None) -&gt; str:\n        if not user_id or not user_id.isalnum():\n            raise ValueError(\"Invalid user ID\")\n        return user_id\n\nclass BuildSelectQuery(NanobrickSimple[str, dict]):\n    async def invoke(self, user_id: str, *, deps=None) -&gt; dict:\n        return {\n            \"query\": \"SELECT * FROM users WHERE id = ?\",\n            \"params\": [user_id]\n        }\n\nclass ExecuteQuery(NanobrickBase[dict, dict, DatabaseDeps]):\n    def __init__(self, deps: DatabaseDeps):\n        super().__init__()\n        self.deps = deps\n    \n    async def invoke(self, query_info: dict, *, deps=None) -&gt; dict:\n        result = await self.deps[\"db_connection\"].execute(\n            query_info[\"query\"],\n            query_info[\"params\"]\n        )\n        return result\n\nclass MapToUser(NanobrickSimple[dict, User]):\n    async def invoke(self, row: dict, *, deps=None) -&gt; User:\n        return User(**row)\n\n\n\nService Layer Pattern\nBuild business logic as composable services:\n\nfrom nanobricks import create_service, ServiceDeps\n\nclass OrderService:\n    \"\"\"Order processing service with complete business logic.\"\"\"\n    \n    def __init__(self):\n        # Core processing pipeline\n        self.process_order = (\n            ValidateOrder()\n            | CheckInventory()\n            | CalculatePricing()\n            | ApplyDiscounts()\n            | ProcessPayment()\n            | UpdateInventory()\n            | CreateShipment()\n            | SendConfirmation()\n        ).with_skill(\"logging\").with_skill(\"observability\")\n        \n        # Add retry for payment processing\n        self.process_payment_safe = (\n            ProcessPayment()\n            .with_skill(\"retry\", max_attempts=3)\n            .with_skill(\"circuit_breaker\")\n        )\n    \n    async def create_order(self, order_data: dict, deps: ServiceDeps) -&gt; dict:\n        try:\n            # Run the pipeline\n            result = await self.process_order.invoke(order_data, deps=deps)\n            \n            # Log success metric\n            if deps.get(\"metrics\"):\n                await deps[\"metrics\"].increment(\"orders.created\")\n            \n            return result\n            \n        except Exception as e:\n            # Log failure metric\n            if deps.get(\"metrics\"):\n                await deps[\"metrics\"].increment(\"orders.failed\")\n            raise\n\n# Make it an API service\norder_api = create_api_service(\n    OrderService(),\n    routes=[\n        (\"POST\", \"/orders\", \"create_order\"),\n        (\"GET\", \"/orders/{id}\", \"get_order\"),\n        (\"PUT\", \"/orders/{id}\", \"update_order\"),\n        (\"DELETE\", \"/orders/{id}\", \"cancel_order\")\n    ],\n    middleware=[\n        RateLimiter(requests_per_minute=100),\n        Authenticator(),\n        RequestValidator(),\n        ResponseTransformer()\n    ]\n)\n\n\n\nEvent-Driven Architecture\nBuild event-driven systems with nanobricks:\n\nfrom nanobricks.patterns import EventBus, EventHandler\n\n# Define event handlers as nanobricks\nclass OrderCreatedHandler(NanobrickSimple[OrderEvent, None]):\n    async def invoke(self, event: OrderEvent, *, deps=None) -&gt; None:\n        # Send email\n        await EmailService().send_order_confirmation(event.order)\n\nclass InventoryUpdateHandler(NanobrickSimple[OrderEvent, None]):\n    async def invoke(self, event: OrderEvent, *, deps=None) -&gt; None:\n        # Update inventory\n        for item in event.order.items:\n            await InventoryService().decrement(item.sku, item.quantity)\n\nclass AnalyticsHandler(NanobrickSimple[OrderEvent, None]):\n    async def invoke(self, event: OrderEvent, *, deps=None) -&gt; None:\n        # Track analytics\n        await Analytics().track(\"order_created\", {\n            \"order_id\": event.order.id,\n            \"total\": event.order.total,\n            \"items\": len(event.order.items)\n        })\n\n# Create event bus\nevent_bus = EventBus()\n\n# Register handlers\nevent_bus.subscribe(\"order.created\", OrderCreatedHandler())\nevent_bus.subscribe(\"order.created\", InventoryUpdateHandler())\nevent_bus.subscribe(\"order.created\", AnalyticsHandler())\n\n# Emit events in your service\nclass OrderService:\n    def __init__(self, event_bus: EventBus):\n        self.event_bus = event_bus\n    \n    async def create_order(self, order_data: dict) -&gt; Order:\n        # Create order\n        order = await self.process_order.invoke(order_data)\n        \n        # Emit event\n        await self.event_bus.emit(\"order.created\", OrderEvent(order))\n        \n        return order\n\n\n\nCQRS Pattern\nSeparate commands and queries:\n\nfrom nanobricks import Command, Query\n\n# Commands modify state\nclass CreateUserCommand(Command[dict, str]):\n    \"\"\"Creates a new user and returns their ID.\"\"\"\n    \n    def __init__(self):\n        self.pipeline = (\n            ValidateUserData()\n            | CheckEmailUniqueness()\n            | HashPassword()\n            | CreateUser()\n            | EmitUserCreatedEvent()\n        )\n    \n    async def execute(self, user_data: dict, *, deps=None) -&gt; str:\n        result = await self.pipeline.invoke(user_data, deps=deps)\n        return result[\"user_id\"]\n\n# Queries read state\nclass GetUserQuery(Query[str, UserDTO]):\n    \"\"\"Retrieves user information.\"\"\"\n    \n    def __init__(self):\n        self.pipeline = (\n            ValidateUserId()\n            | LoadFromCache()\n            | LoadFromDatabase()\n            | MapToDTO()\n        )\n    \n    async def execute(self, user_id: str, *, deps=None) -&gt; UserDTO:\n        return await self.pipeline.invoke(user_id, deps=deps)\n\n# CQRS service\nclass UserCQRS:\n    def __init__(self):\n        # Commands\n        self.create_user = CreateUserCommand()\n        self.update_user = UpdateUserCommand()\n        self.delete_user = DeleteUserCommand()\n        \n        # Queries\n        self.get_user = GetUserQuery()\n        self.list_users = ListUsersQuery()\n        self.search_users = SearchUsersQuery()",
    "crumbs": [
      "Home",
      "Building Systems",
      "Nanobricks SDK Guide"
    ]
  },
  {
    "objectID": "sdk-guide.html#real-world-examples",
    "href": "sdk-guide.html#real-world-examples",
    "title": "Nanobricks SDK Guide",
    "section": "Real-World Examples",
    "text": "Real-World Examples\n\nBuilding a REST API\nComplete REST API with authentication, rate limiting, and monitoring:\n\nfrom nanobricks import create_rest_api\nfrom nanobricks.skills import skill\n\n# Define your API endpoints as nanobricks\n@skill(\"logging\")\n@skill(\"observability\")\nclass UserEndpoints:\n    \"\"\"User management endpoints.\"\"\"\n    \n    async def get_user(self, request: Request) -&gt; Response:\n        user_id = request.path_params[\"id\"]\n        \n        pipeline = (\n            ExtractUserId()\n            | ValidatePermissions()\n            | LoadUser()\n            | SerializeUser()\n        )\n        \n        user = await pipeline.invoke(user_id, deps=request.deps)\n        return Response(user, status=200)\n    \n    async def create_user(self, request: Request) -&gt; Response:\n        pipeline = (\n            ParseJSON()\n            | ValidateUserData()\n            | CheckUniqueness()\n            | CreateUser()\n            | SendWelcomeEmail()\n            | SerializeUser()\n        )\n        \n        user = await pipeline.invoke(request.body, deps=request.deps)\n        return Response(user, status=201)\n\n# Create the API\napi = create_rest_api(\n    title=\"User Management API\",\n    version=\"1.0.0\",\n    endpoints=[\n        UserEndpoints(),\n        OrderEndpoints(),\n        ProductEndpoints()\n    ],\n    middleware=[\n        CORSMiddleware(origins=[\"https://myapp.com\"]),\n        RateLimitMiddleware(requests_per_minute=100),\n        AuthenticationMiddleware(jwt_secret=SECRET),\n        RequestLoggingMiddleware(),\n        MetricsMiddleware()\n    ],\n    error_handlers={\n        ValidationError: validation_error_handler,\n        AuthenticationError: auth_error_handler,\n        Exception: generic_error_handler\n    }\n)\n\n# Run it\nif __name__ == \"__main__\":\n    api.run(host=\"0.0.0.0\", port=8000)\n\n\n\nBuilding a Data Pipeline\nETL pipeline for data processing:\n\nfrom nanobricks import create_data_pipeline\nfrom nanobricks.transformers import *\nfrom nanobricks.validators import *\n\n# Define your data pipeline\netl_pipeline = create_data_pipeline(\n    name=\"sales_etl\",\n    \n    # Extract\n    source=S3Source(\n        bucket=\"raw-data\",\n        prefix=\"sales/\",\n        format=\"csv\"\n    ),\n    \n    # Transform\n    transformers=[\n        # Parse and validate\n        CSVParser(delimiter=\",\", encoding=\"utf-8\"),\n        SchemaValidator(schema={\n            \"date\": \"datetime\",\n            \"product_id\": \"string\",\n            \"quantity\": \"integer\",\n            \"price\": \"decimal\",\n            \"customer_id\": \"string\"\n        }),\n        \n        # Clean and enrich\n        DataCleaner(\n            remove_nulls=True,\n            trim_strings=True,\n            standardize_dates=True\n        ),\n        ProductEnricher(product_api_url=PRODUCT_API),\n        CustomerEnricher(customer_db=customer_repo),\n        \n        # Aggregate\n        GroupBy(keys=[\"date\", \"product_id\"]),\n        Aggregator({\n            \"total_quantity\": \"sum(quantity)\",\n            \"total_revenue\": \"sum(quantity * price)\",\n            \"unique_customers\": \"count_distinct(customer_id)\"\n        }),\n        \n        # Add derived metrics\n        MetricsCalculator([\n            \"avg_order_value = total_revenue / unique_customers\",\n            \"units_per_customer = total_quantity / unique_customers\"\n        ])\n    ],\n    \n    # Load\n    destination=WarehouseDestination(\n        connection_string=WAREHOUSE_URL,\n        table=\"sales_summary\",\n        mode=\"append\"\n    ),\n    \n    # Error handling\n    error_handler=ErrorHandler(\n        on_error=\"continue\",  # or \"fail\", \"retry\"\n        max_errors=100,\n        error_output=S3Source(bucket=\"errors\", prefix=\"sales/\")\n    ),\n    \n    # Monitoring\n    monitors=[\n        DataQualityMonitor(\n            checks=[\n                \"total_revenue &gt; 0\",\n                \"unique_customers &gt; 0\",\n                \"date is not null\"\n            ]\n        ),\n        PerformanceMonitor(\n            alert_on_duration_seconds=300,\n            alert_on_memory_gb=4\n        )\n    ]\n)\n\n# Run with scheduling\nscheduler = PipelineScheduler(\n    pipeline=etl_pipeline,\n    schedule=\"0 2 * * *\",  # Daily at 2 AM\n    retry_policy=RetryPolicy(max_attempts=3, backoff=\"exponential\"),\n    notifications=[\n        EmailNotification(on=[\"failure\", \"success\"]),\n        SlackNotification(on=[\"failure\"])\n    ]\n)\n\nscheduler.start()\n\n\n\nBuilding a Microservice\nComplete microservice with all production features:\n\nfrom nanobricks import Microservice, create_microservice\n\n# Define your microservice\npayment_service = create_microservice(\n    name=\"payment-processor\",\n    version=\"2.1.0\",\n    \n    # Core business logic as nanobricks\n    handlers={\n        # API endpoints\n        \"POST /payments\": ProcessPayment(),\n        \"GET /payments/{id}\": GetPayment(),\n        \"POST /payments/{id}/refund\": RefundPayment(),\n        \n        # Message queue handlers\n        \"payment.requested\": PaymentRequestHandler(),\n        \"payment.cancelled\": PaymentCancelHandler(),\n        \n        # Scheduled jobs\n        \"*/5 * * * *\": ReconciliationJob(),  # Every 5 minutes\n        \"0 0 * * *\": DailyReportJob()       # Daily at midnight\n    },\n    \n    # Dependencies\n    dependencies={\n        \"payment_gateway\": StripeGateway(api_key=STRIPE_KEY),\n        \"database\": PostgresConnection(DATABASE_URL),\n        \"cache\": RedisCache(REDIS_URL),\n        \"message_queue\": RabbitMQ(RABBITMQ_URL)\n    },\n    \n    # Skills for production\n    skills=[\n        (\"logging\", {\"level\": \"INFO\", \"format\": \"json\"}),\n        (\"api\", {\"port\": 8080, \"docs\": True}),\n        (\"observability\", {\"traces\": True, \"metrics\": True}),\n        (\"docker\", {\"base_image\": \"python:3.13-slim\"}),\n        (\"kubernetes\", {\"replicas\": 3, \"autoscale\": True})\n    ],\n    \n    # Configuration\n    config={\n        \"rate_limits\": {\n            \"default\": 100,  # requests per minute\n            \"payment_processing\": 10\n        },\n        \"circuit_breaker\": {\n            \"failure_threshold\": 5,\n            \"timeout_seconds\": 60\n        },\n        \"retry\": {\n            \"max_attempts\": 3,\n            \"backoff\": \"exponential\"\n        }\n    }\n)\n\n# Deploy it\nif __name__ == \"__main__\":\n    # Local development\n    payment_service.run_local()\n    \n    # Or generate deployment artifacts\n    payment_service.generate_docker_image()\n    payment_service.generate_kubernetes_manifests()\n    payment_service.generate_helm_chart()",
    "crumbs": [
      "Home",
      "Building Systems",
      "Nanobricks SDK Guide"
    ]
  },
  {
    "objectID": "sdk-guide.html#best-practices",
    "href": "sdk-guide.html#best-practices",
    "title": "Nanobricks SDK Guide",
    "section": "Best Practices",
    "text": "Best Practices\n\n1. Keep Bricks Atomic\nEach brick should do ONE thing:\n\n# Good: Single responsibility\nclass ValidateEmail(NanobrickSimple[str, str]):\n    async def invoke(self, email: str, *, deps=None) -&gt; str:\n        if \"@\" not in email:\n            raise ValueError(\"Invalid email\")\n        return email\n\n# Bad: Multiple responsibilities\nclass ProcessUser(NanobrickSimple[dict, dict]):\n    async def invoke(self, user: dict, *, deps=None) -&gt; dict:\n        # Validates email AND hashes password AND saves to DB\n        # Too many responsibilities!\n        pass\n\n\n\n2. Use Type Hints\nAlways specify types for better IDE support and safety:\n\nfrom typing import List, Dict, Optional\n\nclass DataProcessor(NanobrickBase[List[Dict], ProcessedData, AppDeps]):\n    \"\"\"Process raw data into structured format.\"\"\"\n    \n    async def invoke(\n        self, \n        raw_data: List[Dict], \n        *, \n        deps: Optional[AppDeps] = None\n    ) -&gt; ProcessedData:\n        # Type hints enable IDE autocomplete and type checking\n        pass\n\n\n\n3. Compose, Don’t Inherit\nBuild functionality through composition:\n\n# Good: Composition\nuser_processor = (\n    ValidateUser()\n    | EnrichUser()\n    | SaveUser()\n).with_skill(\"logging\")\n\n# Bad: Deep inheritance\nclass ValidatingEnrichingSavingUserProcessor(\n    ValidatingProcessor,\n    EnrichingProcessor,\n    SavingProcessor\n):\n    # Complex inheritance hierarchy\n    pass\n\n\n\n4. Handle Errors Gracefully\nUse appropriate error handling patterns:\n\n# Use fallbacks for resilience\nsafe_api_call = Fallback(\n    primary=ExternalAPICall(),\n    fallback=CachedResponse()\n)\n\n# Use circuit breakers for external services\nprotected_service = (\n    PaymentGateway()\n    .with_skill(\"circuit_breaker\", failure_threshold=5)\n    .with_skill(\"retry\", max_attempts=3)\n)\n\n# Validate early in pipelines\npipeline = (\n    ValidateInput()  # Fail fast on bad input\n    | ExpensiveOperation()\n    | SaveResult()\n)\n\n\n\n5. Test in Isolation\nTest each brick independently:\n\n# Test individual bricks\nasync def test_validator():\n    validator = EmailValidator()\n    \n    # Valid case\n    result = await validator.invoke(\"test@example.com\")\n    assert result == \"test@example.com\"\n    \n    # Invalid case\n    with pytest.raises(ValueError):\n        await validator.invoke(\"invalid-email\")\n\n# Test composed pipelines\nasync def test_pipeline():\n    pipeline = Validator() | Processor() | Saver()\n    \n    # Mock dependencies\n    mock_deps = {\"db\": MockDatabase(), \"cache\": MockCache()}\n    \n    result = await pipeline.invoke(test_data, deps=mock_deps)\n    assert result[\"status\"] == \"saved\"",
    "crumbs": [
      "Home",
      "Building Systems",
      "Nanobricks SDK Guide"
    ]
  },
  {
    "objectID": "sdk-guide.html#deployment-patterns",
    "href": "sdk-guide.html#deployment-patterns",
    "title": "Nanobricks SDK Guide",
    "section": "Deployment Patterns",
    "text": "Deployment Patterns\n\nContainer Deployment\n\n# Add Docker skill to any brick or service\ndockerized = my_service.with_skill(\"docker\", {\n    \"base_image\": \"python:3.13-slim\",\n    \"port\": 8080,\n    \"healthcheck\": \"/health\"\n})\n\n# Generate Dockerfile and docker-compose\ndockerized.generate_dockerfile()\ndockerized.generate_compose()\n\n\n\nKubernetes Deployment\n\n# Add Kubernetes skill\nk8s_service = my_service.with_skill(\"kubernetes\", {\n    \"namespace\": \"production\",\n    \"replicas\": 3,\n    \"resources\": {\n        \"requests\": {\"cpu\": \"100m\", \"memory\": \"256Mi\"},\n        \"limits\": {\"cpu\": \"1000m\", \"memory\": \"1Gi\"}\n    },\n    \"autoscaling\": {\n        \"min_replicas\": 2,\n        \"max_replicas\": 10,\n        \"target_cpu\": 70\n    }\n})\n\n# Generate manifests\nk8s_service.generate_manifests()\nk8s_service.generate_helm_chart()\n\n\n\nServerless Deployment\n\n# Create serverless functions from nanobricks\nfrom nanobricks.serverless import create_lambda\n\nlambda_handler = create_lambda(\n    ProcessImage()\n    | ExtractText()\n    | TranslateText()\n    | SaveResult(),\n    \n    memory_mb=512,\n    timeout_seconds=30,\n    environment={\"REGION\": \"us-east-1\"}\n)\n\n# Deploy to AWS Lambda\nlambda_handler.deploy()",
    "crumbs": [
      "Home",
      "Building Systems",
      "Nanobricks SDK Guide"
    ]
  },
  {
    "objectID": "sdk-guide.html#monitoring-and-observability",
    "href": "sdk-guide.html#monitoring-and-observability",
    "title": "Nanobricks SDK Guide",
    "section": "Monitoring and Observability",
    "text": "Monitoring and Observability\n\nBuilt-in Observability\n\n# Add observability to any brick\nobservable = my_service.with_skill(\"observability\", {\n    \"traces\": True,\n    \"metrics\": True,\n    \"logs\": True,\n    \"service_name\": \"payment-processor\"\n})\n\n# Automatic instrumentation:\n# - Distributed tracing with OpenTelemetry\n# - Metrics collection (latency, errors, throughput)\n# - Structured logging with correlation IDs\n# - Custom metrics and spans\n\n\n\nCustom Metrics\n\nclass PaymentProcessor(NanobrickBase[Payment, Receipt, AppDeps]):\n    async def invoke(self, payment: Payment, *, deps: AppDeps) -&gt; Receipt:\n        # Record custom metrics\n        metrics = deps.get(\"metrics\")\n        if metrics:\n            metrics.increment(\"payments.processed\")\n            metrics.histogram(\"payment.amount\", payment.amount)\n            \n            with metrics.timer(\"payment.processing_time\"):\n                receipt = await self.process_payment(payment)\n            \n            metrics.gauge(\"payment.queue_size\", self.queue_size)\n        \n        return receipt",
    "crumbs": [
      "Home",
      "Building Systems",
      "Nanobricks SDK Guide"
    ]
  },
  {
    "objectID": "sdk-guide.html#performance-optimization",
    "href": "sdk-guide.html#performance-optimization",
    "title": "Nanobricks SDK Guide",
    "section": "Performance Optimization",
    "text": "Performance Optimization\n\nCaching Strategies\n\n# Add caching to expensive operations\ncached_processor = (\n    DataProcessor()\n    .with_skill(\"cache\", ttl=300, max_size=1000)\n)\n\n# Multi-level caching\npipeline = (\n    CheckMemoryCache()\n    | CheckRedisCache()\n    | LoadFromDatabase()\n    | UpdateCaches()\n)\n\n\n\nBatch Processing\n\n# Process items in batches\nbatch_processor = (\n    DataValidator()\n    | BatchProcessor(batch_size=100)\n    | BulkDatabaseWriter()\n)\n\n# Automatic batching with backpressure\nstream_processor = create_stream_processor(\n    source=KafkaConsumer(topic=\"events\"),\n    processor=EventProcessor(),\n    batch_size=50,\n    flush_interval_seconds=5\n)",
    "crumbs": [
      "Home",
      "Building Systems",
      "Nanobricks SDK Guide"
    ]
  },
  {
    "objectID": "sdk-guide.html#summary",
    "href": "sdk-guide.html#summary",
    "title": "Nanobricks SDK Guide",
    "section": "Summary",
    "text": "Summary\nNanobricks provides a powerful SDK for building production-ready Python systems:\n\nAtomic Components: Build complex systems from simple, testable parts\nStandardized Patterns: Every component follows the same interface\nProgressive Enhancement: Add capabilities through skills\nProduction Ready: Built-in support for all production concerns\nFlexible Architecture: Supports any architecture pattern\n\nStart building better Python systems with Nanobricks today!",
    "crumbs": [
      "Home",
      "Building Systems",
      "Nanobricks SDK Guide"
    ]
  },
  {
    "objectID": "distribution.html",
    "href": "distribution.html",
    "title": "Distribution & Deployment",
    "section": "",
    "text": "The fastest way to start using Nanobricks in your project:\n\n\n# From anywhere on your system\ntask -d /path/to/nanobricks dist:new:project:uv NAME=my-awesome-app DIR=/path/to/workspace\n\n# Or if you're already in the Nanobricks directory\ntask dist:new:project:uv NAME=my-awesome-app DIR=/path/to/workspace\n\n# The project will be created at: /path/to/workspace/my-awesome-app\n# The task automatically creates a Python 3.13 venv and installs dependencies\n\ncd /path/to/workspace/my-awesome-app\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Verify it works\ntask dev:test\n\n# Or without activation\nuv run task dev:test\n\n\n\n# From anywhere on your system\ntask -d /path/to/nanobricks dist:new:project NAME=my-awesome-app DIR=/path/to/workspace\n\n# Or if you're already in the Nanobricks directory\ntask dist:new:project NAME=my-awesome-app DIR=/path/to/workspace\n\n# The project will be created at: /path/to/workspace/my-awesome-app\ncd /path/to/workspace/my-awesome-app\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\nuv pip install -e .\n\n# Verify it works\ntask dev:test\nImportant: The DIR parameter is required to specify where to create your project. Without it, the task will fail to prevent accidentally creating projects inside the Nanobricks directory.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Distribution & Deployment"
    ]
  },
  {
    "objectID": "distribution.html#sec-quickstart",
    "href": "distribution.html#sec-quickstart",
    "title": "Distribution & Deployment",
    "section": "",
    "text": "The fastest way to start using Nanobricks in your project:\n\n\n# From anywhere on your system\ntask -d /path/to/nanobricks dist:new:project:uv NAME=my-awesome-app DIR=/path/to/workspace\n\n# Or if you're already in the Nanobricks directory\ntask dist:new:project:uv NAME=my-awesome-app DIR=/path/to/workspace\n\n# The project will be created at: /path/to/workspace/my-awesome-app\n# The task automatically creates a Python 3.13 venv and installs dependencies\n\ncd /path/to/workspace/my-awesome-app\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Verify it works\ntask dev:test\n\n# Or without activation\nuv run task dev:test\n\n\n\n# From anywhere on your system\ntask -d /path/to/nanobricks dist:new:project NAME=my-awesome-app DIR=/path/to/workspace\n\n# Or if you're already in the Nanobricks directory\ntask dist:new:project NAME=my-awesome-app DIR=/path/to/workspace\n\n# The project will be created at: /path/to/workspace/my-awesome-app\ncd /path/to/workspace/my-awesome-app\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\nuv pip install -e .\n\n# Verify it works\ntask dev:test\nImportant: The DIR parameter is required to specify where to create your project. Without it, the task will fail to prevent accidentally creating projects inside the Nanobricks directory.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Distribution & Deployment"
    ]
  },
  {
    "objectID": "distribution.html#task-based-distribution-system",
    "href": "distribution.html#task-based-distribution-system",
    "title": "Distribution & Deployment",
    "section": "Task-Based Distribution System",
    "text": "Task-Based Distribution System\nNanobricks uses Task for all distribution operations. All distribution tasks are namespaced under dist:.\n\nAvailable Tasks\n# Core distribution tasks\ntask dist:build              # Build wheel and source distributions\ntask dist:new:project        # Create a new project with Nanobricks\ntask dist:new:project:uv     # Create a new project using uv init\ntask dist:install:local      # Install Nanobricks in editable mode\ntask dist:install:wheel      # Install from built wheel\ntask dist:verify            # Verify installation works\ntask dist:clean             # Clean build artifacts\n\n# Additional tasks\ntask dist:local:repo        # Set up local package repository\ntask dist:publish:test      # Test package publishing (dry run)\ntask dist:publish:pypi      # Publish to PyPI (requires credentials)",
    "crumbs": [
      "Home",
      "Getting Started",
      "Distribution & Deployment"
    ]
  },
  {
    "objectID": "distribution.html#creating-new-projects",
    "href": "distribution.html#creating-new-projects",
    "title": "Distribution & Deployment",
    "section": "Creating New Projects",
    "text": "Creating New Projects\n\nUsing the Project Generator\n\nWith uv init (Recommended)\nUse uv init for full compatibility with uv’s workflow:\n# From the Nanobricks directory\ntask dist:new:project:uv NAME=my-app DIR=/path/to/workspace\n\n# From anywhere else\ntask -d /path/to/nanobricks dist:new:project:uv NAME=my-app DIR=/path/to/workspace\n\n\nManual structure\nCreate the project structure manually:\n# From the Nanobricks directory\ntask dist:new:project NAME=my-app DIR=/path/to/workspace\n\n# From anywhere else\ntask -d /path/to/nanobricks dist:new:project NAME=my-app DIR=/path/to/workspace\nNote: The DIR parameter is required for both commands to specify where to create your project.\nThis creates the project at DIR/NAME:\nmy-app/\n├── Taskfile.yml         # Project tasks\n├── pyproject.toml       # With Nanobricks dependency\n├── README.md           \n├── .gitignore\n├── src/\n│   └── my_app/\n│       └── __init__.py  # Example brick included\n└── tests/\n    └── test_example.py  # Example test\n\n\n\nManual Setup\nIf you prefer manual setup, add to your pyproject.toml:\n[project]\ndependencies = [\n    # For development (editable install)\n    \"nanobricks @ file:///path/to/nanobricks\",\n    \n    # For stable install from wheel\n    # \"nanobricks @ file:///path/to/nanobricks/dist/nanobricks-0.1.0-py3-none-any.whl\",\n    \n    # From Git (when available)\n    # \"nanobricks @ git+https://github.com/yourusername/nanobricks.git@main\",\n]\n\n# For uv compatibility\n[tool.uv]\ndev-dependencies = [\n    \"pytest&gt;=8.0\",\n    \"pytest-asyncio&gt;=0.23\",\n    \"ruff&gt;=0.7.0\",\n    \"mypy&gt;=1.9\",\n]",
    "crumbs": [
      "Home",
      "Getting Started",
      "Distribution & Deployment"
    ]
  },
  {
    "objectID": "distribution.html#installation-methods",
    "href": "distribution.html#installation-methods",
    "title": "Distribution & Deployment",
    "section": "Installation Methods",
    "text": "Installation Methods\n\nEditable Install (Development)\nBest for active development where you might modify Nanobricks:\ntask dist:install:local PROJECT_PATH=/path/to/your/project\nOr manually with uv:\nuv pip install -e /path/to/nanobricks\n\n\nWheel Install (Stable)\nFor production-like installations:\n# Build and install wheel\ntask dist:install:wheel PROJECT_PATH=/path/to/your/project\n\n\nGit-Based Install\nWhen sharing via Git:\n[project]\ndependencies = [\n    # HTTPS\n    \"nanobricks @ git+https://github.com/yourusername/nanobricks.git@main\",\n    \n    # SSH\n    \"nanobricks @ git+ssh://git@github.com/yourusername/nanobricks.git@main\",\n    \n    # Specific tag/version\n    \"nanobricks @ git+https://github.com/yourusername/nanobricks.git@v0.1.0\",\n]",
    "crumbs": [
      "Home",
      "Getting Started",
      "Distribution & Deployment"
    ]
  },
  {
    "objectID": "distribution.html#building-packages",
    "href": "distribution.html#building-packages",
    "title": "Distribution & Deployment",
    "section": "Building Packages",
    "text": "Building Packages\nBuild wheel and source distributions:\ntask dist:build\n\n# Output will be in:\n# - dist/nanobricks-0.1.0-py3-none-any.whl\n# - dist/nanobricks-0.1.0.tar.gz",
    "crumbs": [
      "Home",
      "Getting Started",
      "Distribution & Deployment"
    ]
  },
  {
    "objectID": "distribution.html#your-first-nanobrick",
    "href": "distribution.html#your-first-nanobrick",
    "title": "Distribution & Deployment",
    "section": "Your First Nanobrick",
    "text": "Your First Nanobrick\nAfter installation, create a simple brick:\n# my_bricks.py\nfrom nanobricks import NanobrickSimple, Pipeline\n\nclass GreetingBrick(NanobrickSimple[str, str]):\n    async def invoke(self, input: str) -&gt; str:\n        return f\"Hello, {input}!\"\n\nclass ShoutBrick(NanobrickSimple[str, str]):\n    async def invoke(self, input: str) -&gt; str:\n        return input.upper() + \"!!!\"\n\n# Create pipeline\ngreeting_pipeline = GreetingBrick() | ShoutBrick()\n\n# Test it\nimport asyncio\nresult = asyncio.run(greeting_pipeline.invoke(\"World\"))\nprint(result)  # \"HELLO, WORLD!!!\"",
    "crumbs": [
      "Home",
      "Getting Started",
      "Distribution & Deployment"
    ]
  },
  {
    "objectID": "distribution.html#docker-integration",
    "href": "distribution.html#docker-integration",
    "title": "Distribution & Deployment",
    "section": "Docker Integration",
    "text": "Docker Integration\n\nDockerfile Example\n# Option 1: Copy wheel\nCOPY ./nanobricks-0.1.0-py3-none-any.whl /tmp/\nRUN pip install /tmp/nanobricks-0.1.0-py3-none-any.whl\n\n# Option 2: Install from local path (development)\nCOPY ./nanobricks /app/nanobricks\nRUN pip install -e /app/nanobricks\n\n# Option 3: Install from git\nRUN pip install git+https://github.com/user/nanobricks.git\n\n\nDocker Compose Development\nversion: '3.8'\n\nservices:\n  app:\n    build: .\n    volumes:\n      # Mount Nanobricks for development\n      - /path/to/nanobricks:/app/nanobricks\n    environment:\n      - PYTHONPATH=/app/nanobricks/src:$PYTHONPATH",
    "crumbs": [
      "Home",
      "Getting Started",
      "Distribution & Deployment"
    ]
  },
  {
    "objectID": "distribution.html#local-package-repository",
    "href": "distribution.html#local-package-repository",
    "title": "Distribution & Deployment",
    "section": "Local Package Repository",
    "text": "Local Package Repository\nSet up a local package index:\n# Create local repo\ntask dist:local:repo\n\n# Install in projects with:\npip install nanobricks --find-links file://$HOME/.nanobricks-repo",
    "crumbs": [
      "Home",
      "Getting Started",
      "Distribution & Deployment"
    ]
  },
  {
    "objectID": "distribution.html#publishing",
    "href": "distribution.html#publishing",
    "title": "Distribution & Deployment",
    "section": "Publishing",
    "text": "Publishing\n\nTest Publishing\nValidate packages before publishing:\ntask dist:publish:test\n\n\nPublish to PyPI\nWhen ready to share publicly:\n# Requires PyPI credentials\ntask dist:publish:pypi",
    "crumbs": [
      "Home",
      "Getting Started",
      "Distribution & Deployment"
    ]
  },
  {
    "objectID": "distribution.html#verification",
    "href": "distribution.html#verification",
    "title": "Distribution & Deployment",
    "section": "Verification",
    "text": "Verification\nVerify Nanobricks is properly installed:\n# In your project's virtual environment\npython -c \"import nanobricks; print(nanobricks.__version__)\"\n\n# Or use the verification task\ncd /path/to/nanobricks\ntask dist:verify",
    "crumbs": [
      "Home",
      "Getting Started",
      "Distribution & Deployment"
    ]
  },
  {
    "objectID": "distribution.html#sdk-use-cases",
    "href": "distribution.html#sdk-use-cases",
    "title": "Distribution & Deployment",
    "section": "SDK Use Cases",
    "text": "SDK Use Cases\nNanobricks is designed as an SDK for building composable Python systems:\n\nDatabase Layer\nfrom nanobricks import NanobrickSimple, with_skill\nfrom nanobricks.skills import DatabaseSkill\n\n@with_skill(DatabaseSkill)\nclass UserRepository(NanobrickSimple[dict, User]):\n    async def invoke(self, query: dict) -&gt; User:\n        # Your DB logic here\n        pass\n\n\nAPI Gateway\nfrom nanobricks.skills import ApiSkill\n\n@with_skill(ApiSkill)\nclass ApiGateway(NanobrickSimple[Request, Response]):\n    async def invoke(self, request: Request) -&gt; Response:\n        # Route to appropriate service\n        pass\n\n\nData Pipeline\n# Compose transformers\npipeline = (\n    CSVReader() |\n    DataValidator() |\n    Transformer() |\n    DatabaseWriter()\n)\n\n\nMicroservices\nfrom nanobricks.skills import MicroserviceSkill\n\n@with_skill(MicroserviceSkill)\nclass OrderService(NanobrickSimple[Order, OrderResult]):\n    async def invoke(self, order: Order) -&gt; OrderResult:\n        # Process order\n        pass",
    "crumbs": [
      "Home",
      "Getting Started",
      "Distribution & Deployment"
    ]
  },
  {
    "objectID": "distribution.html#troubleshooting",
    "href": "distribution.html#troubleshooting",
    "title": "Distribution & Deployment",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nImport Errors\n\nEnsure you’re in the correct virtual environment\nCheck the installation path in pyproject.toml\nRun task dist:verify to test the installation\n\n\n\nTask Not Found\nRun task commands from the Nanobricks root directory:\ncd /path/to/nanobricks\ntask dist:new:project NAME=myapp DIR=/path/to/workspace\nOr use the -d flag from anywhere:\ntask -d /path/to/nanobricks dist:new:project NAME=myapp DIR=/path/to/workspace\n\n\nPython Version\nEnsure you’re using Python 3.13:\npython --version  # Should show Python 3.13.x",
    "crumbs": [
      "Home",
      "Getting Started",
      "Distribution & Deployment"
    ]
  },
  {
    "objectID": "distribution.html#next-steps",
    "href": "distribution.html#next-steps",
    "title": "Distribution & Deployment",
    "section": "Next Steps",
    "text": "Next Steps\n\nExplore examples: Check the examples/ directory for real-world patterns\nRead the guides:\n\nTutorial for step-by-step learning\nSDK Guide for building applications\nPatterns for advanced composition\n\nBuild something: Create your own bricks and share them!\n\nHappy building with Nanobricks! 🧱",
    "crumbs": [
      "Home",
      "Getting Started",
      "Distribution & Deployment"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nanobricks",
    "section": "",
    "text": "Nanobricks is a revolutionary SDK that lets you build production-ready Python systems from small, composable components. Think “Lego blocks for code” - simple pieces that combine to create anything.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Nanobricks"
    ]
  },
  {
    "objectID": "index.html#build-better-python-systems",
    "href": "index.html#build-better-python-systems",
    "title": "Nanobricks",
    "section": "",
    "text": "Nanobricks is a revolutionary SDK that lets you build production-ready Python systems from small, composable components. Think “Lego blocks for code” - simple pieces that combine to create anything.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Nanobricks"
    ]
  },
  {
    "objectID": "index.html#why-nanobricks",
    "href": "index.html#why-nanobricks",
    "title": "Nanobricks",
    "section": "Why Nanobricks?",
    "text": "Why Nanobricks?\n\n🧱 Atomic Components\nEvery nanobrick does ONE thing well. No bloated classes, no complex inheritance - just simple, focused components that you can understand at a glance.\n\n\n🔗 Universal Composition\nAll nanobricks share the same interface. Combine them with the pipe operator (|) to build complex systems from simple parts. What works alone works in combination.\n\n\n🚀 Progressive Enhancement\nStart simple, add capabilities as needed. Skills like logging, API endpoints, and monitoring can be added to any nanobrick without changing its code.\n\n\n🏭 Production Ready\nBuilt-in support for everything you need in production: deployment (Docker, Kubernetes), observability, security, performance optimization, and more.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Nanobricks"
    ]
  },
  {
    "objectID": "index.html#quick-example",
    "href": "index.html#quick-example",
    "title": "Nanobricks",
    "section": "Quick Example",
    "text": "Quick Example\nfrom nanobricks import NanobrickSimple, skill\n\n# Define atomic components\n@skill(\"logging\")\nclass ValidateUser(NanobrickSimple[dict, dict]):\n    async def invoke(self, user: dict, *, deps=None) -&gt; dict:\n        if not user.get(\"email\"):\n            raise ValueError(\"Email required\")\n        return user\n\n@skill(\"api\", port=8080)\nclass UserService(NanobrickSimple[dict, dict]):\n    def __init__(self):\n        super().__init__()\n        self.pipeline = (\n            ValidateUser()\n            | EnrichUser()\n            | SaveUser()\n        ).with_skill(\"retry\", max_attempts=3)\n    \n    async def invoke(self, user: dict, *, deps=None) -&gt; dict:\n        return await self.pipeline.invoke(user, deps=deps)\n\n# Deploy it!\nservice = UserService()\nservice.start_server()  # REST API at http://localhost:8080",
    "crumbs": [
      "Home",
      "Getting Started",
      "Nanobricks"
    ]
  },
  {
    "objectID": "index.html#the-ten-principles",
    "href": "index.html#the-ten-principles",
    "title": "Nanobricks",
    "section": "The Ten Principles",
    "text": "The Ten Principles\n\nBe Simple — Complexity emerges from composition, not components\nBe Standardized — One interface to rule them all\nBe Composable — Play well with others\nBe Self-Sufficient — Carry what you need\nBe Scaffoldable — Work out of the box\nBe Observable — Show what you’re doing\nBe Resilient — Fail gracefully, recover automatically\nBe Configurable — Adapt without changing\nBe Evolutionary — Improve over time\nBe Secure — Safe by default",
    "crumbs": [
      "Home",
      "Getting Started",
      "Nanobricks"
    ]
  },
  {
    "objectID": "index.html#get-started",
    "href": "index.html#get-started",
    "title": "Nanobricks",
    "section": "Get Started",
    "text": "Get Started\n\n🚀 Quickstart Guide\nBuild your first nanobrick in 10 minutes\n\n\n📚 Tutorial\nLearn nanobricks step by step\n\n\n🏗️ SDK Guide\nBuild production systems with nanobricks\n\n\n🎨 Design Patterns\nAdvanced composition patterns and best practices",
    "crumbs": [
      "Home",
      "Getting Started",
      "Nanobricks"
    ]
  },
  {
    "objectID": "index.html#use-cases",
    "href": "index.html#use-cases",
    "title": "Nanobricks",
    "section": "Use Cases",
    "text": "Use Cases\n\nBuild a REST API\napi = create_rest_api(\n    endpoints=[UserEndpoints(), OrderEndpoints()],\n    middleware=[RateLimiter(), Authenticator()],\n    skills=[\"logging\", \"monitoring\", \"docker\"]\n)\n\n\nCreate a Data Pipeline\npipeline = (\n    S3Loader(bucket=\"raw-data\")\n    | CSVParser()\n    | DataValidator(schema)\n    | Transformer()\n    | DatabaseWriter()\n).with_skill(\"monitoring\")\n\n\nDesign a Microservice\nservice = create_microservice(\n    name=\"payment-processor\",\n    handlers={\n        \"POST /payments\": ProcessPayment(),\n        \"payment.requested\": PaymentHandler()\n    },\n    skills=[\"api\", \"kubernetes\", \"observability\"]\n)",
    "crumbs": [
      "Home",
      "Getting Started",
      "Nanobricks"
    ]
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "Nanobricks",
    "section": "Key Features",
    "text": "Key Features\n\nSkills System\nAdd capabilities without changing code: - 🚀 API - REST endpoints with FastAPI - 💻 CLI - Command-line interface with Typer\n- 🎨 UI - Web interface with Streamlit - 📊 Observability - Metrics, traces, and logs - 🐳 Docker - Automatic containerization - ☸️ Kubernetes - Cloud-native deployment - 🤖 AI - LLM integration and agent capabilities\n\n\nType Safety\nFull type inference through entire pipelines. Your IDE knows the types at every step.\n\n\nProduction Features\n\nCircuit breakers and bulkheads\nRate limiting and authentication\nHealth checks and graceful shutdown\nCaching and performance optimization\nSecurity hardening",
    "crumbs": [
      "Home",
      "Getting Started",
      "Nanobricks"
    ]
  },
  {
    "objectID": "index.html#join-the-community",
    "href": "index.html#join-the-community",
    "title": "Nanobricks",
    "section": "Join the Community",
    "text": "Join the Community\n\nGitHub\nDiscord\nExamples\n\n\nBuild it right, build it once.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Nanobricks"
    ]
  },
  {
    "objectID": "roadmap.html",
    "href": "roadmap.html",
    "title": "Nanobricks Roadmap",
    "section": "",
    "text": "This roadmap synthesizes feedback from real-world usage of nanobricks (particularly from the nano-scorm project) and outlines our path forward. The framework’s core abstractions have proven solid, but we need to reduce friction and provide clearer guidance for common patterns.",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#overview",
    "href": "roadmap.html#overview",
    "title": "Implementation Roadmap",
    "section": "",
    "text": "The Nanobricks implementation follows a phased approach, prioritizing core functionality before advanced features.",
    "crumbs": [
      "Home",
      "Evolution",
      "Implementation Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#phase-1-core-framework-week-1-2",
    "href": "roadmap.html#phase-1-core-framework-week-1-2",
    "title": "Implementation Roadmap",
    "section": "Phase 1: Core Framework (Week 1-2)",
    "text": "Phase 1: Core Framework (Week 1-2)\n\nMilestone 1.1: Basic Protocol\n\nDefine Nanobrick protocol with generics\nImplement __or__ operator for composition\nCreate basic invoke() method\nSet up nanobricks-core package with uv\n\n\n\nMilestone 1.2: Type Safety\n\nIntegrate beartype for runtime checking\nConfigure mypy for static analysis\nCreate type-safe composition operators\nDocument type patterns\n\n\n\nMilestone 1.3: First Examples\n\nCreate ValidatorData nanobrick\nCreate DataTransformer nanobrick\nDemonstrate basic pipeline\nWrite initial tests",
    "crumbs": [
      "Home",
      "Evolution",
      "Implementation Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#phase-2-skill-system-week-3-4",
    "href": "roadmap.html#phase-2-skill-system-week-3-4",
    "title": "Implementation Roadmap",
    "section": "Phase 2: Skill System (Week 3-4)",
    "text": "Phase 2: Skill System (Week 3-4)\n\nMilestone 2.1: Skill Interface\n\nDefine Skill protocol\nImplement enhance() method\nCreate skill registry\nAdd activation logic\n\n\n\nMilestone 2.2: Core Skills\n\nImplement SkillAPI (FastAPI)\nImplement SkillCLI (Typer)\nImplement SkillUI (Streamlit)\nCreate examples for each\n\n\n\nMilestone 2.3: Integration Patterns\n\nMount patterns for APIs\nCommand patterns for CLIs\nComponent patterns for UIs\nTest compositions",
    "crumbs": [
      "Home",
      "Evolution",
      "Implementation Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#phase-3-standard-library-week-5-6",
    "href": "roadmap.html#phase-3-standard-library-week-5-6",
    "title": "Implementation Roadmap",
    "section": "Phase 3: Standard Library (Week 5-6)",
    "text": "Phase 3: Standard Library (Week 5-6)\n\nMilestone 3.1: Common Nanobricks\n\nValidators (email, phone, etc.)\nTransformers (JSON, CSV, etc.)\nFilters (data cleaning)\nAggregators (statistics)\n\n\n\nMilestone 3.2: Collections\n\nCreate nanobricks-validators\nCreate nanobricks-transformers\nCreate nanobricks-connectors\nPublish to PyPI",
    "crumbs": [
      "Home",
      "Evolution",
      "Implementation Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#phase-4-scaffolding-tools-week-7-8",
    "href": "roadmap.html#phase-4-scaffolding-tools-week-7-8",
    "title": "Implementation Roadmap",
    "section": "Phase 4: Scaffolding & Tools (Week 7-8)",
    "text": "Phase 4: Scaffolding & Tools (Week 7-8)\n\nMilestone 4.1: Project Generator\n\nCreate nanobrick-new CLI\nTemplates for different brick types\ngo-task automation\nVS Code integration\n\n\n\nMilestone 4.2: Development Tools\n\nTesting framework\nDocumentation generator\nType stub generator\nPerformance profiler",
    "crumbs": [
      "Home",
      "Evolution",
      "Implementation Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#phase-5-ai-integration-week-9-10",
    "href": "roadmap.html#phase-5-ai-integration-week-9-10",
    "title": "Implementation Roadmap",
    "section": "Phase 5: AI Integration (Week 9-10)",
    "text": "Phase 5: AI Integration (Week 9-10)\n\nMilestone 5.1: MCP Support\n\nMCP server skill\nTool/prompt exposure\nExample AI nanobricks\nDocumentation\n\n\n\nMilestone 5.2: Advanced AI\n\nMemory management\nReasoning traces\nMulti-agent support\nCost optimization",
    "crumbs": [
      "Home",
      "Evolution",
      "Implementation Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#phase-6-ecosystem-week-11-12",
    "href": "roadmap.html#phase-6-ecosystem-week-11-12",
    "title": "Implementation Roadmap",
    "section": "Phase 6: Ecosystem (Week 11-12)",
    "text": "Phase 6: Ecosystem (Week 11-12)\n\nMilestone 6.1: Registry\n\nPackage registry design\nDiscovery mechanisms\nVersion management\nDependency resolution\n\n\n\nMilestone 6.2: Community\n\nExample gallery\nTutorial series\nContribution guide\nDiscord/Slack community",
    "crumbs": [
      "Home",
      "Evolution",
      "Implementation Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#success-metrics",
    "href": "roadmap.html#success-metrics",
    "title": "Nanobricks Roadmap",
    "section": "Success Metrics",
    "text": "Success Metrics\n\nDeveloper Satisfaction: &gt;4.5/5 in surveys\nAdoption: 10k+ GitHub stars, 100k+ monthly downloads\nPerformance: &lt;1% overhead vs hand-written code\nReliability: 99.99% uptime in production deployments\nCommunity: 100+ contributors, 1000+ third-party bricks",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#risk-mitigation",
    "href": "roadmap.html#risk-mitigation",
    "title": "Implementation Roadmap",
    "section": "Risk Mitigation",
    "text": "Risk Mitigation\n\nTechnical Risks\n\nType complexity: Start simple, add gradually\nPerformance overhead: Profile early, optimize later\nAPI stability: Mark experimental features clearly\n\n\n\nAdoption Risks\n\nLearning curve: Comprehensive tutorials\nMigration path: From existing code\nEcosystem: Seed with quality examples",
    "crumbs": [
      "Home",
      "Evolution",
      "Implementation Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#next-steps",
    "href": "roadmap.html#next-steps",
    "title": "Implementation Roadmap",
    "section": "Next Steps",
    "text": "Next Steps\n\nSet up development environment\nCreate nanobricks-core package\nImplement basic protocol\nBuild first working example",
    "crumbs": [
      "Home",
      "Evolution",
      "Implementation Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#how-to-contribute",
    "href": "roadmap.html#how-to-contribute",
    "title": "Implementation Roadmap",
    "section": "How to Contribute",
    "text": "How to Contribute\n\nReview and comment on design\nImplement example nanobricks\nTest early versions\nProvide feedback",
    "crumbs": [
      "Home",
      "Evolution",
      "Implementation Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#questions-for-discussion",
    "href": "roadmap.html#questions-for-discussion",
    "title": "Implementation Roadmap",
    "section": "Questions for Discussion",
    "text": "Questions for Discussion\n\nShould we prioritize breadth or depth?\nWhich skills are most critical?\nHow do we ensure quality standards?\nWhat’s the MVP for launch?",
    "crumbs": [
      "Home",
      "Evolution",
      "Implementation Roadmap"
    ]
  },
  {
    "objectID": "comparison.html",
    "href": "comparison.html",
    "title": "Framework Comparison",
    "section": "",
    "text": "This guide compares Nanobricks with other popular Python frameworks to help you understand when and why to choose Nanobricks for your projects.",
    "crumbs": [
      "Home",
      "Building Systems",
      "Framework Comparison"
    ]
  },
  {
    "objectID": "comparison.html#introduction",
    "href": "comparison.html#introduction",
    "title": "Framework Comparison",
    "section": "",
    "text": "This guide compares Nanobricks with other popular Python frameworks to help you understand when and why to choose Nanobricks for your projects.",
    "crumbs": [
      "Home",
      "Building Systems",
      "Framework Comparison"
    ]
  },
  {
    "objectID": "comparison.html#quick-comparison-table",
    "href": "comparison.html#quick-comparison-table",
    "title": "Framework Comparison",
    "section": "Quick Comparison Table",
    "text": "Quick Comparison Table\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nNanobricks\nFlask/Django\nFastAPI\nCelery\nApache Beam\nPrefect/Airflow\n\n\n\n\nPrimary Use\nComposable systems\nWeb apps\nREST APIs\nTask queues\nData pipelines\nWorkflows\n\n\nArchitecture\nComponent-based\nMVC/MTV\nRoute-based\nTask-based\nPipeline-based\nDAG-based\n\n\nComposition\nNative (\\| operator)\nLimited\nLimited\nChain tasks\nTransform-based\nTask dependencies\n\n\nType Safety\nFull inference\nPartial\nGood (Pydantic)\nLimited\nLimited\nLimited\n\n\nAsync Support\nNative\nFlask: No, Django: Limited\nNative\nYes\nLimited\nYes\n\n\nProduction Features\nBuilt-in\nAdd-ons\nSome built-in\nBasic\nEnterprise\nEnterprise\n\n\nLearning Curve\nLow\nMedium\nLow-Medium\nMedium\nHigh\nHigh\n\n\nDeployment\nAny (Docker, K8s, Lambda)\nTraditional\nModern\nWorkers\nRunners\nOrchestrator",
    "crumbs": [
      "Home",
      "Building Systems",
      "Framework Comparison"
    ]
  },
  {
    "objectID": "comparison.html#detailed-comparisons",
    "href": "comparison.html#detailed-comparisons",
    "title": "Framework Comparison",
    "section": "Detailed Comparisons",
    "text": "Detailed Comparisons\n\nNanobricks vs. Web Frameworks (Flask/Django)\n\nFlask/Django Approach\n# Flask\nfrom flask import Flask, request, jsonify\nfrom flask_sqlalchemy import SQLAlchemy\n\napp = Flask(__name__)\ndb = SQLAlchemy(app)\n\n@app.route('/users', methods=['POST'])\ndef create_user():\n    data = request.get_json()\n    # Validation logic mixed with business logic\n    if not data.get('email'):\n        return jsonify({'error': 'Email required'}), 400\n    \n    user = User(email=data['email'])\n    db.session.add(user)\n    db.session.commit()\n    \n    return jsonify({'id': user.id})\n\n\nNanobricks Approach\n\n# Nanobricks - Composable and reusable\nfrom nanobricks import NanobrickSimple, skill\n\nclass ValidateEmail(NanobrickSimple[dict, dict]):\n    async def invoke(self, data: dict, *, deps=None) -&gt; dict:\n        if not data.get('email'):\n            raise ValueError('Email required')\n        return data\n\nclass CreateUser(NanobrickSimple[dict, dict]):\n    async def invoke(self, data: dict, *, deps=None) -&gt; dict:\n        user = await deps['db'].create_user(data)\n        return {'id': user.id}\n\n# Compose and add web interface\nuser_service = (\n    ValidateEmail() \n    | CreateUser()\n).with_skill(\"api\", method=\"POST\", path=\"/users\")\n\nKey Differences: - Separation of Concerns: Nanobricks separates validation, business logic, and web interface - Reusability: Each brick can be reused in CLI, API, or batch processing - Testability: Test each component in isolation - Flexibility: Not tied to web context - same logic works everywhere\n\n\n\nNanobricks vs. FastAPI\n\nFastAPI Approach\n# FastAPI\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\nclass UserCreate(BaseModel):\n    email: str\n    name: str\n\n@app.post(\"/users\")\nasync def create_user(user: UserCreate):\n    # Business logic tied to HTTP endpoint\n    if await db.user_exists(user.email):\n        raise HTTPException(400, \"User exists\")\n    \n    result = await db.create_user(user.dict())\n    return {\"id\": result.id}\n\n\nNanobricks Approach\n\n# Nanobricks - Protocol-agnostic\nclass CheckUserExists(NanobrickSimple[UserCreate, UserCreate]):\n    async def invoke(self, user: UserCreate, *, deps=None) -&gt; UserCreate:\n        if await deps['db'].user_exists(user.email):\n            raise ValueError(\"User exists\")\n        return user\n\n# Use in API, CLI, or anywhere\npipeline = CheckUserExists() | CreateUser()\n\n# Add API interface when needed\napi_service = pipeline.with_skill(\"api\", response_model=UserResponse)\n\n# Or use in CLI\ncli_service = pipeline.with_skill(\"cli\", command=\"create-user\")\n\n# Or in batch processing\nfor user_data in user_list:\n    result = await pipeline.invoke(user_data)\n\nKey Differences: - Protocol Agnostic: Logic not tied to HTTP - Multiple Interfaces: Same logic, multiple access methods - Progressive Enhancement: Add capabilities as needed\n\n\n\nNanobricks vs. Celery\n\nCelery Approach\n# Celery\nfrom celery import Celery, chain\n\napp = Celery('tasks')\n\n@app.task\ndef validate_data(data):\n    # Validation logic\n    return data\n\n@app.task\ndef process_data(data):\n    # Processing logic\n    return processed\n\n@app.task\ndef save_data(data):\n    # Save logic\n    return saved\n\n# Chain tasks\nworkflow = chain(validate_data.s(), process_data.s(), save_data.s())\nresult = workflow.apply_async(args=[data])\n\n\nNanobricks Approach\n\n# Nanobricks - Simpler, type-safe\npipeline = (\n    ValidateData()\n    | ProcessData() \n    | SaveData()\n)\n\n# Run locally\nresult = await pipeline.invoke(data)\n\n# Or distribute with skill\ndistributed = pipeline.with_skill(\"distributed\", \n    backend=\"ray\",  # or \"dask\", \"celery\"\n    workers=4\n)\n\n# Or make async with queues\nqueued = pipeline.with_skill(\"queue\",\n    backend=\"rabbitmq\",\n    queue=\"data-processing\"\n)\n\nKey Differences: - No Decorator Magic: Plain classes, no hidden behavior - Type Safety: Full type inference through pipeline - Flexible Execution: Local, distributed, or queued with skills - Simpler Testing: No need for Celery test infrastructure\n\n\n\nNanobricks vs. Apache Beam\n\nApache Beam Approach\n# Apache Beam\nimport apache_beam as beam\n\ndef parse_user(line):\n    # Parse logic\n    return user_dict\n\ndef validate_user(user):\n    # Validation\n    return user\n\ndef enrich_user(user):\n    # Enrichment\n    return enriched\n\n# Define pipeline\nwith beam.Pipeline() as p:\n    (p \n     | 'Read' &gt;&gt; beam.io.ReadFromText('users.txt')\n     | 'Parse' &gt;&gt; beam.Map(parse_user)\n     | 'Validate' &gt;&gt; beam.Map(validate_user)\n     | 'Enrich' &gt;&gt; beam.Map(enrich_user)\n     | 'Write' &gt;&gt; beam.io.WriteToText('output.txt')\n    )\n\n\nNanobricks Approach\n\n# Nanobricks - Pythonic and simple\nfrom nanobricks import create_pipeline\n\n# Define as classes with clear interfaces\nclass ParseUser(NanobrickSimple[str, dict]):\n    async def invoke(self, line: str, *, deps=None) -&gt; dict:\n        return parse_line(line)\n\nclass ValidateUser(NanobrickSimple[dict, dict]):\n    async def invoke(self, user: dict, *, deps=None) -&gt; dict:\n        # Validation with proper error messages\n        return user\n\n# Compose pipeline\npipeline = create_pipeline(\n    source=FileSource(\"users.txt\"),\n    processors=[\n        ParseUser(),\n        ValidateUser(),\n        EnrichUser()\n    ],\n    destination=FileDestination(\"output.txt\")\n)\n\n# Run locally or distributed\nawait pipeline.run()\n\n# Or with streaming\nstream_pipeline = pipeline.with_skill(\"streaming\",\n    backend=\"kafka\",\n    parallelism=10\n)\n\nKey Differences: - Simpler API: No complex beam transforms - Native Python: No Java-inspired abstractions - Better Error Handling: Clear error messages and debugging - Progressive Scaling: Start simple, scale when needed\n\n\n\nNanobricks vs. Workflow Engines (Prefect/Airflow)\n\nPrefect/Airflow Approach\n# Prefect\nfrom prefect import task, Flow\n\n@task\ndef extract_data():\n    return data\n\n@task\ndef transform_data(data):\n    return transformed\n\n@task\ndef load_data(data):\n    return loaded\n\n# Define flow\nwith Flow(\"ETL\") as flow:\n    raw = extract_data()\n    transformed = transform_data(raw)\n    result = load_data(transformed)\n\n# Schedule and run\nflow.schedule = IntervalSchedule(interval=timedelta(hours=1))\nflow.run()\n\n\nNanobricks Approach\n\n# Nanobricks - Code-first, lightweight\netl_pipeline = (\n    ExtractData()\n    | TransformData()\n    | LoadData()\n).with_skill(\"monitoring\")\n\n# Simple scheduling\nfrom nanobricks.scheduling import Schedule\n\nscheduled = Schedule(\n    pipeline=etl_pipeline,\n    cron=\"0 * * * *\",  # Every hour\n    on_failure=send_alert,\n    on_success=update_metrics\n)\n\n# Or use with existing schedulers\nasync def airflow_task():\n    \"\"\"Use nanobricks within Airflow\"\"\"\n    return await etl_pipeline.invoke(context)\n\nKey Differences: - Lightweight: No heavy orchestrator required - Code-First: Define in code, not YAML/UI - Flexible Deployment: Use standalone or within existing systems - Better Testing: Standard Python testing practices",
    "crumbs": [
      "Home",
      "Building Systems",
      "Framework Comparison"
    ]
  },
  {
    "objectID": "comparison.html#when-to-use-nanobricks",
    "href": "comparison.html#when-to-use-nanobricks",
    "title": "Framework Comparison",
    "section": "When to Use Nanobricks",
    "text": "When to Use Nanobricks\n\n✅ Choose Nanobricks When You Need:\n\nComposable Architecture\n\nBuilding modular, reusable components\nNeed to mix and match functionality\nWant to avoid monolithic designs\n\nMultiple Interfaces\n\nSame logic via API, CLI, and UI\nProtocol-agnostic design\nProgressive enhancement\n\nType Safety\n\nFull type inference through pipelines\nIDE support and autocomplete\nCatch errors at development time\n\nProduction Features\n\nBuilt-in monitoring, deployment, security\nNo need to integrate multiple tools\nConsistent patterns across system\n\nFlexibility\n\nStart simple, scale later\nChange execution model without rewriting\nAdapt to new requirements easily\n\n\n\n\n❌ Nanobricks Might Not Be Ideal For:\n\nSimple CRUD Apps\n\nIf Django admin is all you need\nNo complex business logic\nStandard web app patterns\n\nLegacy Integration\n\nHeavily invested in specific framework\nTeam expertise in other tools\nMigration cost too high\n\nSpecialized Domains\n\nScientific computing (use NumPy/SciPy directly)\nPure ML pipelines (use MLflow/Kubeflow)\nSimple scripts (plain Python is fine)",
    "crumbs": [
      "Home",
      "Building Systems",
      "Framework Comparison"
    ]
  },
  {
    "objectID": "comparison.html#integration-examples",
    "href": "comparison.html#integration-examples",
    "title": "Framework Comparison",
    "section": "Integration Examples",
    "text": "Integration Examples\n\nUsing Nanobricks with Existing Frameworks\n\nWith Flask/Django\n\n# Use nanobricks for business logic in Flask\nfrom flask import Flask\nfrom nanobricks import create_pipeline\n\napp = Flask(__name__)\n\n# Define business logic with nanobricks\nprocess_order = create_pipeline([\n    ValidateOrder(),\n    CalculatePrice(),\n    ProcessPayment(),\n    SendConfirmation()\n])\n\n@app.route('/orders', methods=['POST'])\nasync def create_order():\n    data = request.get_json()\n    try:\n        result = await process_order.invoke(data)\n        return jsonify(result)\n    except ValueError as e:\n        return jsonify({'error': str(e)}), 400\n\n\n\nWith Celery\n\n# Use nanobricks within Celery tasks\nfrom celery import Celery\nfrom nanobricks import load_pipeline\n\ncelery = Celery('tasks')\n\n# Load pre-built nanobrick pipeline\npipeline = load_pipeline('data_processor')\n\n@celery.task\ndef process_data_task(data):\n    # Use nanobricks for actual processing\n    return asyncio.run(pipeline.invoke(data))\n\n\n\nWith FastAPI\n\n# Enhance FastAPI with nanobricks\nfrom fastapi import FastAPI\nfrom nanobricks import NanobrickSimple\n\napp = FastAPI()\n\n# Create reusable processors\nvalidator = DataValidator().with_skill(\"cache\")\nprocessor = DataProcessor().with_skill(\"monitoring\")\n\n@app.post(\"/process\")\nasync def process_endpoint(data: dict):\n    # Use nanobricks for processing\n    validated = await validator.invoke(data)\n    result = await processor.invoke(validated)\n    return result",
    "crumbs": [
      "Home",
      "Building Systems",
      "Framework Comparison"
    ]
  },
  {
    "objectID": "comparison.html#migration-guide",
    "href": "comparison.html#migration-guide",
    "title": "Framework Comparison",
    "section": "Migration Guide",
    "text": "Migration Guide\n\nFrom Flask to Nanobricks\n\n# Before: Flask route with mixed concerns\n@app.route('/analyze', methods=['POST'])\ndef analyze():\n    data = request.get_json()\n    \n    # Validation mixed with logic\n    if not data.get('text'):\n        return jsonify({'error': 'Text required'}), 400\n    \n    # Processing mixed with web handling\n    result = expensive_nlp_analysis(data['text'])\n    cache.set(f\"analysis:{data['text']}\", result)\n    \n    return jsonify(result)\n\n# After: Nanobricks with separation\nclass ValidateText(NanobrickSimple[dict, str]):\n    async def invoke(self, data: dict, *, deps=None) -&gt; str:\n        if not data.get('text'):\n            raise ValueError('Text required')\n        return data['text']\n\nclass AnalyzeText(NanobrickSimple[str, dict]):\n    async def invoke(self, text: str, *, deps=None) -&gt; dict:\n        return await expensive_nlp_analysis(text)\n\n# Compose with caching\nanalyze_pipeline = (\n    ValidateText()\n    | AnalyzeText().with_skill(\"cache\", ttl=3600)\n)\n\n# Add web interface\nanalyze_service = analyze_pipeline.with_skill(\"api\", \n    path=\"/analyze\",\n    method=\"POST\"\n)\n\n\n\nFrom Celery to Nanobricks\n\n# Before: Celery with chained tasks\n@celery.task\ndef fetch_data(url):\n    return requests.get(url).json()\n\n@celery.task \ndef process_data(data):\n    return transform(data)\n\n@celery.task\ndef save_data(data):\n    db.save(data)\n\n# Usage\nchain(fetch_data.s(url), process_data.s(), save_data.s())()\n\n# After: Nanobricks pipeline\npipeline = (\n    FetchData()\n    | ProcessData()\n    | SaveData()\n)\n\n# Run locally or distributed\nresult = await pipeline.invoke(url)\n\n# Or with queue skill\nqueued = pipeline.with_skill(\"queue\", backend=\"redis\")",
    "crumbs": [
      "Home",
      "Building Systems",
      "Framework Comparison"
    ]
  },
  {
    "objectID": "comparison.html#performance-comparison",
    "href": "comparison.html#performance-comparison",
    "title": "Framework Comparison",
    "section": "Performance Comparison",
    "text": "Performance Comparison\n\nOverhead Benchmarks\n\n\n\nOperation\nNanobricks\nFlask\nFastAPI\nCelery\n\n\n\n\nHello World\n0.05ms\n0.8ms\n0.3ms\nN/A\n\n\nSimple Pipeline (3 steps)\n0.15ms\nN/A\nN/A\n50ms+\n\n\nWith Caching\n0.01ms\nManual\nManual\nManual\n\n\nType Validation\nBuilt-in\nManual\nPydantic\nNone\n\n\n\n\n\nScalability Patterns\n\n# Nanobricks scales horizontally with skills\n\n# Start simple\npipeline = ProcessData()\n\n# Add caching when needed\ncached = pipeline.with_skill(\"cache\")\n\n# Scale horizontally\ndistributed = pipeline.with_skill(\"distributed\", workers=10)\n\n# Add monitoring in production\nproduction = pipeline.with_skill(\"monitoring\").with_skill(\"tracing\")",
    "crumbs": [
      "Home",
      "Building Systems",
      "Framework Comparison"
    ]
  },
  {
    "objectID": "comparison.html#ecosystem-integration",
    "href": "comparison.html#ecosystem-integration",
    "title": "Framework Comparison",
    "section": "Ecosystem Integration",
    "text": "Ecosystem Integration\n\nDatabase ORMs\n\n# Works with any ORM\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nclass UserRepository(NanobrickBase[str, User, dict]):\n    async def invoke(self, user_id: str, *, deps=None) -&gt; User:\n        async with AsyncSession(deps['db']) as session:\n            return await session.get(User, user_id)\n\n\n\nMessage Queues\n\n# Integrates with any message queue\nfrom aiokafka import AIOKafkaConsumer\n\nconsumer_pipeline = (\n    ParseMessage()\n    | ValidateMessage()\n    | ProcessMessage()\n).with_skill(\"monitoring\")\n\nasync for msg in consumer:\n    await consumer_pipeline.invoke(msg.value)\n\n\n\nCloud Services\n\n# Cloud-native deployment\naws_pipeline = pipeline.with_skill(\"aws\", {\n    \"lambda\": True,\n    \"api_gateway\": True,\n    \"dynamodb\": True\n})\n\ngcp_pipeline = pipeline.with_skill(\"gcp\", {\n    \"cloud_run\": True,\n    \"firestore\": True\n})",
    "crumbs": [
      "Home",
      "Building Systems",
      "Framework Comparison"
    ]
  },
  {
    "objectID": "comparison.html#summary",
    "href": "comparison.html#summary",
    "title": "Framework Comparison",
    "section": "Summary",
    "text": "Summary\nNanobricks offers a unique approach to building Python systems:\n\nUniversal Composition: Everything composes with the same interface\nProgressive Enhancement: Start simple, add capabilities as needed\n\nType Safety: Full type inference through entire systems\nProduction Ready: Built-in features for real-world deployment\nFramework Agnostic: Use alone or integrate with existing tools\n\nChoose Nanobricks when you want to build maintainable, scalable systems from simple, composable parts. It’s not a replacement for all frameworks, but a new way to think about structuring Python applications.\nFor more examples and patterns, see the SDK Guide and Production Examples.",
    "crumbs": [
      "Home",
      "Building Systems",
      "Framework Comparison"
    ]
  },
  {
    "objectID": "naming-conventions.html",
    "href": "naming-conventions.html",
    "title": "Naming Conventions",
    "section": "",
    "text": "We use an &lt;entity&gt;:&lt;variant&gt; or &lt;component&gt;:&lt;action&gt; naming convention throughout the Nanobricks ecosystem.",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Naming Conventions"
    ]
  },
  {
    "objectID": "naming-conventions.html#core-principle-entity-first",
    "href": "naming-conventions.html#core-principle-entity-first",
    "title": "Naming Conventions",
    "section": "",
    "text": "We use an &lt;entity&gt;:&lt;variant&gt; or &lt;component&gt;:&lt;action&gt; naming convention throughout the Nanobricks ecosystem.",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Naming Conventions"
    ]
  },
  {
    "objectID": "naming-conventions.html#the-pattern",
    "href": "naming-conventions.html#the-pattern",
    "title": "Naming Conventions",
    "section": "The Pattern",
    "text": "The Pattern\n&lt;entity&gt;:&lt;variant&gt;\n&lt;component&gt;:&lt;action&gt;\n\nEntity/Component comes FIRST (the main concept)\nVariant/Action comes SECOND (the specific type or operation)",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Naming Conventions"
    ]
  },
  {
    "objectID": "naming-conventions.html#examples-across-the-stack",
    "href": "naming-conventions.html#examples-across-the-stack",
    "title": "Naming Conventions",
    "section": "Examples Across the Stack",
    "text": "Examples Across the Stack\n\nPython Classes\n\n# ✅ Correct: Entity-first\nclass SkillAPI:      # Skill is the entity, API is the variant\nclass SkillCLI:      \nclass SkillUI:       \nclass SkillDB:       \nclass SkillAI:       \n\nclass NanobrickValidator:\nclass NanobrickTransformer:\nclass NanobrickPersistor:\n\nclass ValidatorEmail:     # Validator entity, Email variant\nclass ValidatorPhone:\nclass ValidatorAddress:\n\n\n# ❌ Incorrect: Variant-first\nclass SkillAI:\nclass SkillCLI:\nclass ValidatorEmail:\n\n\n\nFunctions and Methods\n\n# ✅ Correct\ndef nanobrick_create(name: str) -&gt; Nanobrick:\ndef nanobrick_compose(bricks: List[Nanobrick]) -&gt; Nanobrick:\ndef nanobrick_validate(brick: Nanobrick) -&gt; bool:\n\ndef skill_activate(power: Skill) -&gt; None:\ndef skill_deactivate(power: Skill) -&gt; None:\n\n\n\nAPI Endpoints\n\n# ✅ Correct: RESTful with entity-first\n@app.post(\"/nanobrick/create\")\n@app.get(\"/nanobrick/list\")\n@app.get(\"/nanobrick/{id}\")\n@app.put(\"/nanobrick/{id}/compose\")\n\n@app.post(\"/skill/activate\")\n@app.get(\"/skill/available\")\n\n\n\nCLI Commands\n\n# ✅ Correct: Entity as command, action as subcommand\n$ nanobrick create my-validator\n$ nanobrick list\n$ nanobrick compose pipeline.yaml\n\n$ skill add api\n$ skill remove cli\n$ skill list\n\n\n\nGo Tasks (Taskfile.yml)\n\n# ✅ Correct: Entity:action format\ntasks:\n  # Documentation tasks\n  docs:render:\n  docs:preview:\n  docs:publish:\n  docs:clean:\n  \n  # Nanobrick tasks\n  brick:new:\n  brick:test:\n  brick:lint:\n  brick:publish:\n  \n  # Quarto specific\n  quarto:render:\n  quarto:check:\n  \n  # Development\n  dev:setup:\n  dev:test:\n  dev:lint:\n\n\n\nFile Names\n\n# ✅ Correct\nskill_api.py\nskill_cli.py\nskill_ui.py\nskill_db.py\nskill_ai.py\n\nnanobrick_core.py\nnanobrick_protocol.py\nnanobrick_composition.py\n\nvalidator_email.py\nvalidator_phone.py\ntransformer_json.py\ntransformer_xml.py",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Naming Conventions"
    ]
  },
  {
    "objectID": "naming-conventions.html#benefits",
    "href": "naming-conventions.html#benefits",
    "title": "Naming Conventions",
    "section": "Benefits",
    "text": "Benefits\n\n1. Natural Grouping\nFiles, classes, and functions group together alphabetically by entity:\nnanobrick_compose()\nnanobrick_create()\nnanobrick_validate()\n\n\n2. Easy Discovery\nFind all skill-related code instantly:\nSkillAPI\nSkillCLI\nSkillUI\nskill_activate()\nskill_list()\n\n\n3. Consistent Mental Model\nThe same pattern works everywhere: - Classes: EntityVariant - Functions: entity_action() - CLI: entity action - API: /entity/action - Tasks: entity:action\n\n\n4. Scalability\nAs the project grows, related concepts stay together:\nValidatorEmail\nValidatorPhone\nValidatorAddress\nValidatorCreditCard\nValidatorSSN",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Naming Conventions"
    ]
  },
  {
    "objectID": "naming-conventions.html#implementation-guidelines",
    "href": "naming-conventions.html#implementation-guidelines",
    "title": "Naming Conventions",
    "section": "Implementation Guidelines",
    "text": "Implementation Guidelines\n\nNew Code: Always follow entity-first pattern\nRefactoring: Update naming when modifying existing code\nReviews: Enforce in code reviews\nAutomation: Consider linting rules for enforcement\nDocumentation: Update examples to follow convention",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Naming Conventions"
    ]
  },
  {
    "objectID": "naming-conventions.html#exceptions",
    "href": "naming-conventions.html#exceptions",
    "title": "Naming Conventions",
    "section": "Exceptions",
    "text": "Exceptions\nRare exceptions may exist for: - External API compatibility - Third-party library constraints - Legacy system integration\nDocument any exceptions clearly with rationale.",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Naming Conventions"
    ]
  },
  {
    "objectID": "naming-conventions.html#quick-reference",
    "href": "naming-conventions.html#quick-reference",
    "title": "Naming Conventions",
    "section": "Quick Reference",
    "text": "Quick Reference\n\n\n\nContext\nPattern\nExample\n\n\n\n\nClass\nEntityVariant\nSkillAPI\n\n\nFunction\nentity_action()\nnanobrick_create()\n\n\nMethod\nentity_action()\nself.validator_check()\n\n\nCLI\nentity action\nnanobrick create\n\n\nAPI\n/entity/action\n/nanobrick/create\n\n\nTask\nentity:action\ndocs:render\n\n\nFile\nentity_variant.py\nskill_api.py",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Naming Conventions"
    ]
  },
  {
    "objectID": "tutorial.html",
    "href": "tutorial.html",
    "title": "Nanobricks Tutorial",
    "section": "",
    "text": "This tutorial will teach you how to build production-ready Python systems using nanobricks. We’ll start with the basics and work our way up to complex applications.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Nanobricks Tutorial"
    ]
  },
  {
    "objectID": "tutorial.html#introduction",
    "href": "tutorial.html#introduction",
    "title": "Nanobricks Tutorial",
    "section": "",
    "text": "This tutorial will teach you how to build production-ready Python systems using nanobricks. We’ll start with the basics and work our way up to complex applications.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Nanobricks Tutorial"
    ]
  },
  {
    "objectID": "tutorial.html#core-concepts",
    "href": "tutorial.html#core-concepts",
    "title": "Nanobricks Tutorial",
    "section": "Core Concepts",
    "text": "Core Concepts\n\nWhat is a Nanobrick?\nA nanobrick is: - Atomic: Does one thing well - Self-contained: No hidden dependencies - Composable: Combines with other bricks - Type-safe: Full type inference - Async-first: Built for modern Python\n\n\nThe Nanobrick Protocol\nEvery nanobrick implements this protocol:\n\nfrom typing import Protocol, TypeVar, Generic\n\nT_in = TypeVar('T_in')\nT_out = TypeVar('T_out')\nT_deps = TypeVar('T_deps')\n\nclass NanobrickProtocol(Protocol, Generic[T_in, T_out, T_deps]):\n    name: str\n    version: str\n    \n    async def invoke(self, input: T_in, *, deps: T_deps = None) -&gt; T_out: ...\n    def invoke_sync(self, input: T_in, *, deps: T_deps = None) -&gt; T_out: ...",
    "crumbs": [
      "Home",
      "Getting Started",
      "Nanobricks Tutorial"
    ]
  },
  {
    "objectID": "tutorial.html#building-your-first-nanobrick",
    "href": "tutorial.html#building-your-first-nanobrick",
    "title": "Nanobricks Tutorial",
    "section": "Building Your First Nanobrick",
    "text": "Building Your First Nanobrick\n\nStep 1: Basic Implementation\n\nfrom nanobricks import NanobrickBase\n\nclass TemperatureConverter(NanobrickBase[float, dict, None]):\n    \"\"\"Converts Celsius to Fahrenheit and Kelvin.\"\"\"\n    \n    def __init__(self):\n        self.name = \"temperature_converter\"\n        self.version = \"1.0.0\"\n    \n    async def invoke(self, celsius: float, *, deps=None) -&gt; dict:\n        return {\n            \"celsius\": celsius,\n            \"fahrenheit\": (celsius * 9/5) + 32,\n            \"kelvin\": celsius + 273.15\n        }\n\n# Use it\nconverter = TemperatureConverter()\nresult = await converter.invoke(25.0)\nprint(result)\n# {'celsius': 25.0, 'fahrenheit': 77.0, 'kelvin': 298.15}\n\n\n\nStep 2: Using NanobrickSimple\nFor bricks without dependencies, use NanobrickSimple:\n\nfrom nanobricks import NanobrickSimple\n\nclass WordCounter(NanobrickSimple[str, dict]):\n    \"\"\"Counts words and characters in text.\"\"\"\n    \n    async def invoke(self, text: str, *, deps=None) -&gt; dict:\n        words = text.split()\n        return {\n            \"text\": text,\n            \"word_count\": len(words),\n            \"char_count\": len(text),\n            \"unique_words\": len(set(words))\n        }\n\ncounter = WordCounter()\nstats = await counter.invoke(\"The quick brown fox jumps over the lazy dog\")",
    "crumbs": [
      "Home",
      "Getting Started",
      "Nanobricks Tutorial"
    ]
  },
  {
    "objectID": "tutorial.html#composition-patterns",
    "href": "tutorial.html#composition-patterns",
    "title": "Nanobricks Tutorial",
    "section": "Composition Patterns",
    "text": "Composition Patterns\n\nLinear Pipelines\nThe most common pattern is linear composition:\n\nclass TextCleaner(NanobrickSimple[str, str]):\n    async def invoke(self, text: str, *, deps=None) -&gt; str:\n        return \" \".join(text.split())  # Remove extra whitespace\n\nclass TextLowercaser(NanobrickSimple[str, str]):\n    async def invoke(self, text: str, *, deps=None) -&gt; str:\n        return text.lower()\n\nclass TextTokenizer(NanobrickSimple[str, list]):\n    async def invoke(self, text: str, *, deps=None) -&gt; list:\n        return text.split()\n\n# Compose them\ntext_pipeline = TextCleaner() | TextLowercaser() | TextTokenizer()\n\ntokens = await text_pipeline.invoke(\"  Hello   WORLD  \")\n# ['hello', 'world']\n\n\n\nBranching Pipelines\nUse the Branch pattern for conditional logic:\n\nfrom nanobricks.patterns import Branch\n\nclass IsAdult(NanobrickSimple[dict, bool]):\n    async def invoke(self, person: dict, *, deps=None) -&gt; bool:\n        return person.get(\"age\", 0) &gt;= 18\n\nadult_pipeline = ProcessAdult() | SendWelcomeEmail()\nminor_pipeline = ProcessMinor() | NotifyGuardian()\n\nage_router = Branch(\n    condition=IsAdult(),\n    true_branch=adult_pipeline,\n    false_branch=minor_pipeline\n)\n\nresult = await age_router.invoke({\"name\": \"Alice\", \"age\": 25})\n\n\n\nParallel Execution\nProcess multiple items concurrently:\n\nfrom nanobricks.patterns import Parallel\n\n# Run multiple processors in parallel\nparallel_processor = Parallel([\n    CPUIntensiveTask(),\n    NetworkAPICall(),\n    DatabaseQuery()\n])\n\nresults = await parallel_processor.invoke(data)\n# Returns: [cpu_result, api_result, db_result]",
    "crumbs": [
      "Home",
      "Getting Started",
      "Nanobricks Tutorial"
    ]
  },
  {
    "objectID": "tutorial.html#working-with-dependencies",
    "href": "tutorial.html#working-with-dependencies",
    "title": "Nanobricks Tutorial",
    "section": "Working with Dependencies",
    "text": "Working with Dependencies\n\nDefining Dependencies\nUse TypedDict for type-safe dependencies:\n\nfrom typing import TypedDict\nfrom nanobricks import NanobrickBase\n\nclass AppDeps(TypedDict):\n    db: DatabaseConnection\n    cache: CacheClient\n    logger: Logger\n    config: dict\n\nclass UserLoader(NanobrickBase[str, dict, AppDeps]):\n    \"\"\"Loads user data with caching.\"\"\"\n    \n    async def invoke(self, user_id: str, *, deps: AppDeps) -&gt; dict:\n        # Check cache first\n        cache_key = f\"user:{user_id}\"\n        cached = await deps[\"cache\"].get(cache_key)\n        if cached:\n            deps[\"logger\"].info(f\"Cache hit for {user_id}\")\n            return cached\n        \n        # Load from database\n        user = await deps[\"db\"].fetch_one(\n            \"SELECT * FROM users WHERE id = ?\", user_id\n        )\n        \n        # Cache for next time\n        await deps[\"cache\"].set(cache_key, user, ttl=300)\n        deps[\"logger\"].info(f\"Loaded user {user_id} from DB\")\n        \n        return user\n\n\n\nDependency Injection\nDependencies flow through entire pipelines:\n\nfrom nanobricks.dependencies import DependencyContainer\n\n# Create dependencies\ndeps = DependencyContainer(\n    db=database_connection,\n    cache=redis_client,\n    logger=logger,\n    config={\"environment\": \"production\"}\n)\n\n# All bricks in pipeline receive deps\npipeline = ValidateRequest() | UserLoader() | EnrichUser() | SaveUser()\nresult = await pipeline.invoke(request, deps=deps.to_dict())",
    "crumbs": [
      "Home",
      "Getting Started",
      "Nanobricks Tutorial"
    ]
  },
  {
    "objectID": "tutorial.html#error-handling",
    "href": "tutorial.html#error-handling",
    "title": "Nanobricks Tutorial",
    "section": "Error Handling",
    "text": "Error Handling\n\nFail-Fast Default\nBy default, errors propagate immediately:\n\nclass StrictValidator(NanobrickSimple[dict, dict]):\n    async def invoke(self, data: dict, *, deps=None) -&gt; dict:\n        if \"email\" not in data:\n            raise ValueError(\"Email is required\")\n        if \"@\" not in data[\"email\"]:\n            raise ValueError(\"Invalid email format\")\n        return data\n\n# This will raise ValueError if validation fails\ntry:\n    result = await StrictValidator().invoke({\"name\": \"Alice\"})\nexcept ValueError as e:\n    print(f\"Validation error: {e}\")\n\n\n\nError Recovery\nUse fallback patterns for resilience:\n\nfrom nanobricks.patterns import Fallback\n\nclass UnreliableAPICall(NanobrickSimple[str, dict]):\n    async def invoke(self, query: str, *, deps=None) -&gt; dict:\n        # This might fail\n        response = await external_api.search(query)\n        return response\n\nclass CachedResults(NanobrickSimple[str, dict]):\n    async def invoke(self, query: str, *, deps=None) -&gt; dict:\n        # Return cached/default results\n        return {\"results\": [], \"cached\": True}\n\n# Try API first, fall back to cache\nsafe_search = Fallback(\n    primary=UnreliableAPICall(),\n    fallback=CachedResults()\n)\n\nresult = await safe_search.invoke(\"python nanobricks\")",
    "crumbs": [
      "Home",
      "Getting Started",
      "Nanobricks Tutorial"
    ]
  },
  {
    "objectID": "tutorial.html#using-built-in-components",
    "href": "tutorial.html#using-built-in-components",
    "title": "Nanobricks Tutorial",
    "section": "Using Built-in Components",
    "text": "Using Built-in Components\n\nValidators\n\nfrom nanobricks.validators import (\n    EmailValidator,\n    RangeValidator,\n    TypeValidator,\n    SchemaValidator\n)\n\n# Email validation\nemail_validator = EmailValidator()\nvalid_email = await email_validator.invoke(\"user@example.com\")\n\n# Range validation\nage_validator = RangeValidator(min_value=0, max_value=150)\nvalid_age = await age_validator.invoke(25)\n\n# Schema validation\nuser_schema = {\n    \"name\": str,\n    \"age\": int,\n    \"email\": str,\n    \"active\": bool\n}\nschema_validator = SchemaValidator(schema=user_schema)\nvalid_user = await schema_validator.invoke({\n    \"name\": \"Alice\",\n    \"age\": 30,\n    \"email\": \"alice@example.com\",\n    \"active\": True\n})\n\n\n\nTransformers\n\nfrom nanobricks.transformers import (\n    JSONTransformer,\n    CSVParser,\n    TypeConverter,\n    TextNormalizer\n)\n\n# Parse JSON\njson_parser = JSONTransformer()\ndata = await json_parser.invoke('{\"name\": \"Alice\", \"age\": 30}')\n\n# Parse CSV\ncsv_parser = CSVParser(has_header=True)\nrows = await csv_parser.invoke(\"name,age\\nAlice,30\\nBob,25\")\n\n# Convert types\nconverter = TypeConverter(target_type=int)\nnumber = await converter.invoke(\"42\")\n\n# Normalize text\nnormalizer = TextNormalizer(\n    lowercase=True,\n    remove_punctuation=True,\n    remove_extra_spaces=True\n)\nclean_text = await normalizer.invoke(\"Hello,  WORLD!!!\")",
    "crumbs": [
      "Home",
      "Getting Started",
      "Nanobricks Tutorial"
    ]
  },
  {
    "objectID": "tutorial.html#adding-skills",
    "href": "tutorial.html#adding-skills",
    "title": "Nanobricks Tutorial",
    "section": "Adding Skills",
    "text": "Adding Skills\n\nLogging Skill\n\n# Method 1: Decorator\nfrom nanobricks import skill\n\n@skill(\"logging\", level=\"INFO\")\nclass DataProcessor(NanobrickSimple[dict, dict]):\n    async def invoke(self, data: dict, *, deps=None) -&gt; dict:\n        # Processing happens here\n        return {\"processed\": True, **data}\n\n# Method 2: Runtime\nprocessor = DataProcessor()\nlogged_processor = processor.with_skill(\"logging\", level=\"DEBUG\")\n\n\n\nAPI Skill\n\n@skill(\"api\", port=8000, path=\"/process\")\nclass APIProcessor(NanobrickSimple[dict, dict]):\n    async def invoke(self, data: dict, *, deps=None) -&gt; dict:\n        return {\"status\": \"processed\", \"data\": data}\n\nprocessor = APIProcessor()\nprocessor.start_server()  # Now available via HTTP\n\n\n\nMultiple Skills\n\n@skill(\"logging\")\n@skill(\"api\", port=8080)\n@skill(\"observability\")\nclass ProductionService(NanobrickSimple[dict, dict]):\n    \"\"\"A production-ready service with multiple skills.\"\"\"\n    \n    async def invoke(self, request: dict, *, deps=None) -&gt; dict:\n        # Your business logic here\n        return {\"response\": \"ok\"}",
    "crumbs": [
      "Home",
      "Getting Started",
      "Nanobricks Tutorial"
    ]
  },
  {
    "objectID": "tutorial.html#testing-your-nanobricks",
    "href": "tutorial.html#testing-your-nanobricks",
    "title": "Nanobricks Tutorial",
    "section": "Testing Your Nanobricks",
    "text": "Testing Your Nanobricks\n\nUnit Testing\n\nimport pytest\nfrom myapp.bricks import DataValidator\n\n@pytest.mark.asyncio\nasync def test_validator():\n    validator = DataValidator()\n    \n    # Test valid input\n    result = await validator.invoke({\"email\": \"test@example.com\"})\n    assert result[\"email\"] == \"test@example.com\"\n    \n    # Test invalid input\n    with pytest.raises(ValueError):\n        await validator.invoke({\"email\": \"invalid\"})\n\n\n\nTesting Pipelines\n\n@pytest.mark.asyncio\nasync def test_pipeline():\n    pipeline = Loader() | Validator() | Transformer()\n    \n    result = await pipeline.invoke(\"input_data\")\n    assert result[\"status\"] == \"transformed\"\n\n\n\nTesting with Dependencies\n\nfrom nanobricks.dependencies import MockDatabase, MockCache\n\n@pytest.mark.asyncio\nasync def test_with_deps():\n    mock_deps = {\n        \"db\": MockDatabase({\"users\": [{\"id\": 1, \"name\": \"Test\"}]}),\n        \"cache\": MockCache()\n    }\n    \n    loader = UserLoader()\n    user = await loader.invoke(\"1\", deps=mock_deps)\n    assert user[\"name\"] == \"Test\"",
    "crumbs": [
      "Home",
      "Getting Started",
      "Nanobricks Tutorial"
    ]
  },
  {
    "objectID": "tutorial.html#configuration",
    "href": "tutorial.html#configuration",
    "title": "Nanobricks Tutorial",
    "section": "Configuration",
    "text": "Configuration\n\nUsing TOML Configuration\nCreate nanobrick.toml:\n[project]\nname = \"my-app\"\nversion = \"1.0.0\"\n\n[logging]\nlevel = \"INFO\"\nformat = \"json\"\n\n[database]\nurl = \"postgresql://localhost/myapp\"\npool_size = 10\n\n[features]\ncache_enabled = true\nrate_limiting = true\nLoad in your bricks:\n\nfrom nanobricks.config import get_default_config\n\nclass ConfigurableService(NanobrickSimple[dict, dict]):\n    def __init__(self):\n        super().__init__()\n        self.config = get_default_config()\n    \n    async def invoke(self, data: dict, *, deps=None) -&gt; dict:\n        if self.config.features.cache_enabled:\n            # Use caching logic\n            pass\n        \n        return data",
    "crumbs": [
      "Home",
      "Getting Started",
      "Nanobricks Tutorial"
    ]
  },
  {
    "objectID": "tutorial.html#next-steps",
    "href": "tutorial.html#next-steps",
    "title": "Nanobricks Tutorial",
    "section": "Next Steps",
    "text": "Next Steps\nNow that you understand the basics:\n\nBuild Larger Systems: Check out the SDK Guide\nLearn Patterns: Read the Design Patterns guide\nDeploy to Production: See the Production Guide\nExplore Examples: Browse our example repository",
    "crumbs": [
      "Home",
      "Getting Started",
      "Nanobricks Tutorial"
    ]
  },
  {
    "objectID": "tutorial.html#quick-reference",
    "href": "tutorial.html#quick-reference",
    "title": "Nanobricks Tutorial",
    "section": "Quick Reference",
    "text": "Quick Reference\n\nCreating Bricks\n\n# Simple brick (no deps)\nclass MyBrick(NanobrickSimple[InputType, OutputType]):\n    async def invoke(self, input: InputType, *, deps=None) -&gt; OutputType:\n        pass\n\n# With dependencies\nclass MyBrick(NanobrickBase[InputType, OutputType, DepsType]):\n    async def invoke(self, input: InputType, *, deps: DepsType) -&gt; OutputType:\n        pass\n\n\n\nComposition\n\n# Linear\npipeline = A() | B() | C()\n\n# Branching\nbranch = Branch(condition, true_path, false_path)\n\n# Parallel\nparallel = Parallel([A(), B(), C()])\n\n# Fallback\nsafe = Fallback(primary, backup)\n\n\n\nSkills\n\n# Add at runtime\nbrick.with_skill(\"logging\")\nbrick.with_skill(\"api\", port=8000)\nbrick.with_skill(\"cache\", ttl=300)\n\n# Add with decorator\n@skill(\"logging\")\n@skill(\"api\")\nclass MyBrick(NanobrickSimple[In, Out]):\n    pass",
    "crumbs": [
      "Home",
      "Getting Started",
      "Nanobricks Tutorial"
    ]
  },
  {
    "objectID": "principles.html",
    "href": "principles.html",
    "title": "The Ten Commandments of Nanobricks",
    "section": "",
    "text": "The Nanobricks framework is built on ten fundamental principles that shape every design decision:\n\n\nComplexity emerges from composition, not components. Each nanobrick should do one thing well, with a clear mental model that both humans and AIs can reason about effortlessly.\n\n\n\nOne interface to rule them all. The consistent invoke() pattern and pipe operator create a universal language for composition.\n\n\n\nPlay well with others. Nanobricks combine like Lego blocks - any output can connect to any compatible input, creating endless possibilities.\n\n\n\nCarry what you need. Through skills, nanobricks bring their own batteries - but only activate them when needed.\n\n\n\nWork out of the box. New nanobricks should be productive immediately, with sensible defaults and minimal configuration.\n\n\n\nShow what you’re doing. Built-in tracing, logging, and metrics make debugging a breeze, not a nightmare.\n\n\n\nFail gracefully, recover automatically. Embrace the chaos of distributed systems with circuit breakers, retries, and fallbacks.\n\n\n\nAdapt without changing. TOML configuration allows behavior changes across environments without touching code.\n\n\n\nImprove over time. Through hot-swapping, versioning, and AI-powered adaptation, nanobricks get better with use.\n\n\n\nSafe by default. Input validation, sandboxing, and the principle of least privilege are built into the foundation.",
    "crumbs": [
      "Home",
      "Core Concepts",
      "The Ten Commandments of Nanobricks"
    ]
  },
  {
    "objectID": "principles.html#core-principles",
    "href": "principles.html#core-principles",
    "title": "The Ten Commandments of Nanobricks",
    "section": "",
    "text": "The Nanobricks framework is built on ten fundamental principles that shape every design decision:\n\n\nComplexity emerges from composition, not components. Each nanobrick should do one thing well, with a clear mental model that both humans and AIs can reason about effortlessly.\n\n\n\nOne interface to rule them all. The consistent invoke() pattern and pipe operator create a universal language for composition.\n\n\n\nPlay well with others. Nanobricks combine like Lego blocks - any output can connect to any compatible input, creating endless possibilities.\n\n\n\nCarry what you need. Through skills, nanobricks bring their own batteries - but only activate them when needed.\n\n\n\nWork out of the box. New nanobricks should be productive immediately, with sensible defaults and minimal configuration.\n\n\n\nShow what you’re doing. Built-in tracing, logging, and metrics make debugging a breeze, not a nightmare.\n\n\n\nFail gracefully, recover automatically. Embrace the chaos of distributed systems with circuit breakers, retries, and fallbacks.\n\n\n\nAdapt without changing. TOML configuration allows behavior changes across environments without touching code.\n\n\n\nImprove over time. Through hot-swapping, versioning, and AI-powered adaptation, nanobricks get better with use.\n\n\n\nSafe by default. Input validation, sandboxing, and the principle of least privilege are built into the foundation.",
    "crumbs": [
      "Home",
      "Core Concepts",
      "The Ten Commandments of Nanobricks"
    ]
  },
  {
    "objectID": "principles.html#principle-hierarchy",
    "href": "principles.html#principle-hierarchy",
    "title": "The Ten Commandments of Nanobricks",
    "section": "Principle Hierarchy",
    "text": "Principle Hierarchy\n\nThe Original Five (Core DNA)\n\nSimple\nStandardized\nComposable\nBatteries Included (Self-Sufficient)\nScaffoldable\n\nThese form the essential nature of what makes a nanobrick a nanobrick.\n\n\nThe Extended Five (Distinguishing Features)\n\nObservable\nResilient\nConfigurable\nEvolutionary\nSecure\n\nThese elevate nanobricks from simple components to production-ready, antifragile systems.",
    "crumbs": [
      "Home",
      "Core Concepts",
      "The Ten Commandments of Nanobricks"
    ]
  },
  {
    "objectID": "principles.html#living-by-the-principles",
    "href": "principles.html#living-by-the-principles",
    "title": "The Ten Commandments of Nanobricks",
    "section": "Living by the Principles",
    "text": "Living by the Principles\n\nIn Design\n\nEvery new feature must support, not violate, these principles\nWhen principles conflict, simplicity wins\nThe principles guide us toward antifragility\n\n\n\nIn Implementation\n# Simple: One clear purpose\n@nanobrick\nclass UppercaseTransformer:\n    async def invoke(self, text: str) -&gt; str:\n        return text.upper()\n\n# Composable: Works with others\npipeline = ValidateText() | UppercaseTransformer() | SaveToFile()\n\n# Observable: Built-in visibility\ntransformer = UppercaseTransformer().with_skill(\"observability\")\n\n# Resilient: Handles failures\nsafe_transformer = UppercaseTransformer().with_fallback(lambda x: x)\n\n# Configurable: Adapts to needs\nconfig_aware = UppercaseTransformer(config=\"nanobrick.toml\")\n\n\nIn Evolution\nThe principles themselves can evolve, but only through community consensus and real-world validation. They are our constitution, not our chains.",
    "crumbs": [
      "Home",
      "Core Concepts",
      "The Ten Commandments of Nanobricks"
    ]
  },
  {
    "objectID": "principles.html#the-path-to-antifragility",
    "href": "principles.html#the-path-to-antifragility",
    "title": "The Ten Commandments of Nanobricks",
    "section": "The Path to Antifragility",
    "text": "The Path to Antifragility\nThese ten principles, when followed together, create the conditions for antifragility:\n\nSimplicity enables understanding and adaptation\nStandardization allows universal composition\nComposability creates emergent complexity\nSelf-sufficiency ensures independence\nScaffoldability accelerates adoption\nObservability provides feedback loops\nResilience handles stress gracefully\nConfigurability enables runtime adaptation\nEvolution drives continuous improvement\nSecurity maintains system integrity\n\nTogether, they form a framework that doesn’t just survive stress - it thrives on it.",
    "crumbs": [
      "Home",
      "Core Concepts",
      "The Ten Commandments of Nanobricks"
    ]
  },
  {
    "objectID": "v1/concept.html",
    "href": "v1/concept.html",
    "title": "Nanobricks v1.0",
    "section": "",
    "text": "Nanobricks are the code equivalent of “antifragile nanobots” — atomic, self-sufficient components that gain strength from stress and compose organically into complex systems.\n\n\n\n\n\n\nuv-based: Every nanobrick is a Python package managed by uv\nsrc-layout: Following the src-based project layout standard\nDirectory-based modules: Top-level modules as directories, sub-modules as files\nAbsolute imports: Always use absolute imports for clarity and consistency\n\n\n\n\n\n\n\n\nDesigned for clarity and straightforward implementation\nEasy for both humans and AIs to reason about\nSingle responsibility principle at the atomic level\nMinimal cognitive overhead\n\n\n\n\n\nConsistent interfaces — the “Lego Connector Mechanism” for code\nPredictable behavior patterns\nUniversal protocols for:\n\nInput/Output contracts\nConfiguration management\nError handling\nLifecycle hooks\n\n\n\n\n\n\nSeamless integration patterns\nPipeline-ready (can be chained/piped together)\nSupports multiple composition patterns:\n\nSequential (A → B → C)\nParallel (A + B + C)\nNested (A(B(C)))\nHybrid workflows\n\n\n\n\n\nEach nanobrick ships with modular, self-contained interfaces: - API Layer (FastAPI) — RESTful endpoints auto-generated - CLI Layer (Typer) — Command-line interface out of the box - Frontend Layer (Streamlit) — UI components (app/page/tab/subtab) - Data Layer (SQLModel) — Database interaction when needed\n\n\n\n\nInstant end-to-end functionality\nRails-inspired convention over configuration\ngo-task powered automation\nAI-friendly cursorrules for guided implementation\nProgressive enhancement model\n\n\n\n\n\n\n\nBased on analysis of successful frameworks, nanobricks will leverage:\n\nRunnable Interface Pattern (from LangChain)\n\nStandardized methods: invoke(), batch(), stream()\nEnables uniform interaction across all components\n\nComposition Pattern with Pipe Operator (from LangChain’s LCEL)\n\nUse | operator for intuitive chaining\nDeclarative pipeline construction\n\nDependency Injection (from PydanticAI)\n\nContext-based dependency passing\nEnhances testability and flexibility\n\nDecorator Pattern (from PydanticAI)\n\n@nanobrick decorator for component registration\nClean, pythonic API\n\nGeneric Programming (from PydanticAI)\n\nType-safe interfaces using Python generics\nCompile-time type checking\n\n\n\n\n\nfrom typing import Protocol, TypeVar, Generic, Any\nfrom abc import abstractmethod\n\nInputT = TypeVar('InputT')\nOutputT = TypeVar('OutputT')\nDepsT = TypeVar('DepsT')\n\nclass Nanobrick(Protocol, Generic[InputT, OutputT, DepsT]):\n    \"\"\"Universal interface for all nanobricks - inspired by LangChain's Runnable\"\"\"\n    \n    # Core identity\n    name: str\n    version: str\n    \n    # Primary execution methods (Runnable pattern)\n    @abstractmethod\n    async def invoke(self, input: InputT, *, deps: DepsT = None) -&gt; OutputT:\n        \"\"\"Async execution with dependency injection\"\"\"\n        ...\n    \n    def batch(self, inputs: list[InputT], *, deps: DepsT = None) -&gt; list[OutputT]:\n        \"\"\"Batch processing\"\"\"\n        ...\n    \n    async def stream(self, input: InputT, *, deps: DepsT = None):\n        \"\"\"Streaming execution\"\"\"\n        ...\n    \n    # Composition support (LCEL-inspired)\n    def __or__(self, other: 'Nanobrick') -&gt; 'Nanobrick':\n        \"\"\"Pipe operator for composition\"\"\"\n        ...\n    \n    # Battery interfaces\n    def as_api(self) -&gt; 'FastAPI': ...\n    def as_cli(self) -&gt; 'Typer': ...\n    def as_ui(self) -&gt; 'StreamlitComponent': ...\n    def as_model(self) -&gt; 'SQLModel': ...\n\n\n\n\nSelf-Healing\n\nAutomatic retry with exponential backoff\nGraceful degradation\nCircuit breaker patterns\n\nAdaptation\n\nRuntime configuration updates\nDynamic scaling based on load\nLearning from failures\n\nEvolution\n\nVersion migration support\nBackward compatibility guarantees\nProgressive enhancement\n\n\n\n\n\n\n\n\n\nDefine base Nanobrick protocol\nImplement composition operators\nCreate battery interface adapters\n\n\n\n\n\ngo-task templates\nProject generator CLI\nAI-optimized cursorrules\n\n\n\n\n\nCommon nanobricks (validators, transformers, etc.)\nIntegration patterns\nExample compositions\n\n\n\n\n\nPackage registry\nComposition marketplace\nVisual workflow builder\n\n\n\n\n\nmy-nanobrick/\n├── pyproject.toml      # uv-managed project file\n├── src/\n│   └── my_nanobrick/   # Directory-based module\n│       ├── __init__.py\n│       ├── core.py     # Core nanobrick implementation\n│       ├── api.py      # FastAPI integration\n│       ├── cli.py      # Typer CLI\n│       ├── ui.py       # Streamlit components\n│       └── models.py   # SQLModel definitions\n└── tests/\n\n\n\nfrom my_nanobrick import ValidatorData, DataTransformer\nfrom typing import Dict\n\n# Define a simple nanobrick with type safety\n@nanobrick\nclass ValidatorData(Nanobrick[Dict, Dict, None]):\n    async def invoke(self, input: Dict, *, deps=None) -&gt; Dict:\n        # Validation logic\n        return validated_data\n\n# Another nanobrick\n@nanobrick  \nclass DataTransformer(Nanobrick[Dict, Dict, None]):\n    async def invoke(self, input: Dict, *, deps=None) -&gt; Dict:\n        # Transformation logic\n        return transformed_data\n\n# Compose using pipe operator (LCEL-style)\npipeline = ValidatorData() | DataTransformer()\n\n# Use in multiple ways\napp = pipeline.as_api()  # FastAPI app\ncli = pipeline.as_cli()  # Typer CLI\nui = pipeline.as_ui()    # Streamlit component\n\n\n\n\nInterface Standardization: What’s the minimal yet sufficient interface?\nComposition Rules: How do we handle type safety across compositions?\nState Management: Should nanobricks be stateless or support controlled state?\nDiscovery Mechanism: How do nanobricks find and connect to each other?\nPerformance: How do we ensure composition doesn’t degrade performance?\n\n\n\n\n\n\n\nSet up nanobricks-core package with uv\nImplement Nanobrick protocol with generics\nCreate composition operators (|, parallel, etc.)\nBuild decorator system (@nanobrick)\n\n\n\n\n\nFastAPI adapter (as_api())\nTyper adapter (as_cli())\nStreamlit adapter (as_ui())\nSQLModel adapter (as_model())\n\n\n\n\n\nuv-based project template\ngo-task automation scripts\nAI-optimized cursorrules\nExample nanobricks library\n\n\n\n\n\nTesting patterns for nanobricks\nType checking with mypy/pyright\nDocumentation generation\nBest practices guide",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks v1.0"
    ]
  },
  {
    "objectID": "v1/concept.html#core-philosophy",
    "href": "v1/concept.html#core-philosophy",
    "title": "Nanobricks v1.0",
    "section": "",
    "text": "Nanobricks are the code equivalent of “antifragile nanobots” — atomic, self-sufficient components that gain strength from stress and compose organically into complex systems.",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks v1.0"
    ]
  },
  {
    "objectID": "v1/concept.html#foundational-principles",
    "href": "v1/concept.html#foundational-principles",
    "title": "Nanobricks v1.0",
    "section": "",
    "text": "uv-based: Every nanobrick is a Python package managed by uv\nsrc-layout: Following the src-based project layout standard\nDirectory-based modules: Top-level modules as directories, sub-modules as files\nAbsolute imports: Always use absolute imports for clarity and consistency",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks v1.0"
    ]
  },
  {
    "objectID": "v1/concept.html#the-five-pillars",
    "href": "v1/concept.html#the-five-pillars",
    "title": "Nanobricks v1.0",
    "section": "",
    "text": "Designed for clarity and straightforward implementation\nEasy for both humans and AIs to reason about\nSingle responsibility principle at the atomic level\nMinimal cognitive overhead\n\n\n\n\n\nConsistent interfaces — the “Lego Connector Mechanism” for code\nPredictable behavior patterns\nUniversal protocols for:\n\nInput/Output contracts\nConfiguration management\nError handling\nLifecycle hooks\n\n\n\n\n\n\nSeamless integration patterns\nPipeline-ready (can be chained/piped together)\nSupports multiple composition patterns:\n\nSequential (A → B → C)\nParallel (A + B + C)\nNested (A(B(C)))\nHybrid workflows\n\n\n\n\n\nEach nanobrick ships with modular, self-contained interfaces: - API Layer (FastAPI) — RESTful endpoints auto-generated - CLI Layer (Typer) — Command-line interface out of the box - Frontend Layer (Streamlit) — UI components (app/page/tab/subtab) - Data Layer (SQLModel) — Database interaction when needed\n\n\n\n\nInstant end-to-end functionality\nRails-inspired convention over configuration\ngo-task powered automation\nAI-friendly cursorrules for guided implementation\nProgressive enhancement model",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks v1.0"
    ]
  },
  {
    "objectID": "v1/concept.html#key-design-patterns",
    "href": "v1/concept.html#key-design-patterns",
    "title": "Nanobricks v1.0",
    "section": "",
    "text": "Based on analysis of successful frameworks, nanobricks will leverage:\n\nRunnable Interface Pattern (from LangChain)\n\nStandardized methods: invoke(), batch(), stream()\nEnables uniform interaction across all components\n\nComposition Pattern with Pipe Operator (from LangChain’s LCEL)\n\nUse | operator for intuitive chaining\nDeclarative pipeline construction\n\nDependency Injection (from PydanticAI)\n\nContext-based dependency passing\nEnhances testability and flexibility\n\nDecorator Pattern (from PydanticAI)\n\n@nanobrick decorator for component registration\nClean, pythonic API\n\nGeneric Programming (from PydanticAI)\n\nType-safe interfaces using Python generics\nCompile-time type checking\n\n\n\n\n\nfrom typing import Protocol, TypeVar, Generic, Any\nfrom abc import abstractmethod\n\nInputT = TypeVar('InputT')\nOutputT = TypeVar('OutputT')\nDepsT = TypeVar('DepsT')\n\nclass Nanobrick(Protocol, Generic[InputT, OutputT, DepsT]):\n    \"\"\"Universal interface for all nanobricks - inspired by LangChain's Runnable\"\"\"\n    \n    # Core identity\n    name: str\n    version: str\n    \n    # Primary execution methods (Runnable pattern)\n    @abstractmethod\n    async def invoke(self, input: InputT, *, deps: DepsT = None) -&gt; OutputT:\n        \"\"\"Async execution with dependency injection\"\"\"\n        ...\n    \n    def batch(self, inputs: list[InputT], *, deps: DepsT = None) -&gt; list[OutputT]:\n        \"\"\"Batch processing\"\"\"\n        ...\n    \n    async def stream(self, input: InputT, *, deps: DepsT = None):\n        \"\"\"Streaming execution\"\"\"\n        ...\n    \n    # Composition support (LCEL-inspired)\n    def __or__(self, other: 'Nanobrick') -&gt; 'Nanobrick':\n        \"\"\"Pipe operator for composition\"\"\"\n        ...\n    \n    # Battery interfaces\n    def as_api(self) -&gt; 'FastAPI': ...\n    def as_cli(self) -&gt; 'Typer': ...\n    def as_ui(self) -&gt; 'StreamlitComponent': ...\n    def as_model(self) -&gt; 'SQLModel': ...\n\n\n\n\nSelf-Healing\n\nAutomatic retry with exponential backoff\nGraceful degradation\nCircuit breaker patterns\n\nAdaptation\n\nRuntime configuration updates\nDynamic scaling based on load\nLearning from failures\n\nEvolution\n\nVersion migration support\nBackward compatibility guarantees\nProgressive enhancement",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks v1.0"
    ]
  },
  {
    "objectID": "v1/concept.html#implementation-strategy",
    "href": "v1/concept.html#implementation-strategy",
    "title": "Nanobricks v1.0",
    "section": "",
    "text": "Define base Nanobrick protocol\nImplement composition operators\nCreate battery interface adapters\n\n\n\n\n\ngo-task templates\nProject generator CLI\nAI-optimized cursorrules\n\n\n\n\n\nCommon nanobricks (validators, transformers, etc.)\nIntegration patterns\nExample compositions\n\n\n\n\n\nPackage registry\nComposition marketplace\nVisual workflow builder",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks v1.0"
    ]
  },
  {
    "objectID": "v1/concept.html#package-structure",
    "href": "v1/concept.html#package-structure",
    "title": "Nanobricks v1.0",
    "section": "",
    "text": "my-nanobrick/\n├── pyproject.toml      # uv-managed project file\n├── src/\n│   └── my_nanobrick/   # Directory-based module\n│       ├── __init__.py\n│       ├── core.py     # Core nanobrick implementation\n│       ├── api.py      # FastAPI integration\n│       ├── cli.py      # Typer CLI\n│       ├── ui.py       # Streamlit components\n│       └── models.py   # SQLModel definitions\n└── tests/",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks v1.0"
    ]
  },
  {
    "objectID": "v1/concept.html#example-usage",
    "href": "v1/concept.html#example-usage",
    "title": "Nanobricks v1.0",
    "section": "",
    "text": "from my_nanobrick import ValidatorData, DataTransformer\nfrom typing import Dict\n\n# Define a simple nanobrick with type safety\n@nanobrick\nclass ValidatorData(Nanobrick[Dict, Dict, None]):\n    async def invoke(self, input: Dict, *, deps=None) -&gt; Dict:\n        # Validation logic\n        return validated_data\n\n# Another nanobrick\n@nanobrick  \nclass DataTransformer(Nanobrick[Dict, Dict, None]):\n    async def invoke(self, input: Dict, *, deps=None) -&gt; Dict:\n        # Transformation logic\n        return transformed_data\n\n# Compose using pipe operator (LCEL-style)\npipeline = ValidatorData() | DataTransformer()\n\n# Use in multiple ways\napp = pipeline.as_api()  # FastAPI app\ncli = pipeline.as_cli()  # Typer CLI\nui = pipeline.as_ui()    # Streamlit component",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks v1.0"
    ]
  },
  {
    "objectID": "v1/concept.html#open-questions",
    "href": "v1/concept.html#open-questions",
    "title": "Nanobricks v1.0",
    "section": "",
    "text": "Interface Standardization: What’s the minimal yet sufficient interface?\nComposition Rules: How do we handle type safety across compositions?\nState Management: Should nanobricks be stateless or support controlled state?\nDiscovery Mechanism: How do nanobricks find and connect to each other?\nPerformance: How do we ensure composition doesn’t degrade performance?",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks v1.0"
    ]
  },
  {
    "objectID": "v1/concept.html#implementation-roadmap",
    "href": "v1/concept.html#implementation-roadmap",
    "title": "Nanobricks v1.0",
    "section": "",
    "text": "Set up nanobricks-core package with uv\nImplement Nanobrick protocol with generics\nCreate composition operators (|, parallel, etc.)\nBuild decorator system (@nanobrick)\n\n\n\n\n\nFastAPI adapter (as_api())\nTyper adapter (as_cli())\nStreamlit adapter (as_ui())\nSQLModel adapter (as_model())\n\n\n\n\n\nuv-based project template\ngo-task automation scripts\nAI-optimized cursorrules\nExample nanobricks library\n\n\n\n\n\nTesting patterns for nanobricks\nType checking with mypy/pyright\nDocumentation generation\nBest practices guide",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks v1.0"
    ]
  },
  {
    "objectID": "architecture-diagrams.html",
    "href": "architecture-diagrams.html",
    "title": "Architecture Diagrams",
    "section": "",
    "text": "classDiagram\n    class NanobrickProtocol {\n        &lt;&lt;interface&gt;&gt;\n        +name: str\n        +version: str\n        +invoke(input: T_in, deps: T_deps) T_out\n        +invoke_sync(input: T_in, deps: T_deps) T_out\n        +__or__(other: NanobrickProtocol) NanobrickProtocol\n    }\n    \n    class NanobrickBase {\n        &lt;&lt;abstract&gt;&gt;\n        +name: str\n        +version: str\n        +invoke(input: T_in, deps: T_deps) T_out\n        +invoke_sync(input: T_in, deps: T_deps) T_out\n        +with_skill(skill: str, **config) NanobrickEnhanced\n    }\n    \n    class NanobrickSimple {\n        +invoke(input: T_in, deps: None) T_out\n    }\n    \n    class MyBrick {\n        +invoke(input: str, deps: None) str\n    }\n    \n    NanobrickProtocol &lt;|.. NanobrickBase : implements\n    NanobrickBase &lt;|-- NanobrickSimple : extends\n    NanobrickSimple &lt;|-- MyBrick : extends\n\n\n\n\nclassDiagram\n    class NanobrickProtocol {\n        &lt;&lt;interface&gt;&gt;\n        +name: str\n        +version: str\n        +invoke(input: T_in, deps: T_deps) T_out\n        +invoke_sync(input: T_in, deps: T_deps) T_out\n        +__or__(other: NanobrickProtocol) NanobrickProtocol\n    }\n    \n    class NanobrickBase {\n        &lt;&lt;abstract&gt;&gt;\n        +name: str\n        +version: str\n        +invoke(input: T_in, deps: T_deps) T_out\n        +invoke_sync(input: T_in, deps: T_deps) T_out\n        +with_skill(skill: str, **config) NanobrickEnhanced\n    }\n    \n    class NanobrickSimple {\n        +invoke(input: T_in, deps: None) T_out\n    }\n    \n    class MyBrick {\n        +invoke(input: str, deps: None) str\n    }\n    \n    NanobrickProtocol &lt;|.. NanobrickBase : implements\n    NanobrickBase &lt;|-- NanobrickSimple : extends\n    NanobrickSimple &lt;|-- MyBrick : extends\n\n\n\n\n\n\n\n\n\n\ngraph LR\n    subgraph \"Individual Bricks\"\n        A[Validator&lt;br/&gt;str → str]\n        B[Transformer&lt;br/&gt;str → dict]\n        C[Persister&lt;br/&gt;dict → bool]\n    end\n    \n    subgraph \"Composed Pipeline\"\n        A --&gt; B --&gt; C\n    end\n    \n    Input[/\"hello\"/] --&gt; A\n    C --&gt; Output[/true/]\n    \n    style A fill:#e1f5fe\n    style B fill:#e1f5fe\n    style C fill:#e1f5fe\n\n\n\n\ngraph LR\n    subgraph \"Individual Bricks\"\n        A[Validator&lt;br/&gt;str → str]\n        B[Transformer&lt;br/&gt;str → dict]\n        C[Persister&lt;br/&gt;dict → bool]\n    end\n    \n    subgraph \"Composed Pipeline\"\n        A --&gt; B --&gt; C\n    end\n    \n    Input[/\"hello\"/] --&gt; A\n    C --&gt; Output[/true/]\n    \n    style A fill:#e1f5fe\n    style B fill:#e1f5fe\n    style C fill:#e1f5fe",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Architecture Diagrams"
    ]
  },
  {
    "objectID": "architecture-diagrams.html#core-architecture",
    "href": "architecture-diagrams.html#core-architecture",
    "title": "Architecture Diagrams",
    "section": "",
    "text": "classDiagram\n    class NanobrickProtocol {\n        &lt;&lt;interface&gt;&gt;\n        +name: str\n        +version: str\n        +invoke(input: T_in, deps: T_deps) T_out\n        +invoke_sync(input: T_in, deps: T_deps) T_out\n        +__or__(other: NanobrickProtocol) NanobrickProtocol\n    }\n    \n    class NanobrickBase {\n        &lt;&lt;abstract&gt;&gt;\n        +name: str\n        +version: str\n        +invoke(input: T_in, deps: T_deps) T_out\n        +invoke_sync(input: T_in, deps: T_deps) T_out\n        +with_skill(skill: str, **config) NanobrickEnhanced\n    }\n    \n    class NanobrickSimple {\n        +invoke(input: T_in, deps: None) T_out\n    }\n    \n    class MyBrick {\n        +invoke(input: str, deps: None) str\n    }\n    \n    NanobrickProtocol &lt;|.. NanobrickBase : implements\n    NanobrickBase &lt;|-- NanobrickSimple : extends\n    NanobrickSimple &lt;|-- MyBrick : extends\n\n\n\n\nclassDiagram\n    class NanobrickProtocol {\n        &lt;&lt;interface&gt;&gt;\n        +name: str\n        +version: str\n        +invoke(input: T_in, deps: T_deps) T_out\n        +invoke_sync(input: T_in, deps: T_deps) T_out\n        +__or__(other: NanobrickProtocol) NanobrickProtocol\n    }\n    \n    class NanobrickBase {\n        &lt;&lt;abstract&gt;&gt;\n        +name: str\n        +version: str\n        +invoke(input: T_in, deps: T_deps) T_out\n        +invoke_sync(input: T_in, deps: T_deps) T_out\n        +with_skill(skill: str, **config) NanobrickEnhanced\n    }\n    \n    class NanobrickSimple {\n        +invoke(input: T_in, deps: None) T_out\n    }\n    \n    class MyBrick {\n        +invoke(input: str, deps: None) str\n    }\n    \n    NanobrickProtocol &lt;|.. NanobrickBase : implements\n    NanobrickBase &lt;|-- NanobrickSimple : extends\n    NanobrickSimple &lt;|-- MyBrick : extends\n\n\n\n\n\n\n\n\n\n\ngraph LR\n    subgraph \"Individual Bricks\"\n        A[Validator&lt;br/&gt;str → str]\n        B[Transformer&lt;br/&gt;str → dict]\n        C[Persister&lt;br/&gt;dict → bool]\n    end\n    \n    subgraph \"Composed Pipeline\"\n        A --&gt; B --&gt; C\n    end\n    \n    Input[/\"hello\"/] --&gt; A\n    C --&gt; Output[/true/]\n    \n    style A fill:#e1f5fe\n    style B fill:#e1f5fe\n    style C fill:#e1f5fe\n\n\n\n\ngraph LR\n    subgraph \"Individual Bricks\"\n        A[Validator&lt;br/&gt;str → str]\n        B[Transformer&lt;br/&gt;str → dict]\n        C[Persister&lt;br/&gt;dict → bool]\n    end\n    \n    subgraph \"Composed Pipeline\"\n        A --&gt; B --&gt; C\n    end\n    \n    Input[/\"hello\"/] --&gt; A\n    C --&gt; Output[/true/]\n    \n    style A fill:#e1f5fe\n    style B fill:#e1f5fe\n    style C fill:#e1f5fe",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Architecture Diagrams"
    ]
  },
  {
    "objectID": "architecture-diagrams.html#composition-patterns",
    "href": "architecture-diagrams.html#composition-patterns",
    "title": "Architecture Diagrams",
    "section": "Composition Patterns",
    "text": "Composition Patterns\n\nLinear Pipeline\n\ngraph LR\n    Input[Input Data] --&gt; V[ValidateBrick]\n    V --&gt; T[TransformBrick]\n    T --&gt; E[EnrichBrick]\n    E --&gt; S[SaveBrick]\n    S --&gt; Output[Output]\n    \n    style V fill:#b3e5fc\n    style T fill:#81d4fa\n    style E fill:#4fc3f7\n    style S fill:#29b6f6\n\n\n\n\ngraph LR\n    Input[Input Data] --&gt; V[ValidateBrick]\n    V --&gt; T[TransformBrick]\n    T --&gt; E[EnrichBrick]\n    E --&gt; S[SaveBrick]\n    S --&gt; Output[Output]\n    \n    style V fill:#b3e5fc\n    style T fill:#81d4fa\n    style E fill:#4fc3f7\n    style S fill:#29b6f6\n\n\n\n\n\n\nCode:\npipeline = ValidateBrick() | TransformBrick() | EnrichBrick() | SaveBrick()\n\n\nBranching Pattern\n\ngraph TB\n    Input[Input] --&gt; C{ConditionBrick}\n    C --&gt;|True| T[TrueBranch]\n    C --&gt;|False| F[FalseBranch]\n    T --&gt; M[MergeBrick]\n    F --&gt; M\n    M --&gt; Output[Output]\n    \n    style C fill:#fff59d\n    style T fill:#a5d6a7\n    style F fill:#ef9a9a\n\n\n\n\ngraph TB\n    Input[Input] --&gt; C{ConditionBrick}\n    C --&gt;|True| T[TrueBranch]\n    C --&gt;|False| F[FalseBranch]\n    T --&gt; M[MergeBrick]\n    F --&gt; M\n    M --&gt; Output[Output]\n    \n    style C fill:#fff59d\n    style T fill:#a5d6a7\n    style F fill:#ef9a9a\n\n\n\n\n\n\nCode:\nbranch = Branch(\n    condition=ConditionBrick(),\n    true_branch=ProcessAdult(),\n    false_branch=ProcessMinor()\n)\n\n\nParallel Execution\n\ngraph TB\n    Input[Input] --&gt; Split{FanOut}\n    Split --&gt; P1[Processor1]\n    Split --&gt; P2[Processor2]\n    Split --&gt; P3[Processor3]\n    P1 --&gt; Merge{FanIn}\n    P2 --&gt; Merge\n    P3 --&gt; Merge\n    Merge --&gt; Output[Output]\n    \n    style Split fill:#ce93d8\n    style Merge fill:#ce93d8\n    style P1 fill:#b39ddb\n    style P2 fill:#b39ddb\n    style P3 fill:#b39ddb\n\n\n\n\ngraph TB\n    Input[Input] --&gt; Split{FanOut}\n    Split --&gt; P1[Processor1]\n    Split --&gt; P2[Processor2]\n    Split --&gt; P3[Processor3]\n    P1 --&gt; Merge{FanIn}\n    P2 --&gt; Merge\n    P3 --&gt; Merge\n    Merge --&gt; Output[Output]\n    \n    style Split fill:#ce93d8\n    style Merge fill:#ce93d8\n    style P1 fill:#b39ddb\n    style P2 fill:#b39ddb\n    style P3 fill:#b39ddb\n\n\n\n\n\n\nCode:\nparallel = Parallel([\n    Processor1(),\n    Processor2(),\n    Processor3()\n])\n\n\nError Handling with Fallback\n\ngraph LR\n    Input[Input] --&gt; P{PrimaryBrick}\n    P --&gt;|Success| Output[Output]\n    P -.-&gt;|Error| F[FallbackBrick]\n    F --&gt; Output\n    \n    style P fill:#a5d6a7\n    style F fill:#ffcc80\n\n\n\n\ngraph LR\n    Input[Input] --&gt; P{PrimaryBrick}\n    P --&gt;|Success| Output[Output]\n    P -.-&gt;|Error| F[FallbackBrick]\n    F --&gt; Output\n    \n    style P fill:#a5d6a7\n    style F fill:#ffcc80\n\n\n\n\n\n\nCode:\nsafe = Fallback(\n    primary=UnreliableAPI(),\n    fallback=CachedResponse()\n)",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Architecture Diagrams"
    ]
  },
  {
    "objectID": "architecture-diagrams.html#skills-enhancement",
    "href": "architecture-diagrams.html#skills-enhancement",
    "title": "Architecture Diagrams",
    "section": "Skills Enhancement",
    "text": "Skills Enhancement\n\nHow Skills Work\n\ngraph TB\n    subgraph \"Base Brick\"\n        B[DataProcessor&lt;br/&gt;invoke()]\n    end\n    \n    subgraph \"With Logging Skill\"\n        L1[LoggingSkill] --&gt; B1[DataProcessor]\n        L1 -.-&gt; |logs| Log1[Log Output]\n    end\n    \n    subgraph \"With Multiple Skills\"\n        L2[LoggingSkill] --&gt; A[APISkill] --&gt; B2[DataProcessor]\n        L2 -.-&gt; |logs| Log2[Log Output]\n        A -.-&gt; |exposes| API[REST API]\n    end\n    \n    style B fill:#e3f2fd\n    style B1 fill:#e3f2fd\n    style B2 fill:#e3f2fd\n    style L1 fill:#fff9c4\n    style L2 fill:#fff9c4\n    style A fill:#f3e5f5\n\n\n\n\ngraph TB\n    subgraph \"Base Brick\"\n        B[DataProcessor&lt;br/&gt;invoke()]\n    end\n    \n    subgraph \"With Logging Skill\"\n        L1[LoggingSkill] --&gt; B1[DataProcessor]\n        L1 -.-&gt; |logs| Log1[Log Output]\n    end\n    \n    subgraph \"With Multiple Skills\"\n        L2[LoggingSkill] --&gt; A[APISkill] --&gt; B2[DataProcessor]\n        L2 -.-&gt; |logs| Log2[Log Output]\n        A -.-&gt; |exposes| API[REST API]\n    end\n    \n    style B fill:#e3f2fd\n    style B1 fill:#e3f2fd\n    style B2 fill:#e3f2fd\n    style L1 fill:#fff9c4\n    style L2 fill:#fff9c4\n    style A fill:#f3e5f5\n\n\n\n\n\n\nCode:\n# Base brick\nprocessor = DataProcessor()\n\n# Add skills\nlogged = processor.with_skill(\"logging\")\napi_logged = processor.with_skill(\"logging\").with_skill(\"api\")\n\n\nSkill Composition\n\ngraph LR\n    subgraph \"Skills Available\"\n        S1[Logging]\n        S2[API]\n        S3[CLI]\n        S4[Cache]\n        S5[Monitoring]\n        S6[Docker]\n    end\n    \n    subgraph \"Enhanced Brick\"\n        Core[YourBrick]\n        S1 --&gt; Core\n        S2 --&gt; Core\n        S5 --&gt; Core\n    end\n    \n    Core --&gt; Features[Enhanced&lt;br/&gt;Features]\n    \n    style Core fill:#90caf9\n    style S1 fill:#fff59d\n    style S2 fill:#f48fb1\n    style S5 fill:#a5d6a7\n\n\n\n\ngraph LR\n    subgraph \"Skills Available\"\n        S1[Logging]\n        S2[API]\n        S3[CLI]\n        S4[Cache]\n        S5[Monitoring]\n        S6[Docker]\n    end\n    \n    subgraph \"Enhanced Brick\"\n        Core[YourBrick]\n        S1 --&gt; Core\n        S2 --&gt; Core\n        S5 --&gt; Core\n    end\n    \n    Core --&gt; Features[Enhanced&lt;br/&gt;Features]\n    \n    style Core fill:#90caf9\n    style S1 fill:#fff59d\n    style S2 fill:#f48fb1\n    style S5 fill:#a5d6a7",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Architecture Diagrams"
    ]
  },
  {
    "objectID": "architecture-diagrams.html#dependency-flow",
    "href": "architecture-diagrams.html#dependency-flow",
    "title": "Architecture Diagrams",
    "section": "Dependency Flow",
    "text": "Dependency Flow\n\nDependencies Through Pipeline\n\ngraph TB\n    subgraph \"Dependencies\"\n        DB[(Database)]\n        Cache[(Cache)]\n        Config[Config]\n    end\n    \n    subgraph \"Pipeline\"\n        A[LoadUser] -.-&gt; DB\n        A --&gt; B[EnrichUser]\n        B -.-&gt; Cache\n        B --&gt; C[SaveUser]\n        C -.-&gt; DB\n        C -.-&gt; Config\n    end\n    \n    Input[user_id] --&gt; A\n    C --&gt; Output[User]\n    \n    style A fill:#e1f5fe\n    style B fill:#b3e5fc\n    style C fill:#81d4fa\n\n\n\n\ngraph TB\n    subgraph \"Dependencies\"\n        DB[(Database)]\n        Cache[(Cache)]\n        Config[Config]\n    end\n    \n    subgraph \"Pipeline\"\n        A[LoadUser] -.-&gt; DB\n        A --&gt; B[EnrichUser]\n        B -.-&gt; Cache\n        B --&gt; C[SaveUser]\n        C -.-&gt; DB\n        C -.-&gt; Config\n    end\n    \n    Input[user_id] --&gt; A\n    C --&gt; Output[User]\n    \n    style A fill:#e1f5fe\n    style B fill:#b3e5fc\n    style C fill:#81d4fa\n\n\n\n\n\n\nCode:\ndeps = {\n    \"db\": database_connection,\n    \"cache\": redis_client,\n    \"config\": app_config\n}\n\nresult = await pipeline.invoke(input, deps=deps)",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Architecture Diagrams"
    ]
  },
  {
    "objectID": "architecture-diagrams.html#system-architecture-examples",
    "href": "architecture-diagrams.html#system-architecture-examples",
    "title": "Architecture Diagrams",
    "section": "System Architecture Examples",
    "text": "System Architecture Examples\n\nMicroservice Architecture\n\ngraph TB\n    subgraph \"API Gateway\"\n        GW[Gateway&lt;br/&gt;RateLimiter | Auth | Router]\n    end\n    \n    subgraph \"User Service\"\n        U1[ValidateUser]\n        U2[CreateUser]\n        U3[NotifyUser]\n        U1 --&gt; U2 --&gt; U3\n    end\n    \n    subgraph \"Order Service\"\n        O1[ValidateOrder]\n        O2[ProcessPayment]\n        O3[CreateFulfillment]\n        O1 --&gt; O2 --&gt; O3\n    end\n    \n    subgraph \"Shared Infrastructure\"\n        DB[(PostgreSQL)]\n        MQ[RabbitMQ]\n        Cache[(Redis)]\n    end\n    \n    GW --&gt; U1\n    GW --&gt; O1\n    U2 -.-&gt; DB\n    O2 -.-&gt; DB\n    U3 -.-&gt; MQ\n    O3 -.-&gt; MQ\n    \n    style GW fill:#ffccbc\n    style U1 fill:#c5e1a5\n    style O1 fill:#b3e5fc\n\n\n\n\ngraph TB\n    subgraph \"API Gateway\"\n        GW[Gateway&lt;br/&gt;RateLimiter | Auth | Router]\n    end\n    \n    subgraph \"User Service\"\n        U1[ValidateUser]\n        U2[CreateUser]\n        U3[NotifyUser]\n        U1 --&gt; U2 --&gt; U3\n    end\n    \n    subgraph \"Order Service\"\n        O1[ValidateOrder]\n        O2[ProcessPayment]\n        O3[CreateFulfillment]\n        O1 --&gt; O2 --&gt; O3\n    end\n    \n    subgraph \"Shared Infrastructure\"\n        DB[(PostgreSQL)]\n        MQ[RabbitMQ]\n        Cache[(Redis)]\n    end\n    \n    GW --&gt; U1\n    GW --&gt; O1\n    U2 -.-&gt; DB\n    O2 -.-&gt; DB\n    U3 -.-&gt; MQ\n    O3 -.-&gt; MQ\n    \n    style GW fill:#ffccbc\n    style U1 fill:#c5e1a5\n    style O1 fill:#b3e5fc\n\n\n\n\n\n\n\n\nEvent-Driven Architecture\n\ngraph LR\n    subgraph \"Event Sources\"\n        API[API Events]\n        DB[DB Changes]\n        Queue[Message Queue]\n    end\n    \n    subgraph \"Event Bus\"\n        EB{Event Bus}\n    end\n    \n    subgraph \"Event Handlers\"\n        H1[EmailHandler]\n        H2[AnalyticsHandler]\n        H3[AuditHandler]\n    end\n    \n    API --&gt; EB\n    DB --&gt; EB\n    Queue --&gt; EB\n    \n    EB --&gt; H1\n    EB --&gt; H2\n    EB --&gt; H3\n    \n    style EB fill:#e1bee7\n    style H1 fill:#c5cae9\n    style H2 fill:#c5cae9\n    style H3 fill:#c5cae9\n\n\n\n\ngraph LR\n    subgraph \"Event Sources\"\n        API[API Events]\n        DB[DB Changes]\n        Queue[Message Queue]\n    end\n    \n    subgraph \"Event Bus\"\n        EB{Event Bus}\n    end\n    \n    subgraph \"Event Handlers\"\n        H1[EmailHandler]\n        H2[AnalyticsHandler]\n        H3[AuditHandler]\n    end\n    \n    API --&gt; EB\n    DB --&gt; EB\n    Queue --&gt; EB\n    \n    EB --&gt; H1\n    EB --&gt; H2\n    EB --&gt; H3\n    \n    style EB fill:#e1bee7\n    style H1 fill:#c5cae9\n    style H2 fill:#c5cae9\n    style H3 fill:#c5cae9\n\n\n\n\n\n\n\n\nData Pipeline Architecture\n\ngraph TB\n    subgraph \"Sources\"\n        S3[S3 Bucket]\n        API[External API]\n        DB[(Database)]\n    end\n    \n    subgraph \"Extract\"\n        E[ExtractorBrick]\n    end\n    \n    subgraph \"Transform\"\n        T1[Parser] --&gt; T2[Validator]\n        T2 --&gt; T3[Enricher]\n        T3 --&gt; T4[Aggregator]\n    end\n    \n    subgraph \"Load\"\n        L1[DataLake]\n        L2[Analytics DB]\n        L3[Dashboard]\n    end\n    \n    S3 --&gt; E\n    API --&gt; E\n    DB --&gt; E\n    \n    E --&gt; T1\n    T4 --&gt; L1\n    T4 --&gt; L2\n    T4 --&gt; L3\n    \n    style E fill:#ffcdd2\n    style T1 fill:#f8bbd0\n    style T2 fill:#e1bee7\n    style T3 fill:#ce93d8\n    style T4 fill:#b39ddb\n\n\n\n\ngraph TB\n    subgraph \"Sources\"\n        S3[S3 Bucket]\n        API[External API]\n        DB[(Database)]\n    end\n    \n    subgraph \"Extract\"\n        E[ExtractorBrick]\n    end\n    \n    subgraph \"Transform\"\n        T1[Parser] --&gt; T2[Validator]\n        T2 --&gt; T3[Enricher]\n        T3 --&gt; T4[Aggregator]\n    end\n    \n    subgraph \"Load\"\n        L1[DataLake]\n        L2[Analytics DB]\n        L3[Dashboard]\n    end\n    \n    S3 --&gt; E\n    API --&gt; E\n    DB --&gt; E\n    \n    E --&gt; T1\n    T4 --&gt; L1\n    T4 --&gt; L2\n    T4 --&gt; L3\n    \n    style E fill:#ffcdd2\n    style T1 fill:#f8bbd0\n    style T2 fill:#e1bee7\n    style T3 fill:#ce93d8\n    style T4 fill:#b39ddb",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Architecture Diagrams"
    ]
  },
  {
    "objectID": "architecture-diagrams.html#deployment-patterns",
    "href": "architecture-diagrams.html#deployment-patterns",
    "title": "Architecture Diagrams",
    "section": "Deployment Patterns",
    "text": "Deployment Patterns\n\nContainer Deployment\n\ngraph TB\n    subgraph \"Development\"\n        Dev[YourBrick]\n    end\n    \n    subgraph \"With Docker Skill\"\n        Docker[DockerSkill]\n        App[YourBrick]\n        Docker --&gt; App\n    end\n    \n    subgraph \"Generated\"\n        DF[Dockerfile]\n        DC[docker-compose.yml]\n        IMG[Container Image]\n    end\n    \n    App --&gt; DF\n    App --&gt; DC\n    DF --&gt; IMG\n    \n    subgraph \"Deployment\"\n        REG[Container Registry]\n        K8S[Kubernetes]\n        LAMBDA[AWS Lambda]\n    end\n    \n    IMG --&gt; REG\n    REG --&gt; K8S\n    REG --&gt; LAMBDA\n    \n    style Docker fill:#e3f2fd\n    style IMG fill:#c5cae9\n\n\n\n\ngraph TB\n    subgraph \"Development\"\n        Dev[YourBrick]\n    end\n    \n    subgraph \"With Docker Skill\"\n        Docker[DockerSkill]\n        App[YourBrick]\n        Docker --&gt; App\n    end\n    \n    subgraph \"Generated\"\n        DF[Dockerfile]\n        DC[docker-compose.yml]\n        IMG[Container Image]\n    end\n    \n    App --&gt; DF\n    App --&gt; DC\n    DF --&gt; IMG\n    \n    subgraph \"Deployment\"\n        REG[Container Registry]\n        K8S[Kubernetes]\n        LAMBDA[AWS Lambda]\n    end\n    \n    IMG --&gt; REG\n    REG --&gt; K8S\n    REG --&gt; LAMBDA\n    \n    style Docker fill:#e3f2fd\n    style IMG fill:#c5cae9\n\n\n\n\n\n\n\n\nKubernetes Architecture\n\ngraph TB\n    subgraph \"Kubernetes Cluster\"\n        subgraph \"Namespace: production\"\n            D[Deployment&lt;br/&gt;3 replicas]\n            S[Service&lt;br/&gt;LoadBalancer]\n            HPA[HorizontalPodAutoscaler]\n            CM[ConfigMap]\n            SEC[Secret]\n        end\n        \n        subgraph \"Pods\"\n            P1[Pod 1&lt;br/&gt;YourBrick]\n            P2[Pod 2&lt;br/&gt;YourBrick]\n            P3[Pod 3&lt;br/&gt;YourBrick]\n        end\n    end\n    \n    subgraph \"External\"\n        ING[Ingress]\n        MON[Prometheus]\n        LOG[ELK Stack]\n    end\n    \n    ING --&gt; S\n    S --&gt; P1\n    S --&gt; P2\n    S --&gt; P3\n    HPA --&gt; D\n    D --&gt; P1\n    D --&gt; P2\n    D --&gt; P3\n    CM --&gt; P1\n    SEC --&gt; P1\n    \n    P1 -.-&gt; MON\n    P1 -.-&gt; LOG\n    \n    style D fill:#c8e6c9\n    style S fill:#bbdefb\n    style HPA fill:#fff9c4\n\n\n\n\ngraph TB\n    subgraph \"Kubernetes Cluster\"\n        subgraph \"Namespace: production\"\n            D[Deployment&lt;br/&gt;3 replicas]\n            S[Service&lt;br/&gt;LoadBalancer]\n            HPA[HorizontalPodAutoscaler]\n            CM[ConfigMap]\n            SEC[Secret]\n        end\n        \n        subgraph \"Pods\"\n            P1[Pod 1&lt;br/&gt;YourBrick]\n            P2[Pod 2&lt;br/&gt;YourBrick]\n            P3[Pod 3&lt;br/&gt;YourBrick]\n        end\n    end\n    \n    subgraph \"External\"\n        ING[Ingress]\n        MON[Prometheus]\n        LOG[ELK Stack]\n    end\n    \n    ING --&gt; S\n    S --&gt; P1\n    S --&gt; P2\n    S --&gt; P3\n    HPA --&gt; D\n    D --&gt; P1\n    D --&gt; P2\n    D --&gt; P3\n    CM --&gt; P1\n    SEC --&gt; P1\n    \n    P1 -.-&gt; MON\n    P1 -.-&gt; LOG\n    \n    style D fill:#c8e6c9\n    style S fill:#bbdefb\n    style HPA fill:#fff9c4",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Architecture Diagrams"
    ]
  },
  {
    "objectID": "architecture-diagrams.html#production-patterns",
    "href": "architecture-diagrams.html#production-patterns",
    "title": "Architecture Diagrams",
    "section": "Production Patterns",
    "text": "Production Patterns\n\nCircuit Breaker Pattern\n\nstateDiagram-v2\n    [*] --&gt; Closed\n    Closed --&gt; Open: Failure threshold reached\n    Open --&gt; HalfOpen: After timeout\n    HalfOpen --&gt; Closed: Success\n    HalfOpen --&gt; Open: Failure\n    \n    state Closed {\n        [*] --&gt; Normal\n        Normal --&gt; Normal: Success\n        Normal --&gt; Counting: Failure\n        Counting --&gt; Counting: Failure &lt; threshold\n        Counting --&gt; Normal: Success\n    }\n    \n    state Open {\n        [*] --&gt; Rejecting\n        Rejecting --&gt; Rejecting: All requests fail fast\n    }\n    \n    state HalfOpen {\n        [*] --&gt; Testing\n        Testing --&gt; Testing: Limited requests\n    }\n\n\n\n\nstateDiagram-v2\n    [*] --&gt; Closed\n    Closed --&gt; Open: Failure threshold reached\n    Open --&gt; HalfOpen: After timeout\n    HalfOpen --&gt; Closed: Success\n    HalfOpen --&gt; Open: Failure\n    \n    state Closed {\n        [*] --&gt; Normal\n        Normal --&gt; Normal: Success\n        Normal --&gt; Counting: Failure\n        Counting --&gt; Counting: Failure &lt; threshold\n        Counting --&gt; Normal: Success\n    }\n    \n    state Open {\n        [*] --&gt; Rejecting\n        Rejecting --&gt; Rejecting: All requests fail fast\n    }\n    \n    state HalfOpen {\n        [*] --&gt; Testing\n        Testing --&gt; Testing: Limited requests\n    }\n\n\n\n\n\n\n\n\nBulkhead Pattern\n\ngraph TB\n    subgraph \"Without Bulkhead\"\n        R1[All Requests] --&gt; S1[Single Service]\n        S1 --&gt; F1[Total Failure]\n    end\n    \n    subgraph \"With Bulkhead\"\n        R2[Requests] --&gt; B{Bulkhead}\n        B --&gt; S2A[Service Pool A&lt;br/&gt;Max: 5]\n        B --&gt; S2B[Service Pool B&lt;br/&gt;Max: 5]\n        S2A --&gt; P1[Partial Failure]\n        S2B --&gt; OK[Still Working]\n    end\n    \n    style F1 fill:#ef5350\n    style P1 fill:#ffa726\n    style OK fill:#66bb6a\n\n\n\n\ngraph TB\n    subgraph \"Without Bulkhead\"\n        R1[All Requests] --&gt; S1[Single Service]\n        S1 --&gt; F1[Total Failure]\n    end\n    \n    subgraph \"With Bulkhead\"\n        R2[Requests] --&gt; B{Bulkhead}\n        B --&gt; S2A[Service Pool A&lt;br/&gt;Max: 5]\n        B --&gt; S2B[Service Pool B&lt;br/&gt;Max: 5]\n        S2A --&gt; P1[Partial Failure]\n        S2B --&gt; OK[Still Working]\n    end\n    \n    style F1 fill:#ef5350\n    style P1 fill:#ffa726\n    style OK fill:#66bb6a",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Architecture Diagrams"
    ]
  },
  {
    "objectID": "architecture-diagrams.html#sdk-usage-patterns",
    "href": "architecture-diagrams.html#sdk-usage-patterns",
    "title": "Architecture Diagrams",
    "section": "SDK Usage Patterns",
    "text": "SDK Usage Patterns\n\nRepository Pattern\n\ngraph TB\n    subgraph \"Application Layer\"\n        API[API Handler]\n        CLI[CLI Command]\n        JOB[Background Job]\n    end\n    \n    subgraph \"Repository Layer\"\n        UR[UserRepository]\n        subgraph \"Operations\"\n            GET[GetUser]\n            CREATE[CreateUser]\n            UPDATE[UpdateUser]\n            DELETE[DeleteUser]\n        end\n    end\n    \n    subgraph \"Data Layer\"\n        CACHE[(Cache)]\n        DB[(Database)]\n    end\n    \n    API --&gt; UR\n    CLI --&gt; UR\n    JOB --&gt; UR\n    \n    UR --&gt; GET\n    UR --&gt; CREATE\n    UR --&gt; UPDATE\n    UR --&gt; DELETE\n    \n    GET --&gt; CACHE\n    GET --&gt; DB\n    CREATE --&gt; DB\n    UPDATE --&gt; DB\n    UPDATE --&gt; CACHE\n    DELETE --&gt; DB\n    DELETE --&gt; CACHE\n    \n    style UR fill:#e8f5e9\n    style GET fill:#c8e6c9\n    style CREATE fill:#c8e6c9\n    style UPDATE fill:#c8e6c9\n    style DELETE fill:#c8e6c9\n\n\n\n\ngraph TB\n    subgraph \"Application Layer\"\n        API[API Handler]\n        CLI[CLI Command]\n        JOB[Background Job]\n    end\n    \n    subgraph \"Repository Layer\"\n        UR[UserRepository]\n        subgraph \"Operations\"\n            GET[GetUser]\n            CREATE[CreateUser]\n            UPDATE[UpdateUser]\n            DELETE[DeleteUser]\n        end\n    end\n    \n    subgraph \"Data Layer\"\n        CACHE[(Cache)]\n        DB[(Database)]\n    end\n    \n    API --&gt; UR\n    CLI --&gt; UR\n    JOB --&gt; UR\n    \n    UR --&gt; GET\n    UR --&gt; CREATE\n    UR --&gt; UPDATE\n    UR --&gt; DELETE\n    \n    GET --&gt; CACHE\n    GET --&gt; DB\n    CREATE --&gt; DB\n    UPDATE --&gt; DB\n    UPDATE --&gt; CACHE\n    DELETE --&gt; DB\n    DELETE --&gt; CACHE\n    \n    style UR fill:#e8f5e9\n    style GET fill:#c8e6c9\n    style CREATE fill:#c8e6c9\n    style UPDATE fill:#c8e6c9\n    style DELETE fill:#c8e6c9\n\n\n\n\n\n\n\n\nService Layer Pattern\n\ngraph TB\n    subgraph \"Presentation\"\n        REST[REST API]\n        GQL[GraphQL]\n        CLI[CLI]\n    end\n    \n    subgraph \"Service Layer\"\n        US[UserService]\n        OS[OrderService]\n        PS[PaymentService]\n    end\n    \n    subgraph \"Business Logic\"\n        subgraph \"User Domain\"\n            UV[UserValidator]\n            UE[UserEnricher]\n        end\n        subgraph \"Order Domain\"\n            OV[OrderValidator]\n            OP[OrderProcessor]\n        end\n        subgraph \"Payment Domain\"\n            PV[PaymentValidator]\n            PP[PaymentProcessor]\n        end\n    end\n    \n    subgraph \"Data Access\"\n        REPO[Repositories]\n        EXT[External Services]\n    end\n    \n    REST --&gt; US\n    REST --&gt; OS\n    GQL --&gt; US\n    GQL --&gt; OS\n    CLI --&gt; PS\n    \n    US --&gt; UV --&gt; UE\n    OS --&gt; OV --&gt; OP\n    PS --&gt; PV --&gt; PP\n    \n    UE --&gt; REPO\n    OP --&gt; REPO\n    PP --&gt; EXT\n    \n    style US fill:#e3f2fd\n    style OS fill:#e3f2fd\n    style PS fill:#e3f2fd\n\n\n\n\ngraph TB\n    subgraph \"Presentation\"\n        REST[REST API]\n        GQL[GraphQL]\n        CLI[CLI]\n    end\n    \n    subgraph \"Service Layer\"\n        US[UserService]\n        OS[OrderService]\n        PS[PaymentService]\n    end\n    \n    subgraph \"Business Logic\"\n        subgraph \"User Domain\"\n            UV[UserValidator]\n            UE[UserEnricher]\n        end\n        subgraph \"Order Domain\"\n            OV[OrderValidator]\n            OP[OrderProcessor]\n        end\n        subgraph \"Payment Domain\"\n            PV[PaymentValidator]\n            PP[PaymentProcessor]\n        end\n    end\n    \n    subgraph \"Data Access\"\n        REPO[Repositories]\n        EXT[External Services]\n    end\n    \n    REST --&gt; US\n    REST --&gt; OS\n    GQL --&gt; US\n    GQL --&gt; OS\n    CLI --&gt; PS\n    \n    US --&gt; UV --&gt; UE\n    OS --&gt; OV --&gt; OP\n    PS --&gt; PV --&gt; PP\n    \n    UE --&gt; REPO\n    OP --&gt; REPO\n    PP --&gt; EXT\n    \n    style US fill:#e3f2fd\n    style OS fill:#e3f2fd\n    style PS fill:#e3f2fd",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Architecture Diagrams"
    ]
  },
  {
    "objectID": "architecture-diagrams.html#performance-optimization",
    "href": "architecture-diagrams.html#performance-optimization",
    "title": "Architecture Diagrams",
    "section": "Performance Optimization",
    "text": "Performance Optimization\n\nCaching Layers\n\ngraph TB\n    Request[Request] --&gt; L1{L1 Cache&lt;br/&gt;Memory}\n    L1 --&gt;|Hit| Response1[Fast Response&lt;br/&gt;~0.01ms]\n    L1 --&gt;|Miss| L2{L2 Cache&lt;br/&gt;Redis}\n    L2 --&gt;|Hit| Response2[Quick Response&lt;br/&gt;~1ms]\n    L2 --&gt;|Miss| L3[Database]\n    L3 --&gt; Response3[Slow Response&lt;br/&gt;~50ms]\n    \n    L3 -.-&gt; L2\n    L2 -.-&gt; L1\n    \n    style L1 fill:#c8e6c9\n    style L2 fill:#fff9c4\n    style L3 fill:#ffccbc\n\n\n\n\ngraph TB\n    Request[Request] --&gt; L1{L1 Cache&lt;br/&gt;Memory}\n    L1 --&gt;|Hit| Response1[Fast Response&lt;br/&gt;~0.01ms]\n    L1 --&gt;|Miss| L2{L2 Cache&lt;br/&gt;Redis}\n    L2 --&gt;|Hit| Response2[Quick Response&lt;br/&gt;~1ms]\n    L2 --&gt;|Miss| L3[Database]\n    L3 --&gt; Response3[Slow Response&lt;br/&gt;~50ms]\n    \n    L3 -.-&gt; L2\n    L2 -.-&gt; L1\n    \n    style L1 fill:#c8e6c9\n    style L2 fill:#fff9c4\n    style L3 fill:#ffccbc\n\n\n\n\n\n\n\n\nPipeline Optimization\n\ngraph LR\n    subgraph \"Before Optimization\"\n        A1[Step A&lt;br/&gt;10ms] --&gt; B1[Step B&lt;br/&gt;20ms] --&gt; C1[Step C&lt;br/&gt;15ms]\n    end\n    \n    subgraph \"With Caching\"\n        A2[Step A&lt;br/&gt;10ms] --&gt; Cache{Cached?}\n        Cache --&gt;|Yes| Result1[0.1ms]\n        Cache --&gt;|No| B2[Step B&lt;br/&gt;20ms] --&gt; C2[Step C&lt;br/&gt;15ms]\n    end\n    \n    subgraph \"With Fusion\"\n        ABC[Fused A+B+C&lt;br/&gt;35ms total]\n    end\n    \n    subgraph \"With Parallelization\"\n        A3[Step A]\n        B3[Step B]\n        C3[Step C]\n        Input --&gt; A3\n        Input --&gt; B3\n        Input --&gt; C3\n        A3 --&gt; Merge\n        B3 --&gt; Merge\n        C3 --&gt; Merge\n    end\n\n\n\n\ngraph LR\n    subgraph \"Before Optimization\"\n        A1[Step A&lt;br/&gt;10ms] --&gt; B1[Step B&lt;br/&gt;20ms] --&gt; C1[Step C&lt;br/&gt;15ms]\n    end\n    \n    subgraph \"With Caching\"\n        A2[Step A&lt;br/&gt;10ms] --&gt; Cache{Cached?}\n        Cache --&gt;|Yes| Result1[0.1ms]\n        Cache --&gt;|No| B2[Step B&lt;br/&gt;20ms] --&gt; C2[Step C&lt;br/&gt;15ms]\n    end\n    \n    subgraph \"With Fusion\"\n        ABC[Fused A+B+C&lt;br/&gt;35ms total]\n    end\n    \n    subgraph \"With Parallelization\"\n        A3[Step A]\n        B3[Step B]\n        C3[Step C]\n        Input --&gt; A3\n        Input --&gt; B3\n        Input --&gt; C3\n        A3 --&gt; Merge\n        B3 --&gt; Merge\n        C3 --&gt; Merge\n    end",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Architecture Diagrams"
    ]
  },
  {
    "objectID": "architecture-diagrams.html#summary",
    "href": "architecture-diagrams.html#summary",
    "title": "Architecture Diagrams",
    "section": "Summary",
    "text": "Summary\nThese diagrams illustrate:\n\nCore Architecture: How the protocol and base classes work\nComposition Patterns: Different ways to combine nanobricks\nSkills System: How capabilities are added dynamically\nSystem Architectures: Building complete systems\nDeployment Patterns: Production deployment strategies\nPerformance Patterns: Optimization techniques\n\nEach pattern can be combined with others to create sophisticated, production-ready systems while maintaining simplicity at the component level.",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Architecture Diagrams"
    ]
  },
  {
    "objectID": "superpowers.html",
    "href": "superpowers.html",
    "title": "Skills",
    "section": "",
    "text": "Traditional “batteries included” implies heavy dependencies always present. Skills are different - they’re latent capabilities that:\n\nRemain dormant until activated\nDon’t add complexity when unused\nCan be mixed and matched\nFollow the same nanobrick interface",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "Skills"
    ]
  },
  {
    "objectID": "superpowers.html#concept-skills-vs-batteries",
    "href": "superpowers.html#concept-skills-vs-batteries",
    "title": "Skills",
    "section": "",
    "text": "Traditional “batteries included” implies heavy dependencies always present. Skills are different - they’re latent capabilities that:\n\nRemain dormant until activated\nDon’t add complexity when unused\nCan be mixed and matched\nFollow the same nanobrick interface",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "Skills"
    ]
  },
  {
    "objectID": "superpowers.html#core-skills",
    "href": "superpowers.html#core-skills",
    "title": "Skills",
    "section": "Core Skills",
    "text": "Core Skills\n\nAPI Skill (FastAPI)\nTransform any nanobrick into a REST API:\n@skill\nclass SkillAPI:\n    \"\"\"Adds REST API endpoints to a nanobrick\"\"\"\n    \n    def enhance(self, brick: Nanobrick) -&gt; Nanobrick:\n        # Create FastAPI app\n        app = FastAPI(title=f\"{brick.name} API\")\n        \n        # Add standard endpoints\n        @app.post(\"/invoke\")\n        async def invoke(input: brick.InputType) -&gt; brick.OutputType:\n            return await brick.invoke(input)\n        \n        @app.post(\"/batch\")\n        async def batch(inputs: List[brick.InputType]) -&gt; List[brick.OutputType]:\n            return await brick.batch(inputs)\n        \n        # Attach to brick\n        brick._api = app\n        brick.as_api = lambda: app\n        \n        return brick\nUsage:\n# Create API from any nanobrick\nvalidator = ValidatorData().with_skill(SkillAPI())\napi = validator.as_api()\n\n# Mount into existing app\nmain_app = FastAPI()\nmain_app.mount(\"/validate\", api)\n\n\nCLI Skill (Typer)\nAdd command-line interface:\n@skill\nclass SkillCLI:\n    \"\"\"Adds CLI commands to a nanobrick\"\"\"\n    \n    def enhance(self, brick: Nanobrick) -&gt; Nanobrick:\n        app = typer.Typer()\n        \n        @app.command()\n        def invoke(input_file: Path, output_file: Path):\n            \"\"\"Process input through the nanobrick\"\"\"\n            data = json.loads(input_file.read_text())\n            # Use asyncio.run() here since this is CLI context, not Jupyter\n            result = asyncio.run(brick.invoke(data))\n            output_file.write_text(json.dumps(result))\n        \n        brick._cli = app\n        brick.as_cli = lambda: app\n        \n        return brick\n\n\nUI Skill (Streamlit)\nCreate web UI components:\n@skill\nclass SkillStreamlit:\n    \"\"\"Adds Streamlit UI to a nanobrick\"\"\"\n    \n    component_type: str = \"page\"  # page, tab, sidebar\n    \n    def enhance(self, brick: Nanobrick) -&gt; Nanobrick:\n        def render():\n            st.title(f\"{brick.name}\")\n            \n            # Input section\n            input_data = st.text_area(\"Input\", height=200)\n            \n            if st.button(\"Process\"):\n                try:\n                    data = json.loads(input_data)\n                    # Use asyncio.run() here since this is Streamlit context, not Jupyter\n                    result = asyncio.run(brick.invoke(data))\n                    st.success(\"Processed successfully!\")\n                    st.json(result)\n                except Exception as e:\n                    st.error(f\"Error: {e}\")\n        \n        brick.as_ui = render\n        return brick\n\n\nDatabase Skill (SQLModel)\nAdd persistence capabilities:\n@skill\nclass SkillDatabase:\n    \"\"\"Adds database persistence to a nanobrick\"\"\"\n    \n    model_class: Type[SQLModel]\n    \n    def enhance(self, brick: Nanobrick) -&gt; Nanobrick:\n        # Add CRUD operations\n        async def save(data: dict) -&gt; int:\n            instance = self.model_class(**data)\n            session.add(instance)\n            await session.commit()\n            return instance.id\n        \n        async def load(id: int) -&gt; dict:\n            instance = await session.get(self.model_class, id)\n            return instance.dict()\n        \n        brick.save = save\n        brick.load = load\n        \n        return brick\n\n\nAI Skill (LLM Integration)\nAdd reasoning capabilities:\n@skill\nclass SkillAI:\n    \"\"\"Adds AI/LLM capabilities to a nanobrick\"\"\"\n    \n    model: str = \"gpt-4\"\n    temperature: float = 0.7\n    \n    def enhance(self, brick: Nanobrick) -&gt; Nanobrick:\n        async def think(context: dict) -&gt; str:\n            \"\"\"Use AI to reason about the context\"\"\"\n            prompt = f\"\"\"\n            You are helping a {brick.name} nanobrick.\n            Context: {json.dumps(context)}\n            \n            Provide insights or suggestions.\n            \"\"\"\n            \n            return await llm_call(self.model, prompt, self.temperature)\n        \n        brick.think = think\n        \n        # Enhance invoke with AI\n        original_invoke = brick.invoke\n        \n        async def ai_invoke(input, **kwargs):\n            # Get original result\n            result = await original_invoke(input, **kwargs)\n            \n            # Add AI insights if needed\n            if kwargs.get('use_ai', False):\n                insights = await think({'input': input, 'result': result})\n                result['ai_insights'] = insights\n            \n            return result\n        \n        brick.invoke = ai_invoke\n        \n        return brick",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "Skills"
    ]
  },
  {
    "objectID": "superpowers.html#skill-composition",
    "href": "superpowers.html#skill-composition",
    "title": "Skills",
    "section": "Skill Composition",
    "text": "Skill Composition\n\nMultiple Skills\n# Add multiple capabilities\nsmart_validator = (\n    ValidatorData()\n    .with_skill(SkillAPI())\n    .with_skill(SkillCLI())\n    .with_skill(SkillAI(model=\"claude-3\"))\n)\n\n# Now it can be used as:\n# - A function: result = await smart_validator.invoke(data)\n# - An API: app = smart_validator.as_api()\n# - A CLI: cli = smart_validator.as_cli()\n# - With AI: result = await smart_validator.invoke(data, use_ai=True)\n\n\nConditional Enhancement\nclass NanobrickAdaptive(Nanobrick):\n    def __init__(self, enable_ai: bool = False):\n        if enable_ai:\n            self.with_skill(SkillAI())\n        \n        if os.getenv('ENABLE_API'):\n            self.with_skill(SkillAPI())",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "Skills"
    ]
  },
  {
    "objectID": "superpowers.html#creating-custom-skills",
    "href": "superpowers.html#creating-custom-skills",
    "title": "Skills",
    "section": "Creating Custom Skills",
    "text": "Creating Custom Skills\n@skill\nclass SkillCaching:\n    \"\"\"Adds caching to any nanobrick\"\"\"\n    \n    ttl: int = 3600  # seconds\n    \n    def enhance(self, brick: Nanobrick) -&gt; Nanobrick:\n        cache = {}\n        \n        original_invoke = brick.invoke\n        \n        async def cached_invoke(input, **kwargs):\n            cache_key = hash(json.dumps(input, sort_keys=True))\n            \n            if cache_key in cache:\n                age = time.time() - cache[cache_key]['time']\n                if age &lt; self.ttl:\n                    return cache[cache_key]['result']\n            \n            result = await original_invoke(input, **kwargs)\n            cache[cache_key] = {'result': result, 'time': time.time()}\n            \n            return result\n        \n        brick.invoke = cached_invoke\n        return brick",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "Skills"
    ]
  },
  {
    "objectID": "superpowers.html#skill-guidelines",
    "href": "superpowers.html#skill-guidelines",
    "title": "Skills",
    "section": "Skill Guidelines",
    "text": "Skill Guidelines\n\nMinimal Interface: Skills should enhance, not replace\nLazy Loading: Import dependencies only when activated\nComposable: Multiple skills should work together\nReversible: Consider allowing deactivation\nDocumented: Clear description of what changes",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "Skills"
    ]
  },
  {
    "objectID": "superpowers.html#additional-core-skills",
    "href": "superpowers.html#additional-core-skills",
    "title": "Skills",
    "section": "Additional Core Skills",
    "text": "Additional Core Skills\n\nLogging Skill (Loguru)\nProfessional logging with zero configuration:\n@skill\nclass SkillLogging:\n    \"\"\"Adds structured logging to a nanobrick\"\"\"\n    \n    level: str = \"INFO\"\n    format: str = \"{time} | {level} | {brick.name} | {message}\"\n    rotation: str = \"10 MB\"\n    \n    def enhance(self, brick: Nanobrick) -&gt; Nanobrick:\n        from loguru import logger\n        \n        # Configure logger for this brick\n        brick_logger = logger.bind(brick_name=brick.name)\n        brick_logger.add(\n            f\"logs/{brick.name}.log\",\n            level=self.level,\n            format=self.format,\n            rotation=self.rotation\n        )\n        \n        # Wrap invoke with logging\n        original_invoke = brick.invoke\n        \n        async def logged_invoke(input, **kwargs):\n            brick_logger.info(f\"Invoking with input: {input}\")\n            try:\n                result = await original_invoke(input, **kwargs)\n                brick_logger.success(f\"Successfully processed\")\n                return result\n            except Exception as e:\n                brick_logger.error(f\"Failed: {e}\")\n                raise\n        \n        brick.invoke = logged_invoke\n        brick.logger = brick_logger\n        \n        return brick\n\n\nObservability Skill (OpenTelemetry)\nFull observability with tracing, metrics, and logs:\n@skill\nclass SkillObservability:\n    \"\"\"Adds observability to a nanobrick\"\"\"\n    \n    service_name: str = None\n    exporter: str = \"jaeger\"  # jaeger, prometheus, zipkin\n    \n    def enhance(self, brick: Nanobrick) -&gt; Nanobrick:\n        from opentelemetry import trace, metrics\n        \n        tracer = trace.get_tracer(self.service_name or brick.name)\n        meter = metrics.get_meter(self.service_name or brick.name)\n        \n        # Create metrics\n        invocation_counter = meter.create_counter(\n            f\"{brick.name}_invocations\",\n            description=f\"Number of invocations of {brick.name}\"\n        )\n        \n        duration_histogram = meter.create_histogram(\n            f\"{brick.name}_duration\",\n            description=f\"Duration of {brick.name} invocations\"\n        )\n        \n        # Wrap with tracing\n        original_invoke = brick.invoke\n        \n        async def traced_invoke(input, **kwargs):\n            with tracer.start_as_current_span(f\"{brick.name}.invoke\") as span:\n                span.set_attribute(\"input.size\", len(str(input)))\n                span.set_attribute(\"brick.name\", brick.name)\n                \n                start_time = time.time()\n                try:\n                    result = await original_invoke(input, **kwargs)\n                    span.set_status(trace.Status(trace.StatusCode.OK))\n                    return result\n                except Exception as e:\n                    span.record_exception(e)\n                    span.set_status(trace.Status(trace.StatusCode.ERROR))\n                    raise\n                finally:\n                    duration = time.time() - start_time\n                    invocation_counter.add(1)\n                    duration_histogram.record(duration)\n        \n        brick.invoke = traced_invoke\n        brick.tracer = tracer\n        brick.meter = meter\n        \n        return brick\n\n\nDeployment Skills\n\nDocker Skill\n@skill\nclass SkillDocker:\n    \"\"\"Makes nanobrick Docker-ready\"\"\"\n    \n    base_image: str = \"python:3.13-slim\"\n    expose_port: int = 8000\n    \n    def enhance(self, brick: Nanobrick) -&gt; Nanobrick:\n        # Generate Dockerfile\n        dockerfile_content = f\"\"\"\n        FROM {self.base_image}\n        \n        WORKDIR /app\n        \n        # Install dependencies\n        COPY requirements.txt .\n        RUN pip install -r requirements.txt\n        \n        # Copy brick code\n        COPY . .\n        \n        # Expose port if API skill is active\n        {\"EXPOSE \" + str(self.expose_port) if hasattr(brick, 'as_api') else \"\"}\n        \n        # Run the brick\n        CMD [\"python\", \"-m\", \"nanobricks\", \"run\", \"{brick.name}\"]\n        \"\"\"\n        \n        # Generate docker-compose.yml\n        compose_content = f\"\"\"\n        version: '3.8'\n        services:\n          {brick.name}:\n            build: .\n            image: {brick.name}:latest\n            {\"ports:\" if hasattr(brick, 'as_api') else \"\"}\n            {\"  - \" + str(self.expose_port) + \":\" + str(self.expose_port) if hasattr(brick, 'as_api') else \"\"}\n            environment:\n              - NANOBRICK_NAME={brick.name}\n            volumes:\n              - ./data:/app/data\n            restart: unless-stopped\n        \"\"\"\n        \n        brick.dockerfile = dockerfile_content\n        brick.docker_compose = compose_content\n        \n        def save_docker_files():\n            Path(\"Dockerfile\").write_text(dockerfile_content)\n            Path(\"docker-compose.yml\").write_text(compose_content)\n        \n        brick.save_docker_files = save_docker_files\n        \n        return brick\n\n\nKubernetes Skill\n@skill\nclass SkillKubernetes:\n    \"\"\"Makes nanobrick Kubernetes-ready with Helm charts\"\"\"\n    \n    namespace: str = \"nanobricks\"\n    replicas: int = 3\n    \n    def enhance(self, brick: Nanobrick) -&gt; Nanobrick:\n        # Generate Helm chart structure\n        helm_values = f\"\"\"\n        replicaCount: {self.replicas}\n        \n        image:\n          repository: {brick.name}\n          tag: latest\n          pullPolicy: IfNotPresent\n        \n        service:\n          type: ClusterIP\n          port: 80\n          targetPort: 8000\n        \n        resources:\n          limits:\n            cpu: 1000m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        \n        autoscaling:\n          enabled: true\n          minReplicas: 1\n          maxReplicas: 10\n          targetCPUUtilizationPercentage: 80\n        \"\"\"\n        \n        deployment_template = f\"\"\"\n        apiVersion: apps/v1\n        kind: Deployment\n        metadata:\n          name: {{ include \"{brick.name}.fullname\" . }}\n          labels:\n            {{- include \"{brick.name}.labels\" . | nindent 4 }}\n        spec:\n          replicas: {{ .Values.replicaCount }}\n          selector:\n            matchLabels:\n              {{- include \"{brick.name}.selectorLabels\" . | nindent 6 }}\n          template:\n            metadata:\n              labels:\n                {{- include \"{brick.name}.selectorLabels\" . | nindent 8 }}\n            spec:\n              containers:\n              - name: {{ .Chart.Name }}\n                image: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\"\n                imagePullPolicy: {{ .Values.image.pullPolicy }}\n                ports:\n                - name: http\n                  containerPort: {{ .Values.service.targetPort }}\n                  protocol: TCP\n                resources:\n                  {{- toYaml .Values.resources | nindent 12 }}\n        \"\"\"\n        \n        brick.helm_values = helm_values\n        brick.helm_deployment = deployment_template\n        \n        return brick",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "Skills"
    ]
  },
  {
    "objectID": "superpowers.html#skill-registry-discovery",
    "href": "superpowers.html#skill-registry-discovery",
    "title": "Skills",
    "section": "Skill Registry & Discovery",
    "text": "Skill Registry & Discovery\n\nLazy Loading Registry\nclass SkillRegistry:\n    \"\"\"Central registry for all skills with lazy loading\"\"\"\n    \n    _registry: Dict[str, Type[Skill]] = {}\n    _loaded: Dict[str, Skill] = {}\n    \n    @classmethod\n    def register(cls, name: str, skill_class: Type[Skill]):\n        \"\"\"Register a skill for lazy loading\"\"\"\n        cls._registry[name] = skill_class\n    \n    @classmethod\n    def get(cls, name: str, **config) -&gt; Skill:\n        \"\"\"Get or create a skill instance\"\"\"\n        if name not in cls._loaded:\n            if name not in cls._registry:\n                raise ValueError(f\"Unknown skill: {name}\")\n            \n            # Lazy load and instantiate\n            skill_class = cls._registry[name]\n            cls._loaded[name] = skill_class(**config)\n        \n        return cls._loaded[name]\n    \n    @classmethod\n    def preload(cls, names: List[str]):\n        \"\"\"Preload specific skills for performance\"\"\"\n        for name in names:\n            cls.get(name)\n\n# Register built-in skills\nSkillRegistry.register(\"api\", SkillAPI)\nSkillRegistry.register(\"cli\", SkillCLI)\nSkillRegistry.register(\"ui\", SkillStreamlit)\nSkillRegistry.register(\"db\", SkillDatabase)\nSkillRegistry.register(\"ai\", SkillAI)\nSkillRegistry.register(\"logging\", SkillLogging)\nSkillRegistry.register(\"observability\", SkillObservability)\nSkillRegistry.register(\"docker\", SkillDocker)\nSkillRegistry.register(\"kubernetes\", SkillKubernetes)\n\n\nConfiguration-Based Activation\n# From TOML configuration\nbrick = ValidatorData()\n\n# Load skills from config\nconfig = Config.from_file(\"nanobrick.toml\")\nfor name, settings in config.skills.items():\n    if settings.get(\"enabled\", False):\n        skill = SkillRegistry.get(name, **settings)\n        brick = brick.with_skill(skill)\n\n# Preload known skills\nif config.get(\"performance.preload_skills\"):\n    SkillRegistry.preload([\"api\", \"logging\", \"observability\"])",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "Skills"
    ]
  },
  {
    "objectID": "superpowers.html#future-skills",
    "href": "superpowers.html#future-skills",
    "title": "Skills",
    "section": "Future Skills",
    "text": "Future Skills\n\nSecurity: Authentication, authorization, encryption\nCaching: Redis, Memcached integration\nMessaging: Kafka, RabbitMQ, NATS\nScheduling: Cron, Celery integration\nWorkflow: State machines, BPMN\nGraphQL: Alternative to REST API\nWebSocket: Real-time communication\ngRPC: High-performance RPC",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "Skills"
    ]
  },
  {
    "objectID": "ai-protocols.html",
    "href": "ai-protocols.html",
    "title": "AI Agent Protocols",
    "section": "",
    "text": "The AI agent ecosystem is converging on several protocols for standardized communication. Understanding these helps us choose the right integration strategy for Nanobricks.",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "AI Agent Protocols"
    ]
  },
  {
    "objectID": "ai-protocols.html#overview",
    "href": "ai-protocols.html#overview",
    "title": "AI Agent Protocols",
    "section": "",
    "text": "The AI agent ecosystem is converging on several protocols for standardized communication. Understanding these helps us choose the right integration strategy for Nanobricks.",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "AI Agent Protocols"
    ]
  },
  {
    "objectID": "ai-protocols.html#protocol-comparison",
    "href": "ai-protocols.html#protocol-comparison",
    "title": "AI Agent Protocols",
    "section": "Protocol Comparison",
    "text": "Protocol Comparison\n\nMCP (Model Context Protocol) - Anthropic\nPurpose: Standardizes how applications provide context to LLMs\nKey Features: - Universal “USB-C port for AI applications” - Client-server architecture - Tool and data source exposure - Vendor-agnostic LLM interactions\nBest For: Exposing nanobrick capabilities as tools/resources to LLMs\nExample Integration:\nclass SkillMCP:\n    \"\"\"MCP server for nanobricks\"\"\"\n    \n    def expose_as_tool(self, nanobrick: Nanobrick):\n        return {\n            \"name\": nanobrick.name,\n            \"description\": nanobrick.__doc__,\n            \"input_schema\": nanobrick.input_schema,\n            \"handler\": nanobrick.invoke\n        }\n\n\nA2A (Agent-to-Agent) - Google\nPurpose: Enable communication between opaque agentic applications\nKey Features: - JSON-RPC 2.0 over HTTP(S) - Agent discovery via “Agent Cards” - Preserves agent opacity (no internal state exposure) - Cross-ecosystem collaboration\nBest For: Nanobricks communicating with other AI agents\nExample Integration:\nclass SkillA2A:\n    \"\"\"A2A protocol for inter-agent communication\"\"\"\n    \n    def create_agent_card(self, nanobrick: Nanobrick):\n        return {\n            \"id\": f\"nanobrick.{nanobrick.name}\",\n            \"capabilities\": nanobrick.list_capabilities(),\n            \"endpoints\": {\n                \"invoke\": \"/invoke\",\n                \"stream\": \"/stream\"\n            }\n        }\n\n\nAG-UI (Agent User Interaction)\nPurpose: Standardize front-end to AI agent connections\nKey Features: - Event-driven protocol (16 event types) - Real-time state streaming - Human-in-the-loop collaboration - Transport-agnostic (SSE, WebSockets, webhooks)\nBest For: Creating interactive UIs for nanobricks\nExample Integration:\nclass SkillAGUI:\n    \"\"\"AG-UI for interactive nanobrick interfaces\"\"\"\n    \n    def emit_event(self, event_type: str, data: dict):\n        return {\n            \"type\": event_type,\n            \"timestamp\": datetime.now().isoformat(),\n            \"source\": f\"nanobrick.{self.name}\",\n            \"data\": data\n        }\n\n\nACP (Agent Communication Protocol) - Linux Foundation\nPurpose: Open standard for agent interoperability\nKey Features: - REST-based communication - Async-first, sync supported - No SDK required (plain HTTP) - Offline discovery via metadata\nBest For: RESTful agent interactions with broad compatibility\nExample Integration:\nclass SkillACP:\n    \"\"\"ACP-compliant REST interface\"\"\"\n    \n    @app.post(\"/agent/invoke\")\n    async def invoke(self, request: ACPRequest):\n        # Standard REST endpoint\n        return {\n            \"status\": \"success\",\n            \"result\": await self.nanobrick.invoke(request.input)\n        }",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "AI Agent Protocols"
    ]
  },
  {
    "objectID": "ai-protocols.html#protocol-selection-matrix",
    "href": "ai-protocols.html#protocol-selection-matrix",
    "title": "AI Agent Protocols",
    "section": "Protocol Selection Matrix",
    "text": "Protocol Selection Matrix\n\n\n\nUse Case\nRecommended Protocol\nWhy\n\n\n\n\nLLM tool exposure\nMCP\nDesigned for LLM context\n\n\nAgent collaboration\nA2A\nPreserves agent autonomy\n\n\nInteractive UIs\nAG-UI\nEvent-driven interactivity\n\n\nGeneral interop\nACP\nREST-based simplicity",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "AI Agent Protocols"
    ]
  },
  {
    "objectID": "ai-protocols.html#integration-strategy-for-nanobricks",
    "href": "ai-protocols.html#integration-strategy-for-nanobricks",
    "title": "AI Agent Protocols",
    "section": "Integration Strategy for Nanobricks",
    "text": "Integration Strategy for Nanobricks\n\n1. Primary Protocol: MCP\nRationale: - Anthropic-backed with growing adoption - Clean tool/resource abstraction - Fits nanobrick philosophy\n\n\n2. Secondary Support: A2A\nRationale: - Google backing ensures longevity - Enables multi-agent workflows - Complements MCP\n\n\n3. UI Layer: AG-UI\nRationale: - Purpose-built for UI interaction - Works with Streamlit skill - Real-time capabilities\n\n\n4. Fallback: ACP\nRationale: - Simple REST compatibility - No SDK dependencies - Broad accessibility",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "AI Agent Protocols"
    ]
  },
  {
    "objectID": "ai-protocols.html#implementation-approach",
    "href": "ai-protocols.html#implementation-approach",
    "title": "AI Agent Protocols",
    "section": "Implementation Approach",
    "text": "Implementation Approach\n\nPhase 1: MCP Foundation\n@nanobrick\nclass ValidatorWithMCP:\n    def __init__(self):\n        self.add_skill(SkillMCP())\n    \n    def get_mcp_tools(self):\n        return [{\n            \"name\": \"validate\",\n            \"description\": \"Validate input data\",\n            \"parameters\": self.input_schema\n        }]\n\n\nPhase 2: Multi-Protocol Support\nclass NanobrickProtocolAdapter:\n    \"\"\"Adapt nanobricks to multiple protocols\"\"\"\n    \n    def __init__(self, nanobrick: Nanobrick):\n        self.nanobrick = nanobrick\n        self.protocols = {\n            'mcp': SkillMCP(),\n            'a2a': SkillA2A(),\n            'agui': SkillAGUI(),\n            'acp': SkillACP()\n        }\n    \n    def enable_protocol(self, protocol: str):\n        self.nanobrick.add_skill(self.protocols[protocol])\n\n\nPhase 3: Protocol Bridge\nclass ProtocolBridge:\n    \"\"\"Bridge between different AI protocols\"\"\"\n    \n    async def mcp_to_a2a(self, mcp_tool, a2a_agent):\n        # Convert MCP tool to A2A agent card\n        pass\n    \n    async def agui_to_acp(self, agui_event, acp_endpoint):\n        # Convert AG-UI event to ACP REST call\n        pass",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "AI Agent Protocols"
    ]
  },
  {
    "objectID": "ai-protocols.html#best-practices",
    "href": "ai-protocols.html#best-practices",
    "title": "AI Agent Protocols",
    "section": "Best Practices",
    "text": "Best Practices\n\nStart Simple: Begin with one protocol (MCP recommended)\nProgressive Enhancement: Add protocols as needed\nMaintain Abstraction: Keep protocol details in skills\nDocument Capabilities: Clear metadata for each protocol\nTest Interoperability: Verify cross-protocol communication",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "AI Agent Protocols"
    ]
  },
  {
    "objectID": "ai-protocols.html#future-considerations",
    "href": "ai-protocols.html#future-considerations",
    "title": "AI Agent Protocols",
    "section": "Future Considerations",
    "text": "Future Considerations\n\nProtocol Convergence: Watch for standardization efforts\nPerformance: Monitor overhead of multiple protocols\nSecurity: Each protocol has different security models\nVersioning: Protocols will evolve; plan for updates",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "AI Agent Protocols"
    ]
  },
  {
    "objectID": "ai-protocols.html#resources",
    "href": "ai-protocols.html#resources",
    "title": "AI Agent Protocols",
    "section": "Resources",
    "text": "Resources\n\nMCP Specification\nA2A GitHub\nAG-UI Documentation\nACP Standard",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "AI Agent Protocols"
    ]
  },
  {
    "objectID": "production-examples.html",
    "href": "production-examples.html",
    "title": "Production Examples",
    "section": "",
    "text": "This guide contains complete, production-ready examples showing how to build real applications with Nanobricks. Each example includes error handling, monitoring, deployment configuration, and best practices.",
    "crumbs": [
      "Home",
      "Building Systems",
      "Production Examples"
    ]
  },
  {
    "objectID": "production-examples.html#introduction",
    "href": "production-examples.html#introduction",
    "title": "Production Examples",
    "section": "",
    "text": "This guide contains complete, production-ready examples showing how to build real applications with Nanobricks. Each example includes error handling, monitoring, deployment configuration, and best practices.",
    "crumbs": [
      "Home",
      "Building Systems",
      "Production Examples"
    ]
  },
  {
    "objectID": "production-examples.html#example-1-user-management-api",
    "href": "production-examples.html#example-1-user-management-api",
    "title": "Production Examples",
    "section": "Example 1: User Management API",
    "text": "Example 1: User Management API\nA complete user management service with authentication, validation, and database persistence.\n\nArchitecture Overview\n\ngraph LR\n    A[HTTP Request] --&gt; B[Rate Limiter]\n    B --&gt; C[Authenticator]\n    C --&gt; D[Request Validator]\n    D --&gt; E[User Service]\n    E --&gt; F[Database]\n    E --&gt; G[Cache]\n    E --&gt; H[Event Bus]\n    H --&gt; I[Email Service]\n    H --&gt; J[Analytics]\n\n\n\n\ngraph LR\n    A[HTTP Request] --&gt; B[Rate Limiter]\n    B --&gt; C[Authenticator]\n    C --&gt; D[Request Validator]\n    D --&gt; E[User Service]\n    E --&gt; F[Database]\n    E --&gt; G[Cache]\n    E --&gt; H[Event Bus]\n    H --&gt; I[Email Service]\n    H --&gt; J[Analytics]\n\n\n\n\n\n\n\n\nImplementation\n\nfrom nanobricks import NanobrickBase, NanobrickSimple, create_service\nfrom nanobricks.skills import skill\nfrom nanobricks.patterns import Pipeline, Branch, Fallback\nfrom nanobricks.security import hash_password, verify_password\nimport asyncpg\nimport redis.asyncio as redis\nfrom typing import Optional, Dict, List\nfrom datetime import datetime, timedelta\nimport jwt\n\n# Configuration\nJWT_SECRET = \"your-secret-key\"\nJWT_ALGORITHM = \"HS256\"\nJWT_EXPIRATION = timedelta(hours=24)\n\n# Dependencies\nclass ServiceDeps(TypedDict):\n    db: asyncpg.Pool\n    cache: redis.Redis\n    event_bus: EventBus\n    config: Dict\n\n# Data Models\nfrom pydantic import BaseModel, EmailStr\n\nclass UserCreate(BaseModel):\n    email: EmailStr\n    password: str\n    full_name: str\n\nclass User(BaseModel):\n    id: str\n    email: str\n    full_name: str\n    created_at: datetime\n    active: bool = True\n\nclass LoginRequest(BaseModel):\n    email: EmailStr\n    password: str\n\n# Core Nanobricks\nclass ValidateUserData(NanobrickSimple[UserCreate, UserCreate]):\n    \"\"\"Validates user registration data.\"\"\"\n    \n    async def invoke(self, data: UserCreate, *, deps=None) -&gt; UserCreate:\n        if len(data.password) &lt; 8:\n            raise ValueError(\"Password must be at least 8 characters\")\n        \n        if not data.full_name.strip():\n            raise ValueError(\"Full name is required\")\n        \n        return data\n\nclass CheckEmailUniqueness(NanobrickBase[UserCreate, UserCreate, ServiceDeps]):\n    \"\"\"Ensures email is not already registered.\"\"\"\n    \n    async def invoke(self, data: UserCreate, *, deps: ServiceDeps) -&gt; UserCreate:\n        existing = await deps[\"db\"].fetchrow(\n            \"SELECT id FROM users WHERE email = $1\",\n            data.email\n        )\n        \n        if existing:\n            raise ValueError(f\"Email {data.email} is already registered\")\n        \n        return data\n\nclass HashUserPassword(NanobrickSimple[UserCreate, Dict]):\n    \"\"\"Hashes the user password securely.\"\"\"\n    \n    async def invoke(self, data: UserCreate, *, deps=None) -&gt; Dict:\n        user_dict = data.dict()\n        user_dict[\"password_hash\"] = await hash_password(data.password)\n        del user_dict[\"password\"]\n        return user_dict\n\nclass CreateUserInDB(NanobrickBase[Dict, User, ServiceDeps]):\n    \"\"\"Creates user in database.\"\"\"\n    \n    async def invoke(self, user_data: Dict, *, deps: ServiceDeps) -&gt; User:\n        result = await deps[\"db\"].fetchrow(\"\"\"\n            INSERT INTO users (email, password_hash, full_name, created_at)\n            VALUES ($1, $2, $3, $4)\n            RETURNING id, email, full_name, created_at, active\n        \"\"\", \n            user_data[\"email\"],\n            user_data[\"password_hash\"],\n            user_data[\"full_name\"],\n            datetime.utcnow()\n        )\n        \n        return User(**dict(result))\n\nclass CacheUser(NanobrickBase[User, User, ServiceDeps]):\n    \"\"\"Caches user data for fast retrieval.\"\"\"\n    \n    async def invoke(self, user: User, *, deps: ServiceDeps) -&gt; User:\n        cache_key = f\"user:{user.id}\"\n        await deps[\"cache\"].setex(\n            cache_key,\n            3600,  # 1 hour TTL\n            user.json()\n        )\n        return user\n\nclass EmitUserCreatedEvent(NanobrickBase[User, User, ServiceDeps]):\n    \"\"\"Emits event for other services to react.\"\"\"\n    \n    async def invoke(self, user: User, *, deps: ServiceDeps) -&gt; User:\n        await deps[\"event_bus\"].emit(\"user.created\", {\n            \"user_id\": user.id,\n            \"email\": user.email,\n            \"timestamp\": datetime.utcnow().isoformat()\n        })\n        return user\n\n# Compose the registration pipeline\nregistration_pipeline = Pipeline(\n    ValidateUserData(),\n    CheckEmailUniqueness(),\n    HashUserPassword(),\n    CreateUserInDB(),\n    CacheUser(),\n    EmitUserCreatedEvent()\n).with_skill(\"logging\", level=\"INFO\")\n .with_skill(\"observability\", service_name=\"user-service\")\n\n# Authentication bricks\nclass ValidateCredentials(NanobrickBase[LoginRequest, User, ServiceDeps]):\n    \"\"\"Validates user credentials.\"\"\"\n    \n    async def invoke(self, request: LoginRequest, *, deps: ServiceDeps) -&gt; User:\n        result = await deps[\"db\"].fetchrow(\"\"\"\n            SELECT id, email, password_hash, full_name, created_at, active\n            FROM users WHERE email = $1\n        \"\"\", request.email)\n        \n        if not result:\n            raise ValueError(\"Invalid credentials\")\n        \n        if not await verify_password(request.password, result[\"password_hash\"]):\n            raise ValueError(\"Invalid credentials\")\n        \n        return User(\n            id=result[\"id\"],\n            email=result[\"email\"],\n            full_name=result[\"full_name\"],\n            created_at=result[\"created_at\"],\n            active=result[\"active\"]\n        )\n\nclass GenerateJWT(NanobrickSimple[User, Dict]):\n    \"\"\"Generates JWT token for authenticated user.\"\"\"\n    \n    async def invoke(self, user: User, *, deps=None) -&gt; Dict:\n        payload = {\n            \"sub\": user.id,\n            \"email\": user.email,\n            \"exp\": datetime.utcnow() + JWT_EXPIRATION\n        }\n        \n        token = jwt.encode(payload, JWT_SECRET, algorithm=JWT_ALGORITHM)\n        \n        return {\n            \"access_token\": token,\n            \"token_type\": \"bearer\",\n            \"user\": user.dict()\n        }\n\n# Login pipeline\nlogin_pipeline = Pipeline(\n    ValidateCredentials(),\n    GenerateJWT()\n).with_skill(\"logging\")\n\n# User lookup with caching\nclass GetUserById(NanobrickBase[str, User, ServiceDeps]):\n    \"\"\"Gets user by ID with cache fallback.\"\"\"\n    \n    async def invoke(self, user_id: str, *, deps: ServiceDeps) -&gt; User:\n        # Try cache first\n        cache_key = f\"user:{user_id}\"\n        cached = await deps[\"cache\"].get(cache_key)\n        \n        if cached:\n            return User.parse_raw(cached)\n        \n        # Load from database\n        result = await deps[\"db\"].fetchrow(\"\"\"\n            SELECT id, email, full_name, created_at, active\n            FROM users WHERE id = $1\n        \"\"\", user_id)\n        \n        if not result:\n            raise ValueError(f\"User {user_id} not found\")\n        \n        user = User(**dict(result))\n        \n        # Update cache\n        await deps[\"cache\"].setex(cache_key, 3600, user.json())\n        \n        return user\n\n# Create the complete service\n@skill(\"api\", port=8000, title=\"User Management API\")\n@skill(\"docker\", base_image=\"python:3.13-slim\")\n@skill(\"kubernetes\", replicas=3, autoscale=True)\nclass UserManagementService:\n    \"\"\"Complete user management service.\"\"\"\n    \n    def __init__(self, deps: ServiceDeps):\n        self.deps = deps\n        self.register = registration_pipeline\n        self.login = login_pipeline\n        self.get_user = GetUserById()\n    \n    async def create_user(self, user_data: UserCreate) -&gt; User:\n        \"\"\"POST /users - Create new user\"\"\"\n        return await self.register.invoke(user_data, deps=self.deps)\n    \n    async def authenticate(self, credentials: LoginRequest) -&gt; Dict:\n        \"\"\"POST /auth/login - Authenticate user\"\"\"\n        return await self.login.invoke(credentials, deps=self.deps)\n    \n    async def get_user_by_id(self, user_id: str) -&gt; User:\n        \"\"\"GET /users/{user_id} - Get user by ID\"\"\"\n        return await self.get_user.invoke(user_id, deps=self.deps)\n    \n    async def health_check(self) -&gt; Dict:\n        \"\"\"GET /health - Health check endpoint\"\"\"\n        return {\n            \"status\": \"healthy\",\n            \"timestamp\": datetime.utcnow().isoformat(),\n            \"service\": \"user-management\"\n        }\n\n# Deployment configuration\nif __name__ == \"__main__\":\n    import asyncio\n    \n    async def main():\n        # Initialize dependencies\n        db = await asyncpg.create_pool(\"postgresql://localhost/users\")\n        cache = await redis.from_url(\"redis://localhost\")\n        event_bus = EventBus()\n        \n        deps = ServiceDeps(\n            db=db,\n            cache=cache,\n            event_bus=event_bus,\n            config={\"environment\": \"production\"}\n        )\n        \n        # Create and start service\n        service = UserManagementService(deps)\n        await service.start_server()\n    \n    asyncio.run(main())\n\n\n\nTesting\n\nimport pytest\nfrom nanobricks.testing import create_test_client\n\n@pytest.fixture\nasync def test_deps():\n    \"\"\"Create test dependencies with mocks.\"\"\"\n    return ServiceDeps(\n        db=MockDatabase(),\n        cache=MockCache(),\n        event_bus=MockEventBus(),\n        config={\"environment\": \"test\"}\n    )\n\n@pytest.mark.asyncio\nasync def test_user_registration(test_deps):\n    service = UserManagementService(test_deps)\n    \n    user_data = UserCreate(\n        email=\"test@example.com\",\n        password=\"securepass123\",\n        full_name=\"Test User\"\n    )\n    \n    user = await service.create_user(user_data)\n    \n    assert user.email == \"test@example.com\"\n    assert user.full_name == \"Test User\"\n    assert user.id is not None\n\n@pytest.mark.asyncio\nasync def test_duplicate_email(test_deps):\n    service = UserManagementService(test_deps)\n    \n    # Create first user\n    user_data = UserCreate(\n        email=\"test@example.com\",\n        password=\"securepass123\",\n        full_name=\"Test User\"\n    )\n    await service.create_user(user_data)\n    \n    # Try to create with same email\n    with pytest.raises(ValueError, match=\"already registered\"):\n        await service.create_user(user_data)\n\n\n\nDeployment\n# docker-compose.yml\nversion: '3.8'\n\nservices:\n  user-service:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - DATABASE_URL=postgresql://postgres:password@db/users\n      - REDIS_URL=redis://redis:6379\n    depends_on:\n      - db\n      - redis\n  \n  db:\n    image: postgres:15\n    environment:\n      - POSTGRES_DB=users\n      - POSTGRES_PASSWORD=password\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n  \n  redis:\n    image: redis:7-alpine\n    volumes:\n      - redis_data:/data\n\nvolumes:\n  postgres_data:\n  redis_data:",
    "crumbs": [
      "Home",
      "Building Systems",
      "Production Examples"
    ]
  },
  {
    "objectID": "production-examples.html#example-2-real-time-data-processing-pipeline",
    "href": "production-examples.html#example-2-real-time-data-processing-pipeline",
    "title": "Production Examples",
    "section": "Example 2: Real-time Data Processing Pipeline",
    "text": "Example 2: Real-time Data Processing Pipeline\nA production data pipeline that processes streaming data with validation, transformation, and storage.\n\nArchitecture\n\ngraph TB\n    A[Kafka Stream] --&gt; B[Message Consumer]\n    B --&gt; C[Schema Validator]\n    C --&gt; D[Data Enricher]\n    D --&gt; E[Business Rules]\n    E --&gt; F{Router}\n    F --&gt;|Type A| G[Processor A]\n    F --&gt;|Type B| H[Processor B]\n    G --&gt; I[Aggregator]\n    H --&gt; I\n    I --&gt; J[Data Lake]\n    I --&gt; K[Analytics DB]\n    I --&gt; L[Real-time Dashboard]\n\n\n\n\ngraph TB\n    A[Kafka Stream] --&gt; B[Message Consumer]\n    B --&gt; C[Schema Validator]\n    C --&gt; D[Data Enricher]\n    D --&gt; E[Business Rules]\n    E --&gt; F{Router}\n    F --&gt;|Type A| G[Processor A]\n    F --&gt;|Type B| H[Processor B]\n    G --&gt; I[Aggregator]\n    H --&gt; I\n    I --&gt; J[Data Lake]\n    I --&gt; K[Analytics DB]\n    I --&gt; L[Real-time Dashboard]\n\n\n\n\n\n\n\n\nImplementation\n\nfrom nanobricks import NanobrickBase, NanobrickSimple, create_pipeline\nfrom nanobricks.patterns import Router, Parallel, Window\nfrom typing import Dict, List, Any\nimport aiokafka\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n# Data models\nclass SensorReading(BaseModel):\n    sensor_id: str\n    timestamp: datetime\n    temperature: float\n    humidity: float\n    location: Dict[str, float]  # lat, lon\n\nclass EnrichedReading(SensorReading):\n    city: str\n    country: str\n    weather_conditions: str\n    anomaly_score: float\n\n# Processing bricks\nclass ValidateSensorData(NanobrickSimple[Dict, SensorReading]):\n    \"\"\"Validates and parses raw sensor data.\"\"\"\n    \n    async def invoke(self, raw_data: Dict, *, deps=None) -&gt; SensorReading:\n        try:\n            reading = SensorReading(**raw_data)\n            \n            # Validate ranges\n            if not -50 &lt;= reading.temperature &lt;= 70:\n                raise ValueError(f\"Invalid temperature: {reading.temperature}\")\n            \n            if not 0 &lt;= reading.humidity &lt;= 100:\n                raise ValueError(f\"Invalid humidity: {reading.humidity}\")\n            \n            return reading\n            \n        except Exception as e:\n            # Log to dead letter queue\n            raise ValueError(f\"Invalid sensor data: {e}\")\n\nclass EnrichWithLocation(NanobrickBase[SensorReading, EnrichedReading, ServiceDeps]):\n    \"\"\"Enriches reading with location data.\"\"\"\n    \n    async def invoke(self, reading: SensorReading, *, deps: ServiceDeps) -&gt; EnrichedReading:\n        # Get location from cache or API\n        location_key = f\"{reading.location['lat']},{reading.location['lon']}\"\n        location_data = await deps[\"cache\"].get(location_key)\n        \n        if not location_data:\n            # Call geocoding API\n            location_data = await deps[\"geocoding_api\"].reverse_geocode(\n                reading.location['lat'],\n                reading.location['lon']\n            )\n            await deps[\"cache\"].setex(location_key, 86400, location_data)\n        \n        return EnrichedReading(\n            **reading.dict(),\n            city=location_data['city'],\n            country=location_data['country'],\n            weather_conditions=\"normal\",  # Would call weather API\n            anomaly_score=0.0\n        )\n\nclass DetectAnomalies(NanobrickBase[EnrichedReading, EnrichedReading, ServiceDeps]):\n    \"\"\"Detects anomalies using ML model.\"\"\"\n    \n    async def invoke(self, reading: EnrichedReading, *, deps: ServiceDeps) -&gt; EnrichedReading:\n        # Use pre-trained model\n        model = deps[\"ml_models\"][\"anomaly_detector\"]\n        \n        features = [\n            reading.temperature,\n            reading.humidity,\n            reading.timestamp.hour,\n            reading.timestamp.weekday()\n        ]\n        \n        anomaly_score = model.predict_proba([features])[0][1]\n        reading.anomaly_score = float(anomaly_score)\n        \n        if anomaly_score &gt; 0.8:\n            # Trigger alert\n            await deps[\"event_bus\"].emit(\"anomaly.detected\", {\n                \"sensor_id\": reading.sensor_id,\n                \"score\": anomaly_score,\n                \"reading\": reading.dict()\n            })\n        \n        return reading\n\nclass AggregateReadings(NanobrickSimple[List[EnrichedReading], Dict]):\n    \"\"\"Aggregates readings for analytics.\"\"\"\n    \n    async def invoke(self, readings: List[EnrichedReading], *, deps=None) -&gt; Dict:\n        df = pd.DataFrame([r.dict() for r in readings])\n        \n        return {\n            \"timestamp\": datetime.utcnow(),\n            \"count\": len(readings),\n            \"avg_temperature\": df['temperature'].mean(),\n            \"avg_humidity\": df['humidity'].mean(),\n            \"anomaly_count\": len(df[df['anomaly_score'] &gt; 0.5]),\n            \"by_city\": df.groupby('city').agg({\n                'temperature': 'mean',\n                'humidity': 'mean',\n                'sensor_id': 'count'\n            }).to_dict()\n        }\n\n# Create processing pipeline\n@skill(\"monitoring\", metrics=[\"throughput\", \"latency\", \"errors\"])\n@skill(\"scaling\", auto_scale=True, min_instances=2, max_instances=10)\nclass SensorDataPipeline:\n    \"\"\"Real-time sensor data processing pipeline.\"\"\"\n    \n    def __init__(self, deps: ServiceDeps):\n        self.deps = deps\n        \n        # Main processing pipeline\n        self.process_reading = Pipeline(\n            ValidateSensorData(),\n            EnrichWithLocation(),\n            DetectAnomalies()\n        ).with_skill(\"retry\", max_attempts=3)\n        \n        # Aggregation window\n        self.windowed_aggregation = Window(\n            window_size=timedelta(minutes=5),\n            aggregator=AggregateReadings()\n        )\n        \n        # Storage writers\n        self.lake_writer = DataLakeWriter()\n        self.analytics_writer = AnalyticsDBWriter()\n        self.dashboard_updater = DashboardUpdater()\n        \n        # Parallel output\n        self.output_handlers = Parallel([\n            self.lake_writer,\n            self.analytics_writer,\n            self.dashboard_updater\n        ])\n    \n    async def start(self):\n        \"\"\"Start consuming from Kafka.\"\"\"\n        consumer = aiokafka.AIOKafkaConsumer(\n            'sensor-readings',\n            bootstrap_servers='localhost:9092',\n            group_id='sensor-processor',\n            value_deserializer=lambda m: json.loads(m.decode('utf-8'))\n        )\n        \n        await consumer.start()\n        \n        try:\n            async for msg in consumer:\n                try:\n                    # Process individual reading\n                    reading = await self.process_reading.invoke(\n                        msg.value,\n                        deps=self.deps\n                    )\n                    \n                    # Add to aggregation window\n                    await self.windowed_aggregation.add(reading)\n                    \n                    # Check if window is ready\n                    if self.windowed_aggregation.is_ready():\n                        aggregated = await self.windowed_aggregation.invoke()\n                        await self.output_handlers.invoke(aggregated, deps=self.deps)\n                    \n                except Exception as e:\n                    # Send to dead letter queue\n                    await self.deps[\"dead_letter_queue\"].send({\n                        \"error\": str(e),\n                        \"message\": msg.value,\n                        \"timestamp\": datetime.utcnow()\n                    })\n                    \n        finally:\n            await consumer.stop()\n\n# Monitoring and alerting\nclass PipelineMonitor(NanobrickSimple[Dict, None]):\n    \"\"\"Monitors pipeline health and sends alerts.\"\"\"\n    \n    def __init__(self, thresholds: Dict):\n        super().__init__()\n        self.thresholds = thresholds\n    \n    async def invoke(self, metrics: Dict, *, deps=None) -&gt; None:\n        # Check error rate\n        if metrics['error_rate'] &gt; self.thresholds['max_error_rate']:\n            await deps[\"alerting\"].send_alert(\n                severity=\"high\",\n                message=f\"High error rate: {metrics['error_rate']:.2%}\",\n                runbook_url=\"https://wiki/sensor-pipeline-errors\"\n            )\n        \n        # Check latency\n        if metrics['p99_latency'] &gt; self.thresholds['max_latency_ms']:\n            await deps[\"alerting\"].send_alert(\n                severity=\"medium\",\n                message=f\"High latency: {metrics['p99_latency']}ms\",\n                runbook_url=\"https://wiki/sensor-pipeline-latency\"\n            )\n\n\n\nPerformance Optimization\n\n# Add performance optimizations\noptimized_pipeline = (\n    SensorDataPipeline(deps)\n    .with_skill(\"cache\", ttl=300)\n    .with_skill(\"batch\", size=100)\n    .with_skill(\"profile\", trace=True)\n)\n\n# Configure for high throughput\nhigh_throughput_config = {\n    \"consumer\": {\n        \"fetch_min_bytes\": 1024 * 1024,  # 1MB\n        \"max_poll_records\": 500\n    },\n    \"processing\": {\n        \"parallelism\": 8,\n        \"batch_timeout_ms\": 100\n    },\n    \"output\": {\n        \"compression\": \"snappy\",\n        \"batch_size\": 1000\n    }\n}",
    "crumbs": [
      "Home",
      "Building Systems",
      "Production Examples"
    ]
  },
  {
    "objectID": "production-examples.html#example-3-e-commerce-order-processing-system",
    "href": "production-examples.html#example-3-e-commerce-order-processing-system",
    "title": "Production Examples",
    "section": "Example 3: E-commerce Order Processing System",
    "text": "Example 3: E-commerce Order Processing System\nA complete order processing system with inventory management, payment processing, and fulfillment.\n\nSystem Architecture\n\ngraph TB\n    subgraph \"Order Service\"\n        A[Order API] --&gt; B[Order Validator]\n        B --&gt; C[Inventory Check]\n        C --&gt; D[Price Calculator]\n        D --&gt; E[Payment Processor]\n    end\n    \n    subgraph \"Inventory Service\"\n        C --&gt; F[Stock Manager]\n        F --&gt; G[Warehouse API]\n    end\n    \n    subgraph \"Payment Service\"\n        E --&gt; H[Payment Gateway]\n        H --&gt; I[Fraud Detection]\n    end\n    \n    subgraph \"Fulfillment Service\"\n        E --&gt; J[Order Queue]\n        J --&gt; K[Shipping Calculator]\n        K --&gt; L[Label Generator]\n        L --&gt; M[Tracking Service]\n    end\n\n\n\n\ngraph TB\n    subgraph \"Order Service\"\n        A[Order API] --&gt; B[Order Validator]\n        B --&gt; C[Inventory Check]\n        C --&gt; D[Price Calculator]\n        D --&gt; E[Payment Processor]\n    end\n    \n    subgraph \"Inventory Service\"\n        C --&gt; F[Stock Manager]\n        F --&gt; G[Warehouse API]\n    end\n    \n    subgraph \"Payment Service\"\n        E --&gt; H[Payment Gateway]\n        H --&gt; I[Fraud Detection]\n    end\n    \n    subgraph \"Fulfillment Service\"\n        E --&gt; J[Order Queue]\n        J --&gt; K[Shipping Calculator]\n        K --&gt; L[Label Generator]\n        L --&gt; M[Tracking Service]\n    end\n\n\n\n\n\n\n\n\nImplementation\n\nfrom nanobricks import create_microservice, NanobrickBase\nfrom nanobricks.patterns import Saga, Compensate\nfrom decimal import Decimal\nimport stripe\nimport asyncio\n\n# Domain models\nclass OrderItem(BaseModel):\n    product_id: str\n    quantity: int\n    unit_price: Decimal\n\nclass Order(BaseModel):\n    id: str\n    customer_id: str\n    items: List[OrderItem]\n    shipping_address: Dict\n    total: Decimal = Decimal(\"0\")\n    status: str = \"pending\"\n\n# Inventory management\nclass CheckInventory(NanobrickBase[Order, Order, ServiceDeps]):\n    \"\"\"Checks and reserves inventory.\"\"\"\n    \n    async def invoke(self, order: Order, *, deps: ServiceDeps) -&gt; Order:\n        inventory_service = deps[\"inventory_service\"]\n        \n        # Check availability for all items\n        for item in order.items:\n            available = await inventory_service.check_stock(\n                item.product_id,\n                item.quantity\n            )\n            \n            if not available:\n                raise ValueError(\n                    f\"Insufficient stock for product {item.product_id}\"\n                )\n        \n        # Reserve inventory\n        reservation_id = await inventory_service.reserve_items(\n            [(item.product_id, item.quantity) for item in order.items],\n            duration_minutes=30  # 30 minute reservation\n        )\n        \n        # Store reservation ID for potential rollback\n        order.metadata = {\"reservation_id\": reservation_id}\n        \n        return order\n    \n    async def compensate(self, order: Order, *, deps: ServiceDeps) -&gt; None:\n        \"\"\"Release inventory reservation on failure.\"\"\"\n        if reservation_id := order.metadata.get(\"reservation_id\"):\n            await deps[\"inventory_service\"].release_reservation(reservation_id)\n\nclass CalculatePricing(NanobrickSimple[Order, Order]):\n    \"\"\"Calculates order total with taxes and discounts.\"\"\"\n    \n    def __init__(self, tax_rate: Decimal = Decimal(\"0.08\")):\n        super().__init__()\n        self.tax_rate = tax_rate\n    \n    async def invoke(self, order: Order, *, deps=None) -&gt; Order:\n        subtotal = sum(\n            item.quantity * item.unit_price \n            for item in order.items\n        )\n        \n        # Apply discounts\n        discount = await self.calculate_discount(order, subtotal)\n        \n        # Calculate tax\n        taxable_amount = subtotal - discount\n        tax = taxable_amount * self.tax_rate\n        \n        # Add shipping\n        shipping = await self.calculate_shipping(order)\n        \n        order.total = taxable_amount + tax + shipping\n        order.breakdown = {\n            \"subtotal\": subtotal,\n            \"discount\": discount,\n            \"tax\": tax,\n            \"shipping\": shipping,\n            \"total\": order.total\n        }\n        \n        return order\n\nclass ProcessPayment(NanobrickBase[Order, Order, ServiceDeps]):\n    \"\"\"Processes payment through payment gateway.\"\"\"\n    \n    async def invoke(self, order: Order, *, deps: ServiceDeps) -&gt; Order:\n        stripe.api_key = deps[\"config\"][\"stripe_api_key\"]\n        \n        try:\n            # Create payment intent\n            intent = stripe.PaymentIntent.create(\n                amount=int(order.total * 100),  # Convert to cents\n                currency=\"usd\",\n                customer=order.customer_id,\n                metadata={\n                    \"order_id\": order.id\n                }\n            )\n            \n            # Process payment\n            payment = stripe.PaymentIntent.confirm(\n                intent.id,\n                payment_method=order.payment_method_id\n            )\n            \n            if payment.status == \"succeeded\":\n                order.status = \"paid\"\n                order.payment_id = payment.id\n            else:\n                raise ValueError(f\"Payment failed: {payment.status}\")\n                \n        except stripe.error.CardError as e:\n            raise ValueError(f\"Card declined: {e.user_message}\")\n        \n        return order\n    \n    async def compensate(self, order: Order, *, deps: ServiceDeps) -&gt; None:\n        \"\"\"Refund payment on failure.\"\"\"\n        if payment_id := getattr(order, \"payment_id\", None):\n            stripe.Refund.create(payment_intent=payment_id)\n\nclass CreateFulfillment(NanobrickBase[Order, Order, ServiceDeps]):\n    \"\"\"Creates fulfillment order for shipping.\"\"\"\n    \n    async def invoke(self, order: Order, *, deps: ServiceDeps) -&gt; Order:\n        fulfillment_service = deps[\"fulfillment_service\"]\n        \n        # Calculate shipping options\n        shipping_options = await fulfillment_service.calculate_shipping(\n            items=order.items,\n            destination=order.shipping_address\n        )\n        \n        # Select best option (could be user preference)\n        selected_option = min(\n            shipping_options,\n            key=lambda x: x[\"cost\"] + x[\"days\"] * 5  # Balance cost and speed\n        )\n        \n        # Create fulfillment order\n        fulfillment = await fulfillment_service.create_order({\n            \"order_id\": order.id,\n            \"items\": order.items,\n            \"shipping_address\": order.shipping_address,\n            \"shipping_method\": selected_option[\"method\"],\n            \"expected_delivery\": selected_option[\"estimated_delivery\"]\n        })\n        \n        order.fulfillment_id = fulfillment[\"id\"]\n        order.tracking_number = fulfillment[\"tracking_number\"]\n        order.status = \"processing\"\n        \n        return order\n\n# Create order processing saga\norder_saga = Saga(\n    name=\"order_processing\",\n    steps=[\n        CheckInventory(),\n        CalculatePricing(),\n        ProcessPayment(),\n        CreateFulfillment()\n    ],\n    compensation_strategy=\"reverse\"  # Compensate in reverse order\n).with_skill(\"monitoring\", track_compensation=True)\n\n# Order service with event sourcing\n@skill(\"api\", port=8001)\n@skill(\"event_sourcing\", store=\"postgresql\")\nclass OrderProcessingService:\n    \"\"\"Complete order processing service.\"\"\"\n    \n    def __init__(self, deps: ServiceDeps):\n        self.deps = deps\n        self.saga = order_saga\n        \n        # Event handlers for async processing\n        self.event_handlers = {\n            \"order.created\": self.handle_order_created,\n            \"payment.completed\": self.handle_payment_completed,\n            \"fulfillment.shipped\": self.handle_order_shipped\n        }\n    \n    async def create_order(self, order_data: Dict) -&gt; Order:\n        \"\"\"POST /orders - Create new order\"\"\"\n        order = Order(**order_data)\n        order.id = generate_order_id()\n        \n        try:\n            # Process through saga\n            processed_order = await self.saga.invoke(order, deps=self.deps)\n            \n            # Emit success event\n            await self.deps[\"event_bus\"].emit(\"order.completed\", {\n                \"order\": processed_order.dict(),\n                \"timestamp\": datetime.utcnow()\n            })\n            \n            return processed_order\n            \n        except Exception as e:\n            # Saga will handle compensation\n            await self.deps[\"event_bus\"].emit(\"order.failed\", {\n                \"order_id\": order.id,\n                \"error\": str(e),\n                \"timestamp\": datetime.utcnow()\n            })\n            raise\n    \n    async def handle_order_shipped(self, event: Dict):\n        \"\"\"Handle shipment notification.\"\"\"\n        order_id = event[\"order_id\"]\n        tracking_info = event[\"tracking_info\"]\n        \n        # Update order status\n        await self.deps[\"db\"].execute(\"\"\"\n            UPDATE orders \n            SET status = 'shipped',\n                tracking_number = $1,\n                shipped_at = $2\n            WHERE id = $3\n        \"\"\", tracking_info[\"tracking_number\"], datetime.utcnow(), order_id)\n        \n        # Notify customer\n        await self.deps[\"notification_service\"].send_email(\n            template=\"order_shipped\",\n            order_id=order_id,\n            tracking_info=tracking_info\n        )\n\n\n\nTesting with Saga Compensation\n\n@pytest.mark.asyncio\nasync def test_payment_failure_compensation():\n    \"\"\"Test that inventory is released when payment fails.\"\"\"\n    \n    # Mock payment to fail\n    deps = create_test_deps()\n    deps[\"payment_service\"].mock_failure = True\n    \n    service = OrderProcessingService(deps)\n    \n    order_data = {\n        \"customer_id\": \"cust_123\",\n        \"items\": [\n            {\"product_id\": \"prod_1\", \"quantity\": 2, \"unit_price\": \"29.99\"}\n        ],\n        \"shipping_address\": {...}\n    }\n    \n    # This should fail at payment step\n    with pytest.raises(ValueError, match=\"Payment failed\"):\n        await service.create_order(order_data)\n    \n    # Verify compensation occurred\n    assert deps[\"inventory_service\"].reservations == {}  # Released\n    assert deps[\"metrics\"][\"compensations_triggered\"] == 1",
    "crumbs": [
      "Home",
      "Building Systems",
      "Production Examples"
    ]
  },
  {
    "objectID": "production-examples.html#example-4-ai-powered-customer-support-system",
    "href": "production-examples.html#example-4-ai-powered-customer-support-system",
    "title": "Production Examples",
    "section": "Example 4: AI-Powered Customer Support System",
    "text": "Example 4: AI-Powered Customer Support System\nAn intelligent support system using nanobricks with AI capabilities.\n\nImplementation\n\nfrom nanobricks.ai import AIBrick, ConversationMemory\nfrom nanobricks.skills import create_mcp_server\n\nclass AnalyzeSentiment(AIBrick[str, Dict]):\n    \"\"\"Analyzes customer sentiment using AI.\"\"\"\n    \n    def __init__(self):\n        super().__init__(\n            model=\"gpt-4\",\n            temperature=0.3,\n            max_tokens=100\n        )\n    \n    async def invoke(self, message: str, *, deps=None) -&gt; Dict:\n        response = await self.complete(\n            system=\"Analyze sentiment and urgency of customer message\",\n            user=message,\n            response_format={\"sentiment\": \"positive/neutral/negative\", \"urgency\": \"low/medium/high\"}\n        )\n        return response\n\nclass GenerateResponse(AIBrick[Dict, str]):\n    \"\"\"Generates contextual support response.\"\"\"\n    \n    def __init__(self, knowledge_base: KnowledgeBase):\n        super().__init__(model=\"gpt-4\")\n        self.kb = knowledge_base\n    \n    async def invoke(self, context: Dict, *, deps=None) -&gt; str:\n        # Get relevant knowledge\n        docs = await self.kb.search(\n            context[\"message\"],\n            limit=3\n        )\n        \n        response = await self.complete(\n            system=\"\"\"You are a helpful customer support agent.\n            Use the provided documentation to answer accurately.\n            Be concise and friendly.\"\"\",\n            user=context[\"message\"],\n            context={\"documentation\": docs},\n            max_tokens=500\n        )\n        \n        return response\n\n# Create intelligent support pipeline\nsupport_pipeline = (\n    AnalyzeSentiment()\n    | RouteByUrgency()\n    | LoadCustomerHistory()\n    | GenerateResponse(knowledge_base)\n    | ValidateResponse()\n).with_skill(\"conversation_memory\")\n .with_skill(\"observability\", track_ai_usage=True)",
    "crumbs": [
      "Home",
      "Building Systems",
      "Production Examples"
    ]
  },
  {
    "objectID": "production-examples.html#deployment-best-practices",
    "href": "production-examples.html#deployment-best-practices",
    "title": "Production Examples",
    "section": "Deployment Best Practices",
    "text": "Deployment Best Practices\n\n1. Container Optimization\n# Multi-stage build for smaller images\nFROM python:3.13-slim as builder\n\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install --user -r requirements.txt\n\nFROM python:3.13-slim\nWORKDIR /app\n\n# Copy only necessary files\nCOPY --from=builder /root/.local /root/.local\nCOPY src/ ./src/\nCOPY nanobrick.toml .\n\n# Non-root user\nRUN useradd -m -u 1000 appuser\nUSER appuser\n\nENV PATH=/root/.local/bin:$PATH\nCMD [\"python\", \"-m\", \"nanobricks.run\", \"my-service\"]\n\n\n2. Kubernetes Production Config\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nanobrick-service\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nanobrick-service\n  template:\n    metadata:\n      labels:\n        app: nanobrick-service\n    spec:\n      containers:\n      - name: service\n        image: myregistry/nanobrick-service:v1.0.0\n        ports:\n        - containerPort: 8000\n        env:\n        - name: ENVIRONMENT\n          value: production\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"100m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nanobrick-service\nspec:\n  selector:\n    app: nanobrick-service\n  ports:\n  - port: 80\n    targetPort: 8000\n  type: LoadBalancer\n---\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: nanobrick-service-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: nanobrick-service\n  minReplicas: 3\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n\n\n3. Monitoring Setup\n\nfrom nanobricks.observability import create_monitoring_stack\n\nmonitoring = create_monitoring_stack(\n    service_name=\"my-service\",\n    \n    # Metrics with Prometheus\n    metrics={\n        \"endpoint\": \"http://prometheus:9090\",\n        \"custom_metrics\": [\n            \"orders_processed_total\",\n            \"payment_amount_sum\",\n            \"processing_duration_seconds\"\n        ]\n    },\n    \n    # Tracing with Jaeger\n    tracing={\n        \"endpoint\": \"http://jaeger:14268\",\n        \"sample_rate\": 0.1  # 10% sampling\n    },\n    \n    # Logging with ELK\n    logging={\n        \"endpoint\": \"http://elasticsearch:9200\",\n        \"index\": \"nanobricks-prod\",\n        \"retention_days\": 30\n    },\n    \n    # Alerting rules\n    alerts=[\n        {\n            \"name\": \"high_error_rate\",\n            \"condition\": \"rate(errors_total[5m]) &gt; 0.01\",\n            \"severity\": \"critical\",\n            \"notify\": [\"oncall@company.com\"]\n        },\n        {\n            \"name\": \"high_latency\", \n            \"condition\": \"histogram_quantile(0.99, latency_seconds) &gt; 1\",\n            \"severity\": \"warning\",\n            \"notify\": [\"team@company.com\"]\n        }\n    ]\n)",
    "crumbs": [
      "Home",
      "Building Systems",
      "Production Examples"
    ]
  },
  {
    "objectID": "production-examples.html#performance-tuning",
    "href": "production-examples.html#performance-tuning",
    "title": "Production Examples",
    "section": "Performance Tuning",
    "text": "Performance Tuning\n\nDatabase Connection Pooling\n\nfrom nanobricks.performance import create_connection_pool\n\ndb_pool = create_connection_pool(\n    \"postgresql://localhost/mydb\",\n    min_connections=5,\n    max_connections=20,\n    connection_timeout=5.0,\n    idle_timeout=300.0,\n    max_lifetime=3600.0\n)\n\n# Use with nanobricks\ndeps = ServiceDeps(db=db_pool, ...)\n\n\n\nCaching Strategy\n\nfrom nanobricks.performance import MultiLevelCache\n\ncache = MultiLevelCache(\n    # L1: In-memory LRU cache\n    memory_cache={\"max_size\": 1000, \"ttl\": 60},\n    \n    # L2: Redis cache\n    redis_cache={\"url\": \"redis://localhost\", \"ttl\": 3600},\n    \n    # L3: CDN cache (for read-through)\n    cdn_cache={\"enabled\": True, \"regions\": [\"us-east-1\", \"eu-west-1\"]}\n)\n\n# Apply to expensive operations\ncached_processor = processor.with_skill(\"cache\", cache=cache)",
    "crumbs": [
      "Home",
      "Building Systems",
      "Production Examples"
    ]
  },
  {
    "objectID": "production-examples.html#security-hardening",
    "href": "production-examples.html#security-hardening",
    "title": "Production Examples",
    "section": "Security Hardening",
    "text": "Security Hardening\n\nAPI Security\n\nfrom nanobricks.security import secure_api\n\nsecure_service = secure_api(\n    service,\n    \n    # Authentication\n    auth={\n        \"type\": \"jwt\",\n        \"secret\": JWT_SECRET,\n        \"algorithm\": \"RS256\",\n        \"issuer\": \"https://auth.company.com\"\n    },\n    \n    # Rate limiting per user\n    rate_limit={\n        \"requests_per_minute\": 100,\n        \"burst\": 20,\n        \"key\": lambda req: req.user_id\n    },\n    \n    # Input validation\n    validation={\n        \"max_body_size\": 1024 * 1024,  # 1MB\n        \"allowed_content_types\": [\"application/json\"],\n        \"schema_validation\": True\n    },\n    \n    # Security headers\n    headers={\n        \"X-Frame-Options\": \"DENY\",\n        \"X-Content-Type-Options\": \"nosniff\",\n        \"Strict-Transport-Security\": \"max-age=31536000\"\n    }\n)",
    "crumbs": [
      "Home",
      "Building Systems",
      "Production Examples"
    ]
  },
  {
    "objectID": "production-examples.html#summary",
    "href": "production-examples.html#summary",
    "title": "Production Examples",
    "section": "Summary",
    "text": "Summary\nThese production examples demonstrate:\n\nComplete Systems: Full applications built with nanobricks\nError Handling: Proper error handling and compensation\nTesting: Comprehensive testing strategies\nDeployment: Production-ready deployment configurations\nMonitoring: Observability and alerting setup\nPerformance: Optimization techniques\nSecurity: Hardening for production use\n\nEach example can be adapted and extended for your specific use case. The modular nature of nanobricks makes it easy to swap components, add new features, or scale specific parts of your system independently.",
    "crumbs": [
      "Home",
      "Building Systems",
      "Production Examples"
    ]
  },
  {
    "objectID": "project-history.html",
    "href": "project-history.html",
    "title": "Project History",
    "section": "",
    "text": "In the beginning, there was a dream:\n\n“I’m dreaming of accomplishing this: Coming up with a Python package design that is the code equivalent of ‘antifragile nanobots.’ I call it ‘nanobricks’: atomic, self-sufficient components…”\n\nThis vision laid the foundation for what Nanobricks has become today.",
    "crumbs": [
      "Home",
      "Evolution",
      "Project History"
    ]
  },
  {
    "objectID": "project-history.html#the-original-vision",
    "href": "project-history.html#the-original-vision",
    "title": "Project History",
    "section": "",
    "text": "In the beginning, there was a dream:\n\n“I’m dreaming of accomplishing this: Coming up with a Python package design that is the code equivalent of ‘antifragile nanobots.’ I call it ‘nanobricks’: atomic, self-sufficient components…”\n\nThis vision laid the foundation for what Nanobricks has become today.",
    "crumbs": [
      "Home",
      "Evolution",
      "Project History"
    ]
  },
  {
    "objectID": "project-history.html#initial-concept",
    "href": "project-history.html#initial-concept",
    "title": "Project History",
    "section": "Initial Concept",
    "text": "Initial Concept\nThe original concept outlined five key characteristics:\n\nSimple — Designed and implemented in a straightforward way, easy for both humans and AIs to reason about\nStandardized — Consistent interfaces, like a “Lego Connector Mechanism” for code\nComposable — Easily combined to form more complex components (API integration, Streamlit features, workflow pipelines)\nBatteries included - Self-contained, modular functionality:\n\nAPI (FastAPI)\nCLI (Typer)\nFrontend (Streamlit)\nDB interaction (SQLModel)\n\nScaffoldable - Instant end-to-end usage, inspired by Rails, powered by Task",
    "crumbs": [
      "Home",
      "Evolution",
      "Project History"
    ]
  },
  {
    "objectID": "project-history.html#key-technical-decisions",
    "href": "project-history.html#key-technical-decisions",
    "title": "Project History",
    "section": "Key Technical Decisions",
    "text": "Key Technical Decisions\nFrom the very beginning, certain technical principles were established:\n\nuv-based packages: Every nanobrick as a Python package\nDirectory-based modules: Top-level modules as directories\n\nAbsolute imports: Always use absolute imports\nDesign pattern inspiration: Learn from successful frameworks",
    "crumbs": [
      "Home",
      "Evolution",
      "Project History"
    ]
  },
  {
    "objectID": "project-history.html#evolution-timeline",
    "href": "project-history.html#evolution-timeline",
    "title": "Project History",
    "section": "Evolution Timeline",
    "text": "Evolution Timeline\n\nPhase 1: Core Concept\n\nDefined the Nanobrick interface\nEstablished the Protocol + ABC hybrid approach\nCreated the pipe operator composition pattern\n\n\n\nPhase 2: Skills System\n\nEvolved “batteries” into optional “skills”\nImplemented progressive enhancement\nAdded skill activation patterns\n\n\n\nPhase 3: Type Safety\n\nAdded Python generics support\nImplemented runtime validation strategy\nCreated type-safe composition\n\n\n\nPhase 4: AI Integration\n\nResearched AI protocols (MCP, A2A, AG-UI, ACP)\nChose multi-protocol strategy\nImplemented AI skills\n\n\n\nPhase 5: Production Features\n\nAdded security and performance features\nImplemented hot-swapping\nCreated registry system\n\n\n\nPhase 6: Distribution System\n\nBuilt task-based distribution\nCreated project scaffolding\nEstablished multiple installation methods",
    "crumbs": [
      "Home",
      "Evolution",
      "Project History"
    ]
  },
  {
    "objectID": "project-history.html#lessons-learned",
    "href": "project-history.html#lessons-learned",
    "title": "Project History",
    "section": "Lessons Learned",
    "text": "Lessons Learned\n\nWhat Worked Well\n\nProtocol + ABC Hybrid: Provides both type checking and runtime enforcement\nSkills Pattern: Keeps core simple while enabling rich features\nTask-based Automation: Consistent, powerful build system\nType Safety Strategy: Balance between static and runtime checking\n\n\n\nKey Insights\n\nSimplicity Enables Complexity: Simple rules lead to emergent behaviors\nOptional Everything: Skills activate only when needed\nConvention Over Configuration: Sensible defaults everywhere\nAI-First Design: Patterns that both humans and AI understand",
    "crumbs": [
      "Home",
      "Evolution",
      "Project History"
    ]
  },
  {
    "objectID": "project-history.html#the-present",
    "href": "project-history.html#the-present",
    "title": "Project History",
    "section": "The Present",
    "text": "The Present\nToday, Nanobricks is: - A complete SDK for building composable Python systems - Ready for real-world applications - Distributed via multiple methods - Actively growing its ecosystem",
    "crumbs": [
      "Home",
      "Evolution",
      "Project History"
    ]
  },
  {
    "objectID": "project-history.html#the-future",
    "href": "project-history.html#the-future",
    "title": "Project History",
    "section": "The Future",
    "text": "The Future\nThe journey continues with: - Community contributions - More built-in skills - Enhanced AI capabilities - Visual composition tools - Global nanobrick registry\nThe dream of “antifragile nanobots” for code has become reality, and this is just the beginning.",
    "crumbs": [
      "Home",
      "Evolution",
      "Project History"
    ]
  },
  {
    "objectID": "configuration.html",
    "href": "configuration.html",
    "title": "Configuration & Feature Management",
    "section": "",
    "text": "Nanobricks use TOML for configuration, providing a clean, readable format for managing features, skills, and runtime behavior. This enables both static configuration and dynamic feature flagging.",
    "crumbs": [
      "Home",
      "Building Systems",
      "Configuration & Feature Management"
    ]
  },
  {
    "objectID": "configuration.html#overview",
    "href": "configuration.html#overview",
    "title": "Configuration & Feature Management",
    "section": "",
    "text": "Nanobricks use TOML for configuration, providing a clean, readable format for managing features, skills, and runtime behavior. This enables both static configuration and dynamic feature flagging.",
    "crumbs": [
      "Home",
      "Building Systems",
      "Configuration & Feature Management"
    ]
  },
  {
    "objectID": "configuration.html#configuration-structure",
    "href": "configuration.html#configuration-structure",
    "title": "Configuration & Feature Management",
    "section": "Configuration Structure",
    "text": "Configuration Structure\n\nBasic Nanobrick Configuration\n# nanobrick.toml\n[metadata]\nname = \"ValidatorData\"\nversion = \"1.0.0\"\ndescription = \"Validates data against schemas\"\n\n[skills]\n# Declare available skills\napi = { enabled = false, lazy = true }\ncli = { enabled = true, lazy = true }\nlogging = { enabled = true, provider = \"loguru\" }\nobservability = { enabled = false, provider = \"opentelemetry\" }\n\n[contracts]\n# Define invariants and contracts\npreconditions = [\n    \"input must be dict-like\",\n    \"schema must be provided\"\n]\npostconditions = [\n    \"output is validated data or raises ValidationError\"\n]\n\n[resources]\n# Resource limits and management\nmax_memory_mb = 100\ntimeout_seconds = 30\ncleanup_on_exit = true\n\n[deployment]\n# Deployment configurations\ndocker = { enabled = true, base_image = \"python:3.13-slim\" }\nkubernetes = { enabled = true, helm_chart = \"./charts/data-validator\" }\n\n\nGlobal Configuration\n# ~/.nanobricks/config.toml\n[defaults]\nlazy_loading = true\nenable_contracts = true\nobservability_level = \"INFO\"\n\n[registry]\nsources = [\n    \"https://registry.nanobricks.io\",\n    \"https://company.internal/nanobricks\"\n]\n\n[development]\nhot_reload = true\ndebug_mode = false\ntrace_compositions = true",
    "crumbs": [
      "Home",
      "Building Systems",
      "Configuration & Feature Management"
    ]
  },
  {
    "objectID": "configuration.html#feature-flagging",
    "href": "configuration.html#feature-flagging",
    "title": "Configuration & Feature Management",
    "section": "Feature Flagging",
    "text": "Feature Flagging\n\nStatic Feature Flags\n[features]\n# Feature flags evaluated at load time\nexperimental_branching = false\nenable_hot_swapping = true\nstrict_type_checking = true\n\n[features.dynamic]\n# Features that can change at runtime\ncircuit_breaker = { enabled = true, threshold = 0.5 }\nrate_limiting = { enabled = false, requests_per_minute = 100 }\n\n\nEnvironment-Specific Configuration\n# nanobrick.prod.toml\n[skills]\nlogging = { enabled = true, level = \"WARNING\" }\nobservability = { enabled = true, sampling_rate = 0.1 }\n\n[resources]\nmax_memory_mb = 500\ntimeout_seconds = 60\n\n# nanobrick.dev.toml\n[skills]\nlogging = { enabled = true, level = \"DEBUG\" }\nobservability = { enabled = true, sampling_rate = 1.0 }\n\n[development]\nenable_profiler = true\nmock_external_services = true",
    "crumbs": [
      "Home",
      "Building Systems",
      "Configuration & Feature Management"
    ]
  },
  {
    "objectID": "configuration.html#configuration-api",
    "href": "configuration.html#configuration-api",
    "title": "Configuration & Feature Management",
    "section": "Configuration API",
    "text": "Configuration API\n\nLoading Configuration\nfrom nanobricks import Nanobrick, Config\n\n# Auto-discover configuration\nbrick = ValidatorData()  # Loads nanobrick.toml automatically\n\n# Explicit configuration\nconfig = Config.from_file(\"custom-config.toml\")\nbrick = ValidatorData(config=config)\n\n# Environment-aware loading\nbrick = ValidatorData(env=\"prod\")  # Loads nanobrick.prod.toml\n\n\nRuntime Configuration\n# Update configuration at runtime\nbrick.config.set(\"skills.logging.level\", \"DEBUG\")\n\n# Feature flag checking\nif brick.features.is_enabled(\"experimental_branching\"):\n    result = brick.branch(condition, true_path, false_path)\n\n# Dynamic skill activation\nif brick.config.get(\"skills.api.enabled\"):\n    app = brick.as_api()\n\n\nConfiguration Inheritance\n# base.toml\n[metadata]\nauthor = \"Team Nanobricks\"\nlicense = \"MIT\"\n\n[defaults]\ntimeout = 30\n\n# nanobrick.toml\nextends = \"base.toml\"\n\n[metadata]\nname = \"MyBrick\"  # Overrides not shown, inherits author & license\n\n[defaults]\ntimeout = 60  # Override base timeout",
    "crumbs": [
      "Home",
      "Building Systems",
      "Configuration & Feature Management"
    ]
  },
  {
    "objectID": "configuration.html#skill-configuration",
    "href": "configuration.html#skill-configuration",
    "title": "Configuration & Feature Management",
    "section": "Skill Configuration",
    "text": "Skill Configuration\n\nConditional Activation\n[skills.api]\nenabled = true\nlazy = true\nconditions = [\n    \"environment != 'test'\",\n    \"has_dependency('fastapi')\"\n]\n\n[skills.api.config]\nhost = \"0.0.0.0\"\nport = 8000\nreload = false\n\n\nSkill Dependencies\n[skills.observability]\nenabled = true\nrequires = [\"logging\"]  # Must have logging enabled\n\n[skills.observability.config]\nprovider = \"opentelemetry\"\nexporters = [\"jaeger\", \"prometheus\"]",
    "crumbs": [
      "Home",
      "Building Systems",
      "Configuration & Feature Management"
    ]
  },
  {
    "objectID": "configuration.html#best-practices",
    "href": "configuration.html#best-practices",
    "title": "Configuration & Feature Management",
    "section": "Best Practices",
    "text": "Best Practices\n\n1. Configuration Hierarchy\n~/.nanobricks/config.toml          # User defaults\nproject/nanobricks.toml            # Project defaults\nproject/bricks/my_brick/\n  ├── nanobrick.toml              # Brick configuration\n  ├── nanobrick.dev.toml          # Development overrides\n  ├── nanobrick.prod.toml         # Production overrides\n  └── nanobrick.local.toml        # Local overrides (gitignored)\n\n\n2. Validation\n[config_schema]\n# Optional schema validation for configuration\ntype = \"strict\"\nadditional_properties = false\n\n[config_schema.properties.skills.api.config.port]\ntype = \"integer\"\nminimum = 1024\nmaximum = 65535\n\n\n3. Secret Management\n[secrets]\n# Never store secrets in TOML files\napi_key = \"${NANOBRICK_API_KEY}\"  # Environment variable\ndb_password = \"${vault:database/password}\"  # Vault integration",
    "crumbs": [
      "Home",
      "Building Systems",
      "Configuration & Feature Management"
    ]
  },
  {
    "objectID": "configuration.html#configuration-discovery",
    "href": "configuration.html#configuration-discovery",
    "title": "Configuration & Feature Management",
    "section": "Configuration Discovery",
    "text": "Configuration Discovery\nNanobricks automatically discover configuration in this order:\n\nExplicit config parameter\nEnvironment-specific file (e.g., nanobrick.prod.toml)\nDefault nanobrick.toml in brick directory\nProject-level nanobricks.toml\nUser home directory ~/.nanobricks/config.toml\nSystem defaults\n\nEach level can override previous values, allowing fine-grained control.",
    "crumbs": [
      "Home",
      "Building Systems",
      "Configuration & Feature Management"
    ]
  },
  {
    "objectID": "configuration.html#dynamic-reloading",
    "href": "configuration.html#dynamic-reloading",
    "title": "Configuration & Feature Management",
    "section": "Dynamic Reloading",
    "text": "Dynamic Reloading\n# Enable configuration watching\nbrick.config.watch(on_change=lambda: print(\"Config updated!\"))\n\n# Manual reload\nbrick.config.reload()\n\n# Hot-swapping based on config changes\n@brick.config.on_change(\"skills.api.enabled\")\ndef handle_api_toggle(enabled):\n    if enabled:\n        brick.mount_api()\n    else:\n        brick.unmount_api()",
    "crumbs": [
      "Home",
      "Building Systems",
      "Configuration & Feature Management"
    ]
  },
  {
    "objectID": "configuration.html#integration-with-skills",
    "href": "configuration.html#integration-with-skills",
    "title": "Configuration & Feature Management",
    "section": "Integration with Skills",
    "text": "Integration with Skills\nConfiguration seamlessly integrates with the skill system:\n# Skill reads its config automatically\napi = brick.as_api()  # Uses [skills.api.config]\n\n# Override at activation\napi = brick.as_api(port=9000)  # Overrides config file\n\n# Conditional skill chains\nresult = (\n    brick\n    .with_config({\"logging.level\": \"DEBUG\"})\n    .with_skill(\"observability\")\n    .invoke(data)\n)",
    "crumbs": [
      "Home",
      "Building Systems",
      "Configuration & Feature Management"
    ]
  },
  {
    "objectID": "evolution.html",
    "href": "evolution.html",
    "title": "Concept Evolution",
    "section": "",
    "text": "The original vision established five pillars:\n\nSimple - Straightforward design and implementation\nStandardized - Consistent interfaces\nComposable - Easy combination into complex components\nBatteries Included - Self-contained functionality\nScaffoldable - Rails-inspired instant functionality\n\nKey decisions: - Every nanobrick as a Python package - uv-based package management - src-layout project structure - Absolute imports\n\n\n\nThrough sparring and critical analysis, several refinements emerged:\n\n\n\nPackaging Flexibility\n\nNot every nanobrick must be a package\nSupport for modules within projects\nNano-collections for themed groups\n\nSkills Replace Batteries\n\nLatent capabilities vs. heavy dependencies\nActivate only when needed\nMaintains simplicity principle\n\nComposition Patterns Clarified\n\nPipeline composition (R-style)\nAugmentation composition\nHybrid composition\n\nArchitecture Levels\n\nCore Protocol (minimal)\nSkilled (with capabilities)\nStateful (with memory/learning)",
    "crumbs": [
      "Home",
      "Evolution",
      "Concept Evolution"
    ]
  },
  {
    "objectID": "evolution.html#version-history",
    "href": "evolution.html#version-history",
    "title": "Concept Evolution",
    "section": "",
    "text": "The original vision established five pillars:\n\nSimple - Straightforward design and implementation\nStandardized - Consistent interfaces\nComposable - Easy combination into complex components\nBatteries Included - Self-contained functionality\nScaffoldable - Rails-inspired instant functionality\n\nKey decisions: - Every nanobrick as a Python package - uv-based package management - src-layout project structure - Absolute imports\n\n\n\nThrough sparring and critical analysis, several refinements emerged:\n\n\n\nPackaging Flexibility\n\nNot every nanobrick must be a package\nSupport for modules within projects\nNano-collections for themed groups\n\nSkills Replace Batteries\n\nLatent capabilities vs. heavy dependencies\nActivate only when needed\nMaintains simplicity principle\n\nComposition Patterns Clarified\n\nPipeline composition (R-style)\nAugmentation composition\nHybrid composition\n\nArchitecture Levels\n\nCore Protocol (minimal)\nSkilled (with capabilities)\nStateful (with memory/learning)",
    "crumbs": [
      "Home",
      "Evolution",
      "Concept Evolution"
    ]
  },
  {
    "objectID": "evolution.html#key-insights",
    "href": "evolution.html#key-insights",
    "title": "Concept Evolution",
    "section": "Key Insights",
    "text": "Key Insights\n\nThe Fundamental Tension\n“How do we reconcile ‘simple’ with ‘batteries included’?”\nResolution: Skills as potential energy - capabilities that CAN be activated but don’t add complexity unless used.\n\n\nThe Composition Paradox\n“When you pipe bricks together, what actually flows through?”\nResolution: Clear separation between data flow (through pipes) and capability enhancement (through skills).\n\n\nNature as Guide\n“It’s both super complex and at the same time super simple”\nThis became our guiding principle - simple rules creating complex behaviors through emergence.",
    "crumbs": [
      "Home",
      "Evolution",
      "Concept Evolution"
    ]
  },
  {
    "objectID": "evolution.html#design-decisions-log",
    "href": "evolution.html#design-decisions-log",
    "title": "Concept Evolution",
    "section": "Design Decisions Log",
    "text": "Design Decisions Log\n\n\n\n\n\n\n\n\nDate\nDecision\nRationale\n\n\n\n\n2025-01-22\nAdopt skill model\nReduces complexity while maintaining power\n\n\n2025-01-22\nSupport module-based bricks\nNot everything needs to be a package\n\n\n2025-01-22\nUse pipe operator\nFamiliar from R, intuitive composition\n\n\n2025-01-22\nType safety via beartype/mypy\nRuntime + static checking",
    "crumbs": [
      "Home",
      "Evolution",
      "Concept Evolution"
    ]
  },
  {
    "objectID": "evolution.html#future-evolution",
    "href": "evolution.html#future-evolution",
    "title": "Concept Evolution",
    "section": "Future Evolution",
    "text": "Future Evolution\nThe concept continues to evolve. Key areas for v3.0: - Dynamic discovery mechanisms - AI/Agent integration patterns - Performance optimizations - Real-world usage patterns",
    "crumbs": [
      "Home",
      "Evolution",
      "Concept Evolution"
    ]
  },
  {
    "objectID": "ai-integration.html",
    "href": "ai-integration.html",
    "title": "AI Integration Patterns",
    "section": "",
    "text": "Each nanobrick can have reasoning capabilities by programmatically interfacing with LLMs or agents.\n\n\n\n\n\n\nProtocol Deep Dive\n\n\n\nFor a comprehensive comparison of AI protocols (MCP, A2A, AG-UI, ACP), see the AI Protocols page.",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "AI Integration Patterns"
    ]
  },
  {
    "objectID": "ai-integration.html#vision-ai-powered-nanobricks",
    "href": "ai-integration.html#vision-ai-powered-nanobricks",
    "title": "AI Integration Patterns",
    "section": "",
    "text": "Each nanobrick can have reasoning capabilities by programmatically interfacing with LLMs or agents.\n\n\n\n\n\n\nProtocol Deep Dive\n\n\n\nFor a comprehensive comparison of AI protocols (MCP, A2A, AG-UI, ACP), see the AI Protocols page.",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "AI Integration Patterns"
    ]
  },
  {
    "objectID": "ai-integration.html#integration-options",
    "href": "ai-integration.html#integration-options",
    "title": "AI Integration Patterns",
    "section": "Integration Options",
    "text": "Integration Options\n\nOption 1: MCP Server Skill (Recommended)\nModel Context Protocol (MCP) provides standardized AI communication between applications and LLMs:\n@skill\nclass SkillMCPServer:\n    \"\"\"Enables MCP server protocol for AI communication\"\"\"\n    \n    def enhance(self, nanobrick: Nanobrick) -&gt; Nanobrick:\n        # Add MCP server capabilities\n        nanobrick.mcp_server = MCPServer(\n            tools=nanobrick.get_tools(),\n            prompts=nanobrick.get_prompts()\n        )\n        return nanobrick\nAdvantages: - Standardized protocol (“USB-C for AI”) - Tool/prompt/resource exposure - Vendor-agnostic LLM support - Growing ecosystem adoption - Python SDK available\n\n\nOption 2: Multi-Protocol Support\nBeyond MCP, nanobricks can support multiple protocols:\n\nA2A (Agent-to-Agent) by Google\nFor nanobrick-to-nanobrick communication:\n@skill\nclass SkillA2A:\n    \"\"\"Enable agent discovery and collaboration\"\"\"\n    \n    def create_agent_card(self, nanobrick: Nanobrick):\n        return {\n            \"id\": f\"nanobrick.{nanobrick.name}\",\n            \"capabilities\": nanobrick.list_capabilities(),\n            \"rpc_endpoint\": f\"/agent/{nanobrick.name}/rpc\"\n        }\n\n\nAG-UI (Agent User Interaction)\nFor interactive interfaces:\n@skill  \nclass SkillAgentUI:\n    \"\"\"Adds visual agent interface\"\"\"\n    \n    def enhance(self, nanobrick: Nanobrick) -&gt; Nanobrick:\n        # Create AG-UI compatible interface\n        nanobrick.agent_ui = AgentUI(\n            name=nanobrick.name,\n            capabilities=nanobrick.capabilities\n        )\n        return nanobrick\nAdvantages: - Visual debugging - Interactive development - Agent orchestration\n\n\nACP (Agent Communication Protocol)\nFor REST-based interoperability:\n@skill\nclass SkillACP:\n    \"\"\"REST-based agent communication\"\"\"\n    \n    def as_rest_agent(self, nanobrick: Nanobrick):\n        # Exposes standard REST endpoints\n        return ACPAgent(nanobrick)\n\n\n\nOption 3: Direct LLM Integration\nSimple, direct LLM calls:\n@skill\nclass SkillLLM:\n    \"\"\"Direct LLM integration\"\"\"\n    \n    model: str = \"gpt-4\"\n    \n    async def reason(self, context: dict) -&gt; str:\n        # Direct API calls to LLMs\n        return await llm_call(self.model, context)\nAdvantages: - Simple implementation - Full control - Provider flexibility\n\n\nOption 4: LangChain/LangGraph Integration\nLeverage existing frameworks:\n@skill\nclass SkillLangChain:\n    \"\"\"LangChain integration for complex reasoning\"\"\"\n    \n    def as_tool(self, nanobrick: Nanobrick) -&gt; Tool:\n        # Expose nanobrick as LangChain tool\n        return Tool(\n            name=nanobrick.name,\n            func=nanobrick.invoke,\n            description=nanobrick.__doc__\n        )",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "AI Integration Patterns"
    ]
  },
  {
    "objectID": "ai-integration.html#architecture-considerations",
    "href": "ai-integration.html#architecture-considerations",
    "title": "AI Integration Patterns",
    "section": "Architecture Considerations",
    "text": "Architecture Considerations\n\n1. Separation of Concerns\n\nCore nanobrick logic remains pure\nAI capabilities as optional enhancement\nClear boundaries between reasoning and execution\n\n\n\n2. State Management\nclass AIStatefulNanobrick(StatefulNanobrick):\n    \"\"\"Nanobrick with AI memory\"\"\"\n    \n    memory: ConversationMemory\n    reasoning_trace: List[ReasoningStep]\n    \n    async def think(self, input: Any) -&gt; ThoughtProcess:\n        # AI reasoning with memory\n        ...\n\n\n3. Multi-Agent Patterns\n# Nanobricks as agents\nvalidator_agent = ValidatorData().with_skill(SkillMCPServer())\ntransformer_agent = DataTransformer().with_skill(SkillMCPServer())\n\n# Agent collaboration\norchestrator = AgentOrchestrator([validator_agent, transformer_agent])",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "AI Integration Patterns"
    ]
  },
  {
    "objectID": "ai-integration.html#protocol-selection-strategy",
    "href": "ai-integration.html#protocol-selection-strategy",
    "title": "AI Integration Patterns",
    "section": "Protocol Selection Strategy",
    "text": "Protocol Selection Strategy\n\nPrimary: MCP for LLM Integration\n\nBest for exposing nanobrick capabilities to LLMs\nClean abstraction for tools and resources\nStrong ecosystem support\n\n\n\nSecondary: A2A for Agent Collaboration\n\nWhen nanobricks need to work with other agents\nPreserves autonomy and opacity\nEnables complex multi-agent workflows\n\n\n\nTertiary: AG-UI for Interactivity\n\nFor human-in-the-loop scenarios\nReal-time state updates\nEvent-driven UI synchronization",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "AI Integration Patterns"
    ]
  },
  {
    "objectID": "ai-integration.html#implementation-roadmap",
    "href": "ai-integration.html#implementation-roadmap",
    "title": "AI Integration Patterns",
    "section": "Implementation Roadmap",
    "text": "Implementation Roadmap\n\nPhase 1: MCP Foundation (Start Here)\n\nImplement SkillMCP with tool exposure\nCreate MCP server for nanobricks\nTest with Claude/other LLMs\n\n\n\nPhase 2: Multi-Protocol Support\n\nAdd A2A for agent collaboration\nImplement AG-UI for interactive UIs\nCreate protocol adapters/bridges\n\n\n\nPhase 3: Advanced Features\n\nCross-protocol orchestration\nProtocol translation layer\nUnified agent registry",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "AI Integration Patterns"
    ]
  },
  {
    "objectID": "ai-integration.html#example-multi-protocol-nanobrick",
    "href": "ai-integration.html#example-multi-protocol-nanobrick",
    "title": "AI Integration Patterns",
    "section": "Example: Multi-Protocol Nanobrick",
    "text": "Example: Multi-Protocol Nanobrick\n@nanobrick\nclass SmartValidator(Nanobrick[dict, dict]):\n    \"\"\"Validator with multiple AI protocol support\"\"\"\n    \n    def __init__(self, protocols: List[str] = [\"mcp\"]):\n        # Add requested protocols\n        if \"mcp\" in protocols:\n            self.add_skill(SkillMCP())\n        if \"a2a\" in protocols:\n            self.add_skill(SkillA2A())\n        if \"agui\" in protocols:\n            self.add_skill(SkillAGUI())\n    \n    async def invoke(self, input: dict) -&gt; dict:\n        # Traditional validation\n        errors = self.validate(input)\n        \n        # AI enhancement via MCP\n        if errors and self.has_skill(SkillMCP):\n            reasoning = await self.ai_reason(input, errors)\n            if reasoning.suggests_fix:\n                input = reasoning.apply_fix(input)\n        \n        # Emit events for UI if AG-UI enabled\n        if self.has_skill(SkillAGUI):\n            self.emit_event(\"validation_complete\", {\n                \"errors\": errors,\n                \"fixed\": reasoning.applied_fixes\n            })\n        \n        return input",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "AI Integration Patterns"
    ]
  },
  {
    "objectID": "ai-integration.html#best-practices",
    "href": "ai-integration.html#best-practices",
    "title": "AI Integration Patterns",
    "section": "Best Practices",
    "text": "Best Practices\n\nProgressive Enhancement\n\nNanobricks work without AI\nAI adds value, not dependency\n\nClear Interfaces\n\nExpose tools/prompts explicitly\nDocument AI capabilities\n\nCost Management\n\nCache AI responses\nBatch reasoning when possible",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "AI Integration Patterns"
    ]
  },
  {
    "objectID": "ai-integration.html#open-questions",
    "href": "ai-integration.html#open-questions",
    "title": "AI Integration Patterns",
    "section": "Open Questions",
    "text": "Open Questions\n\nShould AI be a skill or core capability?\nHow do we standardize prompts across nanobricks?\nWhat’s the best way to handle AI failures?",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "AI Integration Patterns"
    ]
  },
  {
    "objectID": "ai-integration.html#resources",
    "href": "ai-integration.html#resources",
    "title": "AI Integration Patterns",
    "section": "Resources",
    "text": "Resources\n\nMCP Documentation\nA2A Protocol\nAG-UI Documentation\nACP Standard\nAI Protocols Comparison",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "AI Integration Patterns"
    ]
  },
  {
    "objectID": "v2/evolved-concept.html",
    "href": "v2/evolved-concept.html",
    "title": "Nanobricks v2.0",
    "section": "",
    "text": "Nanobricks are composable units of potential - they carry capabilities (skills) that activate only when needed, maintaining simplicity while offering power.\n\n\n\n\n\n\nPackages: For complex, reusable nanobricks (via uv)\nModules: For simpler nanobricks within a project\nCollections: Themed groups of related nanobricks in one package\n\n\n\n\nOptional capabilities that can be activated: - API Skill: FastAPI integration - CLI Skill: Typer commands - UI Skill: Streamlit components - DB Skill: SQLModel persistence - AI Skill: LLM/Agent capabilities (via MCP or ag-ui protocols)\n\n\n\n\n\n# Within Python code\nresult = (\n    ValidatorData() \n    | DataTransformer() \n    | DataPersistor()\n).invoke(data)\n\n\n\n# Adding to existing FastAPI\napp = FastAPI()\napp.mount(\"/health\", HealthMonitor().as_api())\napp.mount(\"/metrics\", MetricsCollector().as_api())\n\n# Adding to existing Streamlit\nst_app = st.container()\nst_app.add(DataVisualizer().as_ui())\n\n\n\n# Nanobricks enhancing each other\nenhanced_validator = (\n    ValidatorData()\n    .with_skill(SkillAI(model=\"gpt-4\"))\n    .with_skill(SkillAPI(endpoint=\"/validate\"))\n)\n\n\n\n\n\n\n\nfrom typing import Protocol, TypeVar, Generic\n\nT_in = TypeVar('T_in')\nT_out = TypeVar('T_out')\n\nclass Nanobrick(Protocol, Generic[T_in, T_out]):\n    \"\"\"Minimal interface - just input/output transformation\"\"\"\n    \n    async def invoke(self, input: T_in) -&gt; T_out: ...\n    \n    def __or__(self, other: 'Nanobrick') -&gt; 'Nanobrick': ...\n\n\n\nclass SkilledNanobrick(Nanobrick[T_in, T_out]):\n    \"\"\"Nanobrick with optional skills\"\"\"\n    \n    def with_skill(self, power: Skill) -&gt; 'SkilledNanobrick':\n        \"\"\"Add a skill to this nanobrick\"\"\"\n        ...\n    \n    def activate_api(self) -&gt; FastAPI:\n        \"\"\"Activate API skill if available\"\"\"\n        ...\n\n\n\nclass StatefulNanobrick(SkilledNanobrick[T_in, T_out]):\n    \"\"\"Nanobrick with memory and learning capabilities\"\"\"\n    \n    async def learn_from(self, experience: Experience) -&gt; None:\n        \"\"\"Antifragile learning mechanism\"\"\"\n        ...\n\n\n\n\n\n\nclass Skill(Protocol):\n    \"\"\"Base interface for all skills\"\"\"\n    \n    def enhance(self, nanobrick: Nanobrick) -&gt; Nanobrick:\n        \"\"\"Enhance a nanobrick with this power\"\"\"\n        ...\n\n\n\n@skill\nclass SkillHealthMonitor:\n    \"\"\"Adds /health endpoint to any nanobrick\"\"\"\n    endpoints = [\"/health\", \"/ready\", \"/live\"]\n    \n@skill  \nclass SkillStreamlitTab:\n    \"\"\"Adds a Streamlit tab to any nanobrick\"\"\"\n    tab_name: str = \"Nanobrick Output\"\n    \n@skill\nclass SkillMCPServer:\n    \"\"\"Enables MCP server protocol for AI communication\"\"\"\n    protocol: str = \"mcp\"\n\n\n\n\n\n\nfrom nanobricks.stdlib import ValidatorData, DataTransformer\nfrom my_project.bricks import CustomProcessor\n\n\n\n@inject\nclass MyWorkflow:\n    validator: ValidatorData\n    transformer: DataTransformer\n\n\n\n# Nanobricks register themselves\n@nanobrick(discoverable=True, tags=[\"validation\", \"data\"])\nclass SmartValidator:\n    pass\n\n# Later, discover by capability\nvalidators = discover_nanobricks(tag=\"validation\")\n\n\n\n\n\n\nmy-nanobrick/\n├── pyproject.toml\n└── src/\n    └── my_nanobrick/\n        ├── __init__.py\n        ├── core.py\n        └── skills/\n            ├── api.py\n            ├── cli.py\n            └── ui.py\n\n\n\nnanobricks-validators/\n├── pyproject.toml\n└── src/\n    └── validators/\n        ├── __init__.py\n        ├── email.py      # ValidatorEmail nanobrick\n        ├── phone.py      # ValidatorPhone nanobrick\n        └── address.py    # ValidatorAddress nanobrick\n\n\n\nmy-project/\n├── pyproject.toml\n├── src/\n│   └── my_app/\n│       └── nanobricks/  # Local nanobricks as modules\n│           ├── __init__.py\n│           ├── custom_validator.py\n│           └── custom_transformer.py\n└── external_bricks/     # Git submodules or pip deps\n\n\n\n\n\nSkill Activation: Should skills be activated explicitly or auto-detect context?\nType Safety: How do we maintain type safety through complex compositions?\nTesting Strategy: How do we test nanobricks in isolation vs. composition?\nPerformance: Should we support async-only or both sync/async?\nVersioning: How do we handle nanobrick evolution and compatibility?",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks v2.0"
    ]
  },
  {
    "objectID": "v2/evolved-concept.html#core-philosophy-refined",
    "href": "v2/evolved-concept.html#core-philosophy-refined",
    "title": "Nanobricks v2.0",
    "section": "",
    "text": "Nanobricks are composable units of potential - they carry capabilities (skills) that activate only when needed, maintaining simplicity while offering power.",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks v2.0"
    ]
  },
  {
    "objectID": "v2/evolved-concept.html#key-insights-from-discussion",
    "href": "v2/evolved-concept.html#key-insights-from-discussion",
    "title": "Nanobricks v2.0",
    "section": "",
    "text": "Packages: For complex, reusable nanobricks (via uv)\nModules: For simpler nanobricks within a project\nCollections: Themed groups of related nanobricks in one package\n\n\n\n\nOptional capabilities that can be activated: - API Skill: FastAPI integration - CLI Skill: Typer commands - UI Skill: Streamlit components - DB Skill: SQLModel persistence - AI Skill: LLM/Agent capabilities (via MCP or ag-ui protocols)\n\n\n\n\n\n# Within Python code\nresult = (\n    ValidatorData() \n    | DataTransformer() \n    | DataPersistor()\n).invoke(data)\n\n\n\n# Adding to existing FastAPI\napp = FastAPI()\napp.mount(\"/health\", HealthMonitor().as_api())\napp.mount(\"/metrics\", MetricsCollector().as_api())\n\n# Adding to existing Streamlit\nst_app = st.container()\nst_app.add(DataVisualizer().as_ui())\n\n\n\n# Nanobricks enhancing each other\nenhanced_validator = (\n    ValidatorData()\n    .with_skill(SkillAI(model=\"gpt-4\"))\n    .with_skill(SkillAPI(endpoint=\"/validate\"))\n)",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks v2.0"
    ]
  },
  {
    "objectID": "v2/evolved-concept.html#architecture-levels",
    "href": "v2/evolved-concept.html#architecture-levels",
    "title": "Nanobricks v2.0",
    "section": "",
    "text": "from typing import Protocol, TypeVar, Generic\n\nT_in = TypeVar('T_in')\nT_out = TypeVar('T_out')\n\nclass Nanobrick(Protocol, Generic[T_in, T_out]):\n    \"\"\"Minimal interface - just input/output transformation\"\"\"\n    \n    async def invoke(self, input: T_in) -&gt; T_out: ...\n    \n    def __or__(self, other: 'Nanobrick') -&gt; 'Nanobrick': ...\n\n\n\nclass SkilledNanobrick(Nanobrick[T_in, T_out]):\n    \"\"\"Nanobrick with optional skills\"\"\"\n    \n    def with_skill(self, power: Skill) -&gt; 'SkilledNanobrick':\n        \"\"\"Add a skill to this nanobrick\"\"\"\n        ...\n    \n    def activate_api(self) -&gt; FastAPI:\n        \"\"\"Activate API skill if available\"\"\"\n        ...\n\n\n\nclass StatefulNanobrick(SkilledNanobrick[T_in, T_out]):\n    \"\"\"Nanobrick with memory and learning capabilities\"\"\"\n    \n    async def learn_from(self, experience: Experience) -&gt; None:\n        \"\"\"Antifragile learning mechanism\"\"\"\n        ...",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks v2.0"
    ]
  },
  {
    "objectID": "v2/evolved-concept.html#skill-system",
    "href": "v2/evolved-concept.html#skill-system",
    "title": "Nanobricks v2.0",
    "section": "",
    "text": "class Skill(Protocol):\n    \"\"\"Base interface for all skills\"\"\"\n    \n    def enhance(self, nanobrick: Nanobrick) -&gt; Nanobrick:\n        \"\"\"Enhance a nanobrick with this power\"\"\"\n        ...\n\n\n\n@skill\nclass SkillHealthMonitor:\n    \"\"\"Adds /health endpoint to any nanobrick\"\"\"\n    endpoints = [\"/health\", \"/ready\", \"/live\"]\n    \n@skill  \nclass SkillStreamlitTab:\n    \"\"\"Adds a Streamlit tab to any nanobrick\"\"\"\n    tab_name: str = \"Nanobrick Output\"\n    \n@skill\nclass SkillMCPServer:\n    \"\"\"Enables MCP server protocol for AI communication\"\"\"\n    protocol: str = \"mcp\"",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks v2.0"
    ]
  },
  {
    "objectID": "v2/evolved-concept.html#discovery-mechanisms",
    "href": "v2/evolved-concept.html#discovery-mechanisms",
    "title": "Nanobricks v2.0",
    "section": "",
    "text": "from nanobricks.stdlib import ValidatorData, DataTransformer\nfrom my_project.bricks import CustomProcessor\n\n\n\n@inject\nclass MyWorkflow:\n    validator: ValidatorData\n    transformer: DataTransformer\n\n\n\n# Nanobricks register themselves\n@nanobrick(discoverable=True, tags=[\"validation\", \"data\"])\nclass SmartValidator:\n    pass\n\n# Later, discover by capability\nvalidators = discover_nanobricks(tag=\"validation\")",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks v2.0"
    ]
  },
  {
    "objectID": "v2/evolved-concept.html#project-structure-options",
    "href": "v2/evolved-concept.html#project-structure-options",
    "title": "Nanobricks v2.0",
    "section": "",
    "text": "my-nanobrick/\n├── pyproject.toml\n└── src/\n    └── my_nanobrick/\n        ├── __init__.py\n        ├── core.py\n        └── skills/\n            ├── api.py\n            ├── cli.py\n            └── ui.py\n\n\n\nnanobricks-validators/\n├── pyproject.toml\n└── src/\n    └── validators/\n        ├── __init__.py\n        ├── email.py      # ValidatorEmail nanobrick\n        ├── phone.py      # ValidatorPhone nanobrick\n        └── address.py    # ValidatorAddress nanobrick\n\n\n\nmy-project/\n├── pyproject.toml\n├── src/\n│   └── my_app/\n│       └── nanobricks/  # Local nanobricks as modules\n│           ├── __init__.py\n│           ├── custom_validator.py\n│           └── custom_transformer.py\n└── external_bricks/     # Git submodules or pip deps",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks v2.0"
    ]
  },
  {
    "objectID": "v2/evolved-concept.html#open-questions-for-next-phase",
    "href": "v2/evolved-concept.html#open-questions-for-next-phase",
    "title": "Nanobricks v2.0",
    "section": "",
    "text": "Skill Activation: Should skills be activated explicitly or auto-detect context?\nType Safety: How do we maintain type safety through complex compositions?\nTesting Strategy: How do we test nanobricks in isolation vs. composition?\nPerformance: Should we support async-only or both sync/async?\nVersioning: How do we handle nanobrick evolution and compatibility?",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks v2.0"
    ]
  },
  {
    "objectID": "architecture.html",
    "href": "architecture.html",
    "title": "Architecture Overview",
    "section": "",
    "text": "We use Protocol for type checking with ABC for runtime enforcement:\nfrom typing import Protocol, Generic, TypeVar, runtime_checkable\nfrom abc import ABC, abstractmethod\n\nT_in = TypeVar('T_in')\nT_out = TypeVar('T_out')\nT_deps = TypeVar('T_deps')\n\n@runtime_checkable\nclass NanobrickProtocol(Protocol, Generic[T_in, T_out, T_deps]):\n    \"\"\"Type checking interface\"\"\"\n    name: str\n    version: str\n    \n    async def invoke(self, input: T_in, *, deps: T_deps = None) -&gt; T_out: ...\n    def invoke_sync(self, input: T_in, *, deps: T_deps = None) -&gt; T_out: ...\n    def __or__(self, other: 'NanobrickProtocol') -&gt; 'NanobrickProtocol': ...\n\nclass NanobrickBase(ABC, Generic[T_in, T_out, T_deps]):\n    \"\"\"Runtime enforcement base class\"\"\"\n    \n    @abstractmethod\n    async def invoke(self, input: T_in, *, deps: T_deps = None) -&gt; T_out:\n        \"\"\"Async invocation - must be implemented\"\"\"\n        pass\n    \n    def invoke_sync(self, input: T_in, *, deps: T_deps = None) -&gt; T_out:\n        \"\"\"Sync wrapper - auto-generated from async\"\"\"\n        import asyncio\n        # Note: In Jupyter/Quarto, use await instead of asyncio.run()\n        # return asyncio.run(self.invoke(input, deps=deps))\n        import asyncio\n        try:\n            loop = asyncio.get_running_loop()\n            # We're in an async context, can't use asyncio.run()\n            raise RuntimeError(\"Use await instead of invoke_sync() in Jupyter/Quarto\")\n        except RuntimeError:\n            # No running loop, safe to use asyncio.run()\n            return asyncio.run(self.invoke(input, deps=deps))\n    \n    def __or__(self, other):\n        \"\"\"Composition operator with type preservation\"\"\"\n        return NanobrickComposite(self, other)\n\n\n\nFollowing the SIMPLE principle, our initial error handling is straightforward:\nclass ErrorHandling:\n    \"\"\"Simple error propagation by default\"\"\"\n    \n    # Default: Fail fast\n    pipeline = A() | B() | C()  # If B fails, pipeline stops\n    \n    # Explicit error handling\n    pipeline = (\n        A()\n        | B().with_fallback(lambda e: default_value)\n        | C()\n    )\n    \n    # Future: Circuit breaker skill\n    pipeline = (\n        A()\n        | B().with_skill(\"circuit_breaker\")\n        | C()\n    )\n\n\n\nState as an optional concern with progressive enhancement:\n# Stateless by default\nbrick = DataTransformer()\n\n# Explicit state via skill\nbrick = DataTransformer().with_skill(\"stateful\", \n    backend=\"redis\",\n    ttl=3600\n)\n\n# State scoping\nbrick = DataTransformer().with_state_scope(\"user:123\")\n\n# Shared state between bricks\nstate_store = StateStore(\"redis://localhost\")\npipeline = (\n    A().with_state(state_store)\n    | B().with_state(state_store)  # Shares same store\n    | C()  # Stateless\n)\n\n\n\nDependencies flow through composition with explicit contracts:\n# Define dependencies via TypedDict\nclass MyDeps(TypedDict):\n    db: Database\n    cache: Cache\n    logger: Logger\n\n# Explicit dependency passing\nresult = brick.invoke(data, deps={\"db\": db, \"cache\": cache})\n\n# Dependency injection through composition\npipeline = (\n    ValidateBrick()\n    | TransformBrick()\n    | PersistBrick()\n).with_deps(MyDeps(db=db, cache=cache, logger=logger))\n\n# Partial dependency override\nresult = pipeline.invoke(data, deps={\"cache\": different_cache})\n\n\n\nPractical approach to maintaining type safety:\n# Type aliases for clarity\nValidatedData = NewType('ValidatedData', dict)\nTransformedData = NewType('TransformedData', dict)\n\n# Explicit typing for deep pipelines\npipeline: Nanobrick[RawData, FinalResult, MyDeps] = (\n    Validator[RawData, ValidatedData, MyDeps]()\n    | Transformer[ValidatedData, TransformedData, MyDeps]()\n    | Persister[TransformedData, FinalResult, MyDeps]()\n)\n\n# Type hints for intermediate results\nstep1 = Validator()  # type: Nanobrick[Raw, Valid, Deps]\nstep2 = step1 | Transformer()  # type: Nanobrick[Raw, Trans, Deps]\nstep3 = step2 | Persister()  # type: Nanobrick[Raw, Final, Deps]",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Architecture Overview"
    ]
  },
  {
    "objectID": "architecture.html#core-architecture-decisions",
    "href": "architecture.html#core-architecture-decisions",
    "title": "Architecture Overview",
    "section": "",
    "text": "We use Protocol for type checking with ABC for runtime enforcement:\nfrom typing import Protocol, Generic, TypeVar, runtime_checkable\nfrom abc import ABC, abstractmethod\n\nT_in = TypeVar('T_in')\nT_out = TypeVar('T_out')\nT_deps = TypeVar('T_deps')\n\n@runtime_checkable\nclass NanobrickProtocol(Protocol, Generic[T_in, T_out, T_deps]):\n    \"\"\"Type checking interface\"\"\"\n    name: str\n    version: str\n    \n    async def invoke(self, input: T_in, *, deps: T_deps = None) -&gt; T_out: ...\n    def invoke_sync(self, input: T_in, *, deps: T_deps = None) -&gt; T_out: ...\n    def __or__(self, other: 'NanobrickProtocol') -&gt; 'NanobrickProtocol': ...\n\nclass NanobrickBase(ABC, Generic[T_in, T_out, T_deps]):\n    \"\"\"Runtime enforcement base class\"\"\"\n    \n    @abstractmethod\n    async def invoke(self, input: T_in, *, deps: T_deps = None) -&gt; T_out:\n        \"\"\"Async invocation - must be implemented\"\"\"\n        pass\n    \n    def invoke_sync(self, input: T_in, *, deps: T_deps = None) -&gt; T_out:\n        \"\"\"Sync wrapper - auto-generated from async\"\"\"\n        import asyncio\n        # Note: In Jupyter/Quarto, use await instead of asyncio.run()\n        # return asyncio.run(self.invoke(input, deps=deps))\n        import asyncio\n        try:\n            loop = asyncio.get_running_loop()\n            # We're in an async context, can't use asyncio.run()\n            raise RuntimeError(\"Use await instead of invoke_sync() in Jupyter/Quarto\")\n        except RuntimeError:\n            # No running loop, safe to use asyncio.run()\n            return asyncio.run(self.invoke(input, deps=deps))\n    \n    def __or__(self, other):\n        \"\"\"Composition operator with type preservation\"\"\"\n        return NanobrickComposite(self, other)\n\n\n\nFollowing the SIMPLE principle, our initial error handling is straightforward:\nclass ErrorHandling:\n    \"\"\"Simple error propagation by default\"\"\"\n    \n    # Default: Fail fast\n    pipeline = A() | B() | C()  # If B fails, pipeline stops\n    \n    # Explicit error handling\n    pipeline = (\n        A()\n        | B().with_fallback(lambda e: default_value)\n        | C()\n    )\n    \n    # Future: Circuit breaker skill\n    pipeline = (\n        A()\n        | B().with_skill(\"circuit_breaker\")\n        | C()\n    )\n\n\n\nState as an optional concern with progressive enhancement:\n# Stateless by default\nbrick = DataTransformer()\n\n# Explicit state via skill\nbrick = DataTransformer().with_skill(\"stateful\", \n    backend=\"redis\",\n    ttl=3600\n)\n\n# State scoping\nbrick = DataTransformer().with_state_scope(\"user:123\")\n\n# Shared state between bricks\nstate_store = StateStore(\"redis://localhost\")\npipeline = (\n    A().with_state(state_store)\n    | B().with_state(state_store)  # Shares same store\n    | C()  # Stateless\n)\n\n\n\nDependencies flow through composition with explicit contracts:\n# Define dependencies via TypedDict\nclass MyDeps(TypedDict):\n    db: Database\n    cache: Cache\n    logger: Logger\n\n# Explicit dependency passing\nresult = brick.invoke(data, deps={\"db\": db, \"cache\": cache})\n\n# Dependency injection through composition\npipeline = (\n    ValidateBrick()\n    | TransformBrick()\n    | PersistBrick()\n).with_deps(MyDeps(db=db, cache=cache, logger=logger))\n\n# Partial dependency override\nresult = pipeline.invoke(data, deps={\"cache\": different_cache})\n\n\n\nPractical approach to maintaining type safety:\n# Type aliases for clarity\nValidatedData = NewType('ValidatedData', dict)\nTransformedData = NewType('TransformedData', dict)\n\n# Explicit typing for deep pipelines\npipeline: Nanobrick[RawData, FinalResult, MyDeps] = (\n    Validator[RawData, ValidatedData, MyDeps]()\n    | Transformer[ValidatedData, TransformedData, MyDeps]()\n    | Persister[TransformedData, FinalResult, MyDeps]()\n)\n\n# Type hints for intermediate results\nstep1 = Validator()  # type: Nanobrick[Raw, Valid, Deps]\nstep2 = step1 | Transformer()  # type: Nanobrick[Raw, Trans, Deps]\nstep3 = step2 | Persister()  # type: Nanobrick[Raw, Final, Deps]",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Architecture Overview"
    ]
  },
  {
    "objectID": "architecture.html#enhanced-architectural-features",
    "href": "architecture.html#enhanced-architectural-features",
    "title": "Architecture Overview",
    "section": "Enhanced Architectural Features",
    "text": "Enhanced Architectural Features\n\nContracts & Invariants\nBuilt-in support for design-by-contract:\n@nanobrick(\n    name=\"ValidatorData\",\n    preconditions=[\n        lambda input, deps: isinstance(input, dict),\n        lambda input, deps: \"schema\" in deps\n    ],\n    postconditions=[\n        lambda result: result is not None,\n        lambda result: \"validated\" in result\n    ],\n    invariants=[\n        lambda self: self.error_count &gt;= 0\n    ]\n)\nclass ValidatorData(NanobrickBase):\n    async def invoke(self, input, *, deps=None):\n        # Contracts automatically checked\n        pass\n\n\nResource Management\nContext manager support for proper cleanup:\n# Automatic resource management\nasync with pipeline as p:\n    result = await p.invoke(data)\n    # Resources cleaned up automatically\n\n# Explicit resource limits\n@nanobrick(\n    resources={\n        \"max_memory_mb\": 100,\n        \"timeout_seconds\": 30,\n        \"max_concurrent\": 5\n    }\n)\nclass ResourceBoundedBrick(NanobrickBase):\n    pass\n\n\nHot-Swapping Support\nDynamic brick replacement without stopping pipelines:\n# Create swappable pipeline\npipeline = SwappablePipeline([\n    ValidatorV1(),\n    Transformer(),\n    Persister()\n])\n\n# Hot-swap a component\npipeline.swap(0, ValidatorV2())  # Zero downtime\n\n# Gradual rollout\npipeline.swap(0, ValidatorV2(), rollout_percent=10)\n\n\nBranching & Merging\nBeyond linear pipelines:\n# Conditional branching\npipeline = (\n    Validator()\n    | Branch(\n        condition=lambda x: x.get(\"type\") == \"A\",\n        true_path=ProcessorA(),\n        false_path=ProcessorB()\n    )\n    | Merger()\n)\n\n# Parallel execution\npipeline = (\n    Splitter()\n    | Parallel([\n        ProcessorA(),\n        ProcessorB(),\n        ProcessorC()\n    ])\n    | Aggregator()\n)\n\n# Fan-out/fan-in\npipeline = (\n    Source()\n    | FanOut([ProcessorA(), ProcessorB()])  # Copies to both\n    | FanIn(strategy=\"merge\")  # Combines results\n)",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Architecture Overview"
    ]
  },
  {
    "objectID": "architecture.html#layered-architecture",
    "href": "architecture.html#layered-architecture",
    "title": "Architecture Overview",
    "section": "Layered Architecture",
    "text": "Layered Architecture\n\nLayer 2: Skill System\nOptional capabilities that enhance nanobricks:\nclass Skill(Protocol):\n    \"\"\"Base interface for all skills\"\"\"\n    \n    def enhance(self, nanobrick: Nanobrick) -&gt; Nanobrick: ...\n\n\nLayer 3: Composition Engine\nHandles different composition patterns:\n\nSequential: A | B | C\nParallel: A + B + C\nNested: A(B(C))\nHybrid: Complex workflows\n\n\n\nLayer 4: Runtime Environment\nExecution context and dependency injection:\nclass NanobrickRuntime:\n    \"\"\"Manages nanobrick execution\"\"\"\n    \n    registry: NanobrickRegistry\n    injector: DependencyInjector\n    monitor: PerformanceMonitor",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Architecture Overview"
    ]
  },
  {
    "objectID": "architecture.html#package-structure",
    "href": "architecture.html#package-structure",
    "title": "Architecture Overview",
    "section": "Package Structure",
    "text": "Package Structure\n\nCore Package (nanobricks-core)\nnanobricks-core/\n├── src/\n│   └── nanobricks/\n│       ├── __init__.py\n│       ├── protocol.py      # Core interfaces\n│       ├── skill.py    # Skill system\n│       ├── composition.py   # Composition operators\n│       └── runtime.py       # Execution environment\n\n\nStandard Library (nanobricks-stdlib)\nnanobricks-stdlib/\n├── src/\n│   └── nanobricks_stdlib/\n│       ├── validators/      # Common validators\n│       ├── transformers/    # Data transformers\n│       ├── connectors/      # External connections\n│       └── utils/          # Utility bricks\n\n\nSkills (nanobricks-powers)\nnanobricks-powers/\n├── src/\n│   └── nanobricks_powers/\n│       ├── api.py          # FastAPI power\n│       ├── cli.py          # Typer power\n│       ├── ui.py           # Streamlit power\n│       ├── db.py           # SQLModel power\n│       └── ai.py           # AI/LLM power",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Architecture Overview"
    ]
  },
  {
    "objectID": "architecture.html#design-principles",
    "href": "architecture.html#design-principles",
    "title": "Architecture Overview",
    "section": "Design Principles",
    "text": "Design Principles\n\n1. Simplicity First\n\nMinimal required interface\nComplexity through composition\nClear mental model\n\n\n\n2. Explicit Over Implicit\n\nNo hidden magic\nClear data flow\nPredictable behavior\n\n\n\n3. Composition Over Inheritance\n\nProtocol-based design\nMix-and-match capabilities\nNo deep hierarchies\n\n\n\n4. Fail Fast, Recover Gracefully\n\nType checking at boundaries\nClear error messages\nAntifragile patterns",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Architecture Overview"
    ]
  },
  {
    "objectID": "architecture.html#key-components",
    "href": "architecture.html#key-components",
    "title": "Architecture Overview",
    "section": "Key Components",
    "text": "Key Components\n\nNanobrick Protocol\n\nDefines minimal interface\nEnsures composability\nType-safe generics\n\n\n\nSkill System\n\nOptional enhancements\nLazy activation\nClean separation\n\n\n\nComposition Engine\n\nMultiple patterns\nType inference\nPerformance optimization\n\n\n\nRegistry & Discovery\n\nStatic imports (default)\nDynamic discovery (optional)\nDependency injection",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Architecture Overview"
    ]
  },
  {
    "objectID": "architecture.html#comparison-with-other-frameworks",
    "href": "architecture.html#comparison-with-other-frameworks",
    "title": "Architecture Overview",
    "section": "Comparison with Other Frameworks",
    "text": "Comparison with Other Frameworks\n\n\n\nAspect\nNanobricks\nLangChain\nFastAPI\n\n\n\n\nComposition\nNative pipe operator\nLCEL\nDependency injection\n\n\nType Safety\nBeartype + Mypy\nRuntime checks\nPydantic\n\n\nModularity\nProtocol-based\nClass hierarchies\nFunction-based\n\n\nBatteries\nSkills (optional)\nBuilt-in\nExtensions",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Architecture Overview"
    ]
  },
  {
    "objectID": "architecture.html#performance-considerations",
    "href": "architecture.html#performance-considerations",
    "title": "Architecture Overview",
    "section": "Performance Considerations",
    "text": "Performance Considerations\n\nLazy Loading: Skills load only when activated\nAsync First: Native async/await support\nZero-Cost Abstractions: Protocols have no runtime overhead\nEfficient Composition: Optimized pipe operations",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Architecture Overview"
    ]
  },
  {
    "objectID": "architecture.html#security-model",
    "href": "architecture.html#security-model",
    "title": "Architecture Overview",
    "section": "Security Model",
    "text": "Security Model\n\nIsolated Execution: Each nanobrick runs in isolation\nCapability-Based: Skills grant specific capabilities\nType Safety: Prevents many runtime errors\nAudit Trail: Optional execution logging",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Architecture Overview"
    ]
  },
  {
    "objectID": "human.html",
    "href": "human.html",
    "title": "Nanobricks for Humans",
    "section": "",
    "text": "import sys\nimport os\n\n# Adjust the path to your workspace root if needed\nsys.path.append(os.path.abspath(\"/Users/jankothyson/Code/kaosmaps/nano/nano-1/src\"))\n\nimport asyncio\nimport nest_asyncio\n\nnest_asyncio.apply()",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Nanobricks for Humans"
    ]
  },
  {
    "objectID": "human.html#welcome-human",
    "href": "human.html#welcome-human",
    "title": "Nanobricks for Humans",
    "section": "Welcome, Human! 👋",
    "text": "Welcome, Human! 👋\nThis document is your personal guide through the Nanobricks implementation. As we build each piece, I’ll update this document with working code examples that you can run and experiment with.",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Nanobricks for Humans"
    ]
  },
  {
    "objectID": "human.html#current-status-performance-optimizations-complete",
    "href": "human.html#current-status-performance-optimizations-complete",
    "title": "Nanobricks for Humans",
    "section": "Current Status: Performance Optimizations Complete! ⚡",
    "text": "Current Status: Performance Optimizations Complete! ⚡\nYour nanobricks now fly with intelligent caching, efficient batching, pipeline fusion, and memory pooling! Make expensive operations instant with LRU caching, process items in batches, and reduce overhead with fused pipelines.",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Nanobricks for Humans"
    ]
  },
  {
    "objectID": "human.html#what-weve-built-so-far",
    "href": "human.html#what-weve-built-so-far",
    "title": "Nanobricks for Humans",
    "section": "What We’ve Built So Far",
    "text": "What We’ve Built So Far\n\n1. The Vision\nWe’re creating “Lego bricks for code” - simple, composable components that can build anything:\n# This is what we're building towards:\npipeline = (\n    DataValidator() \n    | DataTransformer() \n    | DataPersistor()\n).invoke(data)\n\n# With skills activated:\nsmart_pipeline = (\n    DataValidator()\n    .with_skill(\"logging\")\n    .with_skill(\"observability\")\n    | DataTransformer()\n    | DataPersistor()\n    .with_skill(\"api\")  # Now it's a REST API!\n)\n\n\n2. The Ten Commandments\nOur guiding principles that shape everything:\n\nBe Simple - One component, one purpose\nBe Standardized - Same interface everywhere\nBe Composable - Mix and match freely\nBe Self-Sufficient - Bring your own batteries\nBe Scaffoldable - Work immediately\nBe Observable - See what’s happening\nBe Resilient - Handle failures gracefully\nBe Configurable - Adapt without code changes\nBe Evolutionary - Get better over time\nBe Secure - Safe by default\n\n\n\n3. The Architecture\nWe decided on a hybrid approach combining the best of both worlds:\n\n# For type checking (IDE support, mypy)\nfrom typing import Protocol, Generic, TypeVar, runtime_checkable\n\nT_in = TypeVar(\"T_in\")\nT_out = TypeVar(\"T_out\")\nT_deps = TypeVar(\"T_deps\")\n\n\n@runtime_checkable\nclass NanobrickProtocol(Protocol, Generic[T_in, T_out, T_deps]):\n    \"\"\"What every nanobrick looks like to the type system\"\"\"\n\n    name: str\n    version: str\n\n    async def invoke(self, input: T_in, *, deps: T_deps = None) -&gt; T_out: ...\n    def invoke_sync(self, input: T_in, *, deps: T_deps = None) -&gt; T_out: ...\n\n\n# For runtime enforcement (actual base class)\nfrom abc import ABC, abstractmethod\n\n\nclass NanobrickBase(ABC, Generic[T_in, T_out, T_deps]):\n    \"\"\"What you actually inherit from\"\"\"\n\n    @abstractmethod\n    async def invoke(self, input: T_in, *, deps: T_deps = None) -&gt; T_out:\n        \"\"\"You must implement this\"\"\"\n        pass\n\n    def invoke_sync(self, input: T_in, *, deps: T_deps = None) -&gt; T_out:\n        \"\"\"We provide this for free!\"\"\"\n        import asyncio\n\n        return asyncio.run(self.invoke(input, deps=deps))",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Nanobricks for Humans"
    ]
  },
  {
    "objectID": "human.html#coming-next-the-implementation-journey",
    "href": "human.html#coming-next-the-implementation-journey",
    "title": "Nanobricks for Humans",
    "section": "Coming Next: The Implementation Journey",
    "text": "Coming Next: The Implementation Journey\n\nStep 1: Hello, Nanobrick! ✅\nStatus: Complete\nOur first working nanobrick is ready! Here’s how to create one:\n\n# import sys\n\n# sys.path.append(\"/Users/jankothyson/Code/kaosmaps/nano/nano-1/src\")\n\nimport asyncio\nfrom nanobricks import NanobrickBase, NanobrickSimple\n\n\n# Method 1: Full control with NanobrickBase\nclass EchoBrick(NanobrickBase[str, str, None]):\n    \"\"\"I repeat what you say!\"\"\"\n\n    async def invoke(self, input: str, *, deps=None) -&gt; str:\n        return input\n\n\n# Method 2: Simpler with NanobrickSimple (no deps needed)\nclass UpperBrick(NanobrickSimple[str, str]):\n    \"\"\"I SHOUT WHAT YOU SAY!\"\"\"\n\n    async def invoke(self, input: str, *, deps=None) -&gt; str:\n        return input.upper()\n\n\n# Use them!\necho = EchoBrick()\nupper = UpperBrick()\n\n# Async usage (native Jupyter style)\nresult = await echo.invoke(\"Hello, World!\")\nprint(f\"Echo async (await): {result}\")\n\nresult = await upper.invoke(\"Hello, World!\")\nprint(f\"Upper async (await): {result}\")\n\n# In Jupyter/Quarto, we use await directly since we're in an async environment\nresult = await echo.invoke(\"Hello, World!\")\nprint(f\"Echo second call (await): {result}\")\n\nresult = await upper.invoke(\"hello\")\nprint(f\"Upper second call (await): {result}\")\n\n# Note: invoke_sync() and asyncio.run() don't work in Jupyter due to the running event loop\n# In non-Jupyter environments, you would use:\n# result = echo.invoke_sync(\"Hello, World!\")\n# result = asyncio.run(echo.invoke(\"Hello, World!\"))\n\nEcho async (await): Hello, World!\nUpper async (await): HELLO, WORLD!\nEcho second call (await): Hello, World!\nUpper second call (await): HELLO\n\n\n\n\nA Note on Async/Sync in Jupyter\nJupyter notebooks run in an async environment, which affects how we call nanobricks:\n\nIn Jupyter/Quarto: Use await directly (the running event loop prevents asyncio.run())\nIn regular Python scripts: Use invoke_sync() or asyncio.run()\n\nThe invoke_sync() method and asyncio.run() are designed for non-async contexts and will raise errors in Jupyter due to the existing event loop. This is why we use await directly in this notebook.\n\n\nWhat Makes a Nanobrick?\nEvery nanobrick has: - A name: Defaults to the class name - A version: Defaults to “0.1.0” - An async invoke method: The core processing logic - Type parameters: Input type, output type, and optional dependencies\n\n# Check the nanobrick properties\nprint(f\"Name: {echo.name}\")\nprint(f\"Version: {echo.version}\")\nprint(f\"String representation: {str(echo)}\")\n\nName: EchoBrick\nVersion: 0.1.0\nString representation: EchoBrick v0.1.0\n\n\n\n\nStep 2: Composition Magic ✅\nStatus: Complete\nThe pipe operator is working! Here’s how to compose nanobricks:\n\nfrom nanobricks import NanobrickSimple\n\n\n# Create some bricks\nclass GreetBrick(NanobrickSimple[str, str]):\n    async def invoke(self, input: str, *, deps=None) -&gt; str:\n        return f\"Hello, {input}!\"\n\n\nclass ShoutBrick(NanobrickSimple[str, str]):\n    async def invoke(self, input: str, *, deps=None) -&gt; str:\n        return input.upper()\n\n\n# Compose them!\ngreet = GreetBrick()\nshout = ShoutBrick()\npipeline = greet | shout\n\n# Use the pipeline\n# Method 1: Native Jupyter async style\nresult = await pipeline.invoke(\"world\")\nprint(f\"Pipeline (await): {result}\")\n\n# Method 2: Another await call (since we're in async environment)\nresult = await pipeline.invoke(\"python\")\nprint(f\"Pipeline (second call): {result}\")\n\nPipeline (await): HELLO, WORLD!\nPipeline (second call): HELLO, PYTHON!\n\n\n\n\nAdvanced Composition\nYou can chain multiple bricks and even compose different types:\n\n# Chain multiple bricks\nexcited = greet | shout | shout  # Double shout!\nresult = await excited.invoke(\"alice\")\nprint(f\"Double shout: {result}\")\n\n\n# Type transformations\nclass LengthBrick(NanobrickSimple[str, int]):\n    async def invoke(self, input: str, *, deps=None) -&gt; int:\n        return len(input)\n\n\n# This creates a pipeline: str -&gt; str -&gt; int\ncount_greeting = greet | LengthBrick()\nresult = await count_greeting.invoke(\"bob\")\nprint(f\"Length of greeting: {result} (for 'Hello, bob!')\")\n\n\n# Error propagation (fail-fast)\nclass ValidateBrick(NanobrickSimple[str, str]):\n    async def invoke(self, input: str, *, deps=None) -&gt; str:\n        if not input.isalpha():\n            raise ValueError(\"Only letters allowed\")\n        return input\n\n\nsafe_greet = ValidateBrick() | greet | shout\ntry:\n    result = await safe_greet.invoke(\"123\")\nexcept ValueError as e:\n    print(f\"Caught expected error: {e}\")\n\nDouble shout: HELLO, ALICE!\nLength of greeting: 11 (for 'Hello, bob!')\nCaught expected error: Only letters allowed\n\n\n\n\nPipeline Class\nFor longer compositions, use the Pipeline class:\n\nfrom nanobricks import Pipeline\n\n\n# First define the missing brick\nclass AddExclamationBrick(NanobrickSimple[str, str]):\n    async def invoke(self, input: str, *, deps=None) -&gt; str:\n        return f\"{input}!!!!\"\n\n\n# Create a pipeline from multiple bricks\npipeline = Pipeline(ValidateBrick(), GreetBrick(), ShoutBrick(), AddExclamationBrick())\n\nresult = await pipeline.invoke(\"world\")\nprint(f\"Pipeline result: {result}\")\n\nPipeline result: HELLO, WORLD!!!!!\n\n\n\n\nStep 3: Dependency Injection ✅\nStatus: Complete\nYour nanobricks can now share resources! Here’s how dependency injection works:\n\nfrom nanobricks import NanobrickBase, StandardDeps, DependencyContainer\nfrom nanobricks.dependencies import MockDatabase, MockLogger\n\n\n# Define a brick that uses dependencies\nclass UserLoaderBrick(NanobrickBase[str, dict, StandardDeps]):\n    async def invoke(self, user_id: str, *, deps=None) -&gt; dict:\n        if not deps or \"db\" not in deps:\n            raise ValueError(\"Database dependency required\")\n\n        # Use the database from deps\n        result = await deps[\"db\"].query(f\"SELECT * FROM users WHERE id = {user_id}\")\n\n        # Optional: use logger if available\n        if \"logger\" in deps:\n            deps[\"logger\"].info(f\"Loaded user {user_id}\")\n\n        return result[0]\n\n\n# Create dependencies\ndeps = DependencyContainer(\n    db=MockDatabase(\n        {\"SELECT * FROM users WHERE id = 123\": [{\"id\": 123, \"name\": \"Alice\"}]}\n    ),\n    logger=MockLogger(),\n    config={\"tenant\": \"AcmeCorp\"},\n)\n\n# Use the brick with dependencies\nloader = UserLoaderBrick()\nuser = await loader.invoke(\"123\", deps=deps.to_dict())\nprint(f\"Loaded user: {user}\")\n\nLoaded user: {'id': 123, 'name': 'Alice'}\n\n\n\n\nDependency Types\nWe provide some common dependency types:\n\nfrom nanobricks import StandardDeps\nfrom typing import TypedDict\n\n# Show what StandardDeps looks like\nprint(\"StandardDeps is a TypedDict with the following fields:\")\nprint(\"- db: Database connection\")\nprint(\"- cache: Cache instance\")\nprint(\"- logger: Logger instance\")\nprint(\"- config: Configuration dict\")\n\n\n# Example of creating your own dependency type\nclass MyDeps(TypedDict):\n    api_client: object  # Would be your APIClient type\n    feature_flags: dict\n    user_context: dict  # Would be your UserContext type\n\n\nprint(\"\\nCustom dependency type created successfully!\")\n\nStandardDeps is a TypedDict with the following fields:\n- db: Database connection\n- cache: Cache instance\n- logger: Logger instance\n- config: Configuration dict\n\nCustom dependency type created successfully!\n\n\n\n\nDependencies in Pipelines\nDependencies flow through entire pipelines automatically:\n\n# Define example bricks for the pipeline\nclass EnrichBrick(NanobrickBase[dict, dict, StandardDeps]):\n    async def invoke(self, data: dict, *, deps=None) -&gt; dict:\n        enriched = data.copy()\n        enriched[\"enriched\"] = True\n        if deps and \"logger\" in deps:\n            deps[\"logger\"].info(\"Data enriched\")\n        return enriched\n\n\nclass SaveBrick(NanobrickBase[dict, dict, StandardDeps]):\n    async def invoke(self, data: dict, *, deps=None) -&gt; dict:\n        if deps and \"db\" in deps:\n            # Simulate saving to database\n            deps[\"db\"].queries.append(f\"INSERT INTO data VALUES ({data})\")\n        return data\n\n\n# Create a UserValidateBrick that works with dict input\nclass UserValidateBrick(NanobrickBase[dict, dict, StandardDeps]):\n    async def invoke(self, data: dict, *, deps=None) -&gt; dict:\n        if \"name\" not in data:\n            raise ValueError(\"Name is required\")\n        return data\n\n\n# All bricks in the pipeline receive the same deps\npipeline = UserValidateBrick() | EnrichBrick() | SaveBrick()\ndata = {\"id\": 123, \"name\": \"Alice\"}\nresult = await pipeline.invoke(data, deps=deps.to_dict())\nprint(f\"Pipeline result: {result}\")\n\nPipeline result: {'id': 123, 'name': 'Alice', 'enriched': True}\n\n\n\n\nStep 4: Configuration Power\nStatus: Complete! ✅\nNanobricks now supports powerful TOML-based configuration with environment overrides!\n\nLoading Configuration\n\nfrom nanobricks.config import load_config, get_default_config\n\n# Load default config (searches for nanobrick.toml, pyproject.toml, etc.)\nconfig = load_config()\nprint(f\"Default config loaded: {type(config)}\")\n\n# Load with specific environment\ntry:\n    prod_config = load_config(environment=\"production\")\n    print(\"Production config loaded successfully\")\nexcept:\n    print(\"No production environment config found (expected)\")\n\n# Load with overrides\ncustom_config = load_config(overrides={\"logging\": {\"level\": \"DEBUG\"}})\nprint(\n    f\"Custom override - log level: {custom_config.get('logging', {}).get('level', 'INFO')}\"\n)\n\n# Get cached default config\nconfig = get_default_config()\nprint(f\"Cached config type: {type(config)}\")\n\nDefault config loaded: &lt;class 'nanobricks.config.Config'&gt;\nProduction config loaded successfully\nCustom override - log level: DEBUG\nCached config type: &lt;class 'nanobricks.config.Config'&gt;\n\n\n\n\nConfiguration Files\nCreate a nanobrick.toml file:\n[project]\nname = \"my-project\"\nversion = \"1.0.0\"\n\n[logging]\nlevel = \"INFO\"\nformat = \"simple\"\n\n[database]\nurl = \"sqlite:///app.db\"\npool_size = 5\n\n# Environment-specific overrides\n[environments.production]\n[environments.production.database]\nurl = \"postgresql://prod-server/app\"\npool_size = 20\n\n[environments.test]\n[environments.test.database]\nurl = \"sqlite:///:memory:\"\n\n\nUsing Config in Bricks\n\nfrom nanobricks import NanobrickBase\nfrom nanobricks.config import get_default_config, Config\n\n\nclass ConfigurableBrick(NanobrickBase[str, str, None]):\n    def __init__(self):\n        self.name = \"configurable\"\n        self.version = \"1.0.0\"\n        self.config = get_default_config()\n\n    async def invoke(self, input: str, *, deps=None) -&gt; str:\n        # Access config safely with get() method\n        log_level = self.config.get(\"logging\", {}).get(\"level\", \"INFO\")\n\n        # Or dictionary style\n        features = self.config.get(\"features\", {})\n\n        return f\"Processed '{input}' with {log_level} logging\"\n\n\n# Test the configurable brick\nconfigurable = ConfigurableBrick()\nresult = await configurable.invoke(\"test data\")\nprint(f\"Result: {result}\")\n\nResult: Processed 'test data' with DEBUG logging\n\n\n\n\nConfig Object Features\n\nDot notation access: config.database.host\nDictionary access: config[\"database\"][\"host\"]\nSafe get with defaults: config.get(\"missing\", \"default\")\nImmutable with freeze: config.freeze()\nEnvironment support: Automatic env-specific overrides\nDiscovery chain: Searches up directory tree for config files\n\n\n\nReal Example from examples/configuration.py\n\n# Demonstrate Config API features\nfrom nanobricks.config import Config\n\n# Create a config object\nconfig = Config(\n    {\n        \"app\": {\n            \"name\": \"demo\",\n            \"version\": \"1.0.0\",\n            \"settings\": {\"debug\": True, \"timeout\": 30},\n        },\n        \"features\": [\"auth\", \"api\", \"cache\"],\n    }\n)\n\n# Dot notation access\nprint(f\"App name: {config.app.name}\")\nprint(f\"Debug mode: {config.app.settings.debug}\")\n\n# Dictionary access\nprint(f\"Features: {config['features']}\")\n\n# Safe get with default\nprint(f\"Missing key: {config.get('missing', 'default value')}\")\n\n# Convert back to dict\ndata = config.to_dict()\nprint(f\"As dict type: {type(data)}\")\n\nApp name: demo\nDebug mode: True\nFeatures: ['auth', 'api', 'cache']\nMissing key: default value\nAs dict type: &lt;class 'dict'&gt;\n\n\n\n\nWorking with Environment-Specific Configs\n\n# Example of how environment configs work\nfrom nanobricks.config import Config\n\n# Simulate a config with environments\nfull_config = Config(\n    {\n        \"database\": {\"url\": \"sqlite:///dev.db\", \"pool_size\": 5},\n        \"environments\": {\n            \"production\": {\n                \"database\": {\"url\": \"postgresql://prod-server/app\", \"pool_size\": 20}\n            },\n            \"test\": {\"database\": {\"url\": \"sqlite:///:memory:\"}},\n        },\n    }\n)\n\n# In real usage, load_config(environment=\"production\") would apply these overrides\nprint(f\"Dev database: {full_config.database.url}\")\nprint(f\"Prod database: {full_config.environments.production.database.url}\")\nprint(f\"Test database: {full_config.environments.test.database.url}\")\n\nDev database: sqlite:///dev.db\nProd database: postgresql://prod-server/app\nTest database: sqlite:///:memory:\n\n\n\n\n\nStep 5: Skills Activate!\nStatus: Complete! ✅\nThe skill framework is now working! Add capabilities on demand without changing your brick’s core logic.\n\nWhat Are Skills?\nSkills are optional capabilities that nanobricks can activate when needed:\n\nTiming: Measure execution time ✅\nRetry: Automatic retry on failure ✅\nCaching: Cache results for repeated calls ✅\nLogging: Automatic logging of inputs/outputs ✅\nAPI: Expose as REST endpoints ✅\nCLI: Command-line interface ✅\nObservability: Metrics and tracing (coming soon)\n\n\n\nCreating a Skill\n\nfrom nanobricks import Skill, NanobrickEnhanced, register_skill\n\n\n@register_skill(\"timing\")\nclass TimingSkill(Skill):\n    \"\"\"Adds timing information to brick invocations.\"\"\"\n\n    def _create_enhanced_brick(self, brick):\n        import time\n\n        class TimingEnhanced(NanobrickEnhanced):\n            async def invoke(self, input, *, deps=None):\n                start = time.time()\n                result = await self._wrapped.invoke(input, deps=deps)\n                elapsed = time.time() - start\n                print(f\"⏱️  {self._wrapped.name} took {elapsed:.3f}s\")\n                return result\n\n        return TimingEnhanced(brick, self)\n\n\nprint(\"TimingSkill registered!\")\n\nTimingSkill registered!\n\n\n\n\nUsing Skills\nThere are three ways to add skills to your bricks:\n\n1. Method Chaining\n\n# Create a simple brick\nclass SlowBrick(NanobrickSimple[str, str]):\n    async def invoke(self, input: str, *, deps=None) -&gt; str:\n        await asyncio.sleep(0.1)  # Simulate slow work\n        return f\"Processed: {input}\"\n\n\n# Add timing skill\nbrick = SlowBrick()\ntimed_brick = brick.with_skill(\"timing\")\n\n# Use it\nresult = await timed_brick.invoke(\"hello\")\nprint(f\"Result: {result}\")\n\n⏱️  SlowBrick took 0.101s\nResult: Processed: hello\n\n\n\n\n2. Function Style\n\nfrom nanobricks import with_skill\n\n\n# Add multiple skills\n@register_skill(\"cache\")\nclass CacheSkill(Skill):\n    def _create_enhanced_brick(self, brick):\n        cache = {}\n\n        class CacheEnhanced(NanobrickEnhanced):\n            async def invoke(self, input, *, deps=None):\n                key = str(input)\n                if key in cache:\n                    print(f\"💾 Cache hit for: {key}\")\n                    return cache[key]\n\n                result = await self._wrapped.invoke(input, deps=deps)\n                cache[key] = result\n                print(f\"💾 Cached: {key}\")\n                return result\n\n        return CacheEnhanced(brick, self)\n\n\n# Chain skills\nenhanced = with_skill(with_skill(brick, \"cache\"), \"timing\")\n\n# First call - slow\nresult1 = await enhanced.invoke(\"test\")\n# Second call - fast (cached)\nresult2 = await enhanced.invoke(\"test\")\n\nprint(f\"Same results: {result1 == result2}\")\n\n💾 Cached: test\n⏱️  SlowBrick+cache took 0.101s\n💾 Cache hit for: test\n⏱️  SlowBrick+cache took 0.000s\nSame results: True\n\n\n\n\n3. Decorator Style\n\nfrom nanobricks import skill\n\n\n@skill(\"timing\")\n@skill(\"cache\")\nclass SmartBrick(NanobrickSimple[str, int]):\n    \"\"\"A brick decorated with skills.\"\"\"\n\n    async def invoke(self, input: str, *, deps=None) -&gt; int:\n        await asyncio.sleep(0.05)  # Simulate work\n        return len(input)\n\n\n# The brick is automatically enhanced\nsmart = SmartBrick()\n\n# Test it\nfor text in [\"hello\", \"world\", \"hello\"]:  # \"hello\" appears twice\n    length = await smart.invoke(text)\n    print(f\"Length of '{text}': {length}\")\n\n💾 Cached: hello\n⏱️  SmartBrick+cache took 0.051s\nLength of 'hello': 5\n💾 Cached: world\n⏱️  SmartBrick+cache took 0.051s\nLength of 'world': 5\n💾 Cache hit for: hello\n⏱️  SmartBrick+cache took 0.000s\nLength of 'hello': 5\n\n\n\n\n\nReal Example: Retry Skill\n\n@register_skill(\"retry\")\nclass RetrySkill(Skill):\n    \"\"\"Adds retry logic to brick invocations.\"\"\"\n\n    def _create_enhanced_brick(self, brick):\n        max_retries = self.config.get(\"max_retries\", 3)\n\n        class RetryEnhanced(NanobrickEnhanced):\n            async def invoke(self, input, *, deps=None):\n                last_error = None\n                for attempt in range(max_retries):\n                    try:\n                        return await self._wrapped.invoke(input, deps=deps)\n                    except Exception as e:\n                        last_error = e\n                        if attempt &lt; max_retries - 1:\n                            print(f\"🔄 Retry {attempt + 1}/{max_retries} after: {e}\")\n\n                raise last_error\n\n        return RetryEnhanced(brick, self)\n\n\n# Test with an unreliable brick\nclass UnreliableBrick(NanobrickSimple[int, int]):\n    def __init__(self):\n        super().__init__()\n        self._count = 0\n\n    async def invoke(self, input: int, *, deps=None) -&gt; int:\n        self._count += 1\n        # Fail first two times\n        if self._count % 3 != 0:\n            raise ValueError(f\"Failed on attempt {self._count}\")\n        return input * 2\n\n\n# Add retry logic\nunreliable = UnreliableBrick()\nreliable = unreliable.with_skill(\"retry\", max_retries=3)\n\ntry:\n    result = await reliable.invoke(21)\n    print(f\"Success! Result: {result}\")\nexcept Exception as e:\n    print(f\"Failed after retries: {e}\")\n\n🔄 Retry 1/3 after: Failed on attempt 1\n🔄 Retry 2/3 after: Failed on attempt 2\nSuccess! Result: 42\n\n\n\n\n\nStep 6: Built-in Skills\nStatus: Complete! ✅\nNanobricks comes with three powerful built-in skills that cover the most common needs.\n\nLogging Skill\nAutomatically log inputs, outputs, and errors with customizable formatting:\n\nimport asyncio\n\n# Import built-in skills\nimport nanobricks.skills\nfrom nanobricks import skill, NanobrickSimple\n\n\n@skill(\"logging\", level=\"INFO\", pretty=True)\nclass DataProcessorBrick(NanobrickSimple[dict, dict]):\n    \"\"\"Processes data with automatic logging.\"\"\"\n\n    async def invoke(self, input: dict, *, deps=None) -&gt; dict:\n        # Your processing logic\n        return {\"processed\": True, \"item_count\": len(input), \"keys\": list(input.keys())}\n\n\n# Use it\nprocessor = DataProcessorBrick()\nresult = await processor.invoke({\"a\": 1, \"b\": 2})\n\n# Logs automatically:\n# 🔵 Input: {\"a\": 1, \"b\": 2}\n# 🟢 Output: {\"processed\": true, \"item_count\": 2, \"keys\": [\"a\", \"b\"]} (took 0.001s)\n\nLogging options: - level: Log level (DEBUG, INFO, WARNING, ERROR) - log_inputs: Log inputs (default: True) - log_outputs: Log outputs (default: True) - log_errors: Log errors (default: True) - truncate: Max length for values (default: 100) - pretty: Pretty-print JSON (default: False)\n\n\nAPI Skill\nTurn any nanobrick into a REST API with FastAPI:\n\n@skill(\"api\", path=\"/analyze\", port=8080, docs=True)\nclass TextAnalyzerBrick(NanobrickSimple[str, dict]):\n    \"\"\"Analyzes text and returns statistics.\"\"\"\n\n    async def invoke(self, input: str, *, deps=None) -&gt; dict:\n        words = input.split()\n        return {\n            \"text\": input,\n            \"word_count\": len(words),\n            \"unique_words\": len(set(words)),\n        }\n\n\nanalyzer = TextAnalyzerBrick()\n\n# Start the API server\n# analyzer.start_server()  # Runs at http://localhost:8080/analyze\n\n# Access the API:\n# POST http://localhost:8080/analyze\n# {\"data\": \"hello world hello\"}\n\n# API docs at http://localhost:8080/docs\n\nAPI options: - path: Endpoint path (default: /{brick_name}) - method: HTTP method (default: POST) - host: Host to bind (default: 0.0.0.0) - port: Port to bind (default: 8000) - docs: Enable Swagger docs (default: True)\n\n\nCLI Skill\nCreate command-line interfaces with Typer:\n\n@skill(\"cli\", command=\"transform\", input_type=\"json\")\nclass DataTransformerBrick(NanobrickSimple[list, dict]):\n    \"\"\"Transforms data into summary statistics.\"\"\"\n\n    async def invoke(self, input: list, *, deps=None) -&gt; dict:\n        return {\n            \"total_items\": len(input),\n            \"first\": input[0] if input else None,\n            \"last\": input[-1] if input else None,\n        }\n\n\ntransformer = DataTransformerBrick()\n\n# Use from command line:\n# transform invoke '[1, 2, 3, 4, 5]'\n# transform invoke data.json --from-file\n# transform info\n# transform example\n\nCLI options: - command: Command name (default: brick name) - description: Help text - input_type: json, text, or file - output_format: json, text, or pretty\n\n\nCombining Built-in Skills\nThe real power comes from combining skills:\n\n@skill(\"logging\", level=\"INFO\")\n@skill(\"api\", port=8000)\n@skill(\"cli\", command=\"process\")\nclass ProcessorBrick(NanobrickSimple[dict, dict]):\n    \"\"\"A brick with multiple interfaces.\"\"\"\n\n    async def invoke(self, input: dict, *, deps=None) -&gt; dict:\n        # Process data\n        result = {k: v * 2 for k, v in input.items() if isinstance(v, (int, float))}\n        return result\n\n\nprocessor = ProcessorBrick()\n\n# Now you can:\n# 1. Call it directly (with logging)\nresult = await processor.invoke({\"a\": 5, \"b\": 10})\n\n# 2. Access via REST API\n# processor.start_server()\n\n# 3. Use from command line\n# process invoke '{\"a\": 5, \"b\": 10}'",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Nanobricks for Humans"
    ]
  },
  {
    "objectID": "human.html#try-it-yourself",
    "href": "human.html#try-it-yourself",
    "title": "Nanobricks for Humans",
    "section": "Try It Yourself!",
    "text": "Try It Yourself!\nOnce we start implementing, you’ll be able to run these examples:\n#| eval: true\n#| echo: true\n#| output: true\n\n# Install the package\npip install -e .\n\n# Run the examples\npython examples/basic_pipeline.py\npython examples/sync_usage.py\npython examples/dependency_injection.py\npython examples/configuration.py",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Nanobricks for Humans"
    ]
  },
  {
    "objectID": "human.html#questions-along-the-way",
    "href": "human.html#questions-along-the-way",
    "title": "Nanobricks for Humans",
    "section": "Questions Along the Way?",
    "text": "Questions Along the Way?\nAs we build, I’ll add troubleshooting tips and explanations here. This document will grow with our implementation!",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Nanobricks for Humans"
    ]
  },
  {
    "objectID": "human.html#installation-setup",
    "href": "human.html#installation-setup",
    "title": "Nanobricks for Humans",
    "section": "Installation & Setup",
    "text": "Installation & Setup\nNow that we have working code, here’s how to get started:\n# Clone the repository\ngit clone &lt;your-repo-url&gt;\ncd nano-1\n\n# Install in development mode\npip install -e \".[dev]\"\n\n# Run the tests\npytest tests/unit/test_protocol.py -v\n\n# Check types\nmypy src/nanobricks/ --strict",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Nanobricks for Humans"
    ]
  },
  {
    "objectID": "human.html#whats-working-now",
    "href": "human.html#whats-working-now",
    "title": "Nanobricks for Humans",
    "section": "What’s Working Now?",
    "text": "What’s Working Now?\n✅ Core protocol with NanobrickProtocol and NanobrickBase\n✅ NanobrickSimple for easy nanobricks without dependencies\n✅ Async and sync invocation\n✅ Type safety with generics\n✅ Full pipe operator composition (|)\n✅ NanobrickComposite for two-brick pipelines\n✅ Pipeline class for multi-brick chains\n✅ Error propagation (fail-fast)\n✅ Type transformations in pipelines\n✅ Dependency injection with StandardDeps\n✅ DependencyContainer for managing deps\n✅ Mock implementations for testing\n✅ Dependencies flow through pipelines\n✅ TOML-based configuration system\n✅ Environment-specific config overrides\n✅ Config discovery chain\n✅ Dot notation config access\n✅ Skill framework with enhance() pattern\n✅ Registry for skill management\n✅ with_skill() method chaining\n✅ @skill decorator support\n✅ Example skills: timing, retry, cache\n✅ Built-in logging skill with smart formatting\n✅ Built-in API skill with FastAPI\n✅ Built-in CLI skill with Typer\n✅ Built-in Docker skill with Dockerfile generation\n✅ Built-in Kubernetes skill with manifest & Helm chart generation\n✅ Performance optimizations: caching, batching, fusion, pooling\n✅ Working examples in examples/ directory\n✅ 221 tests, 74% coverage\n✅ Strict mypy type checking",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Nanobricks for Humans"
    ]
  },
  {
    "objectID": "human.html#try-it-now",
    "href": "human.html#try-it-now",
    "title": "Nanobricks for Humans",
    "section": "Try It Now!",
    "text": "Try It Now!\n# Run the examples\npython examples/basic_pipeline.py\npython examples/sync_usage.py\npython examples/dependency_injection.py\npython examples/configuration.py\npython examples/custom_skills.py\npython examples/builtin_skills.py\npython examples/deployment_example.py  # Docker & Kubernetes deployment\npython examples/performance_example.py  # NEW! Performance optimizations\n\n# Run all tests\npytest tests/unit/ -v",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Nanobricks for Humans"
    ]
  },
  {
    "objectID": "human.html#real-examples-from-the-examples-directory",
    "href": "human.html#real-examples-from-the-examples-directory",
    "title": "Nanobricks for Humans",
    "section": "Real Examples from the examples/ Directory",
    "text": "Real Examples from the examples/ Directory\nHere are some highlights from our working examples:\n\nFrom basic_pipeline.py\n\nimport asyncio\nfrom typing import List\nfrom nanobricks import NanobrickSimple\n\n\n# Email processing example from basic_pipeline.py\nclass ValidateEmailBrick(NanobrickSimple[str, str]):\n    \"\"\"Validates that input looks like an email.\"\"\"\n\n    async def invoke(self, input: str, *, deps=None) -&gt; str:\n        if \"@\" not in input or \".\" not in input:\n            raise ValueError(f\"Invalid email format: {input}\")\n        return input\n\n\nclass NormalizeEmailBrick(NanobrickSimple[str, str]):\n    \"\"\"Normalizes email to lowercase and strips whitespace.\"\"\"\n\n    async def invoke(self, input: str, *, deps=None) -&gt; str:\n        return input.strip().lower()\n\n\nclass ExtractDomainBrick(NanobrickSimple[str, str]):\n    \"\"\"Extracts domain from email address.\"\"\"\n\n    async def invoke(self, input: str, *, deps=None) -&gt; str:\n        return input.split(\"@\")[1]\n\n\n# Create pipeline\nvalidate = ValidateEmailBrick(name=\"validate\")\nnormalize = NormalizeEmailBrick(name=\"normalize\")\nextract = ExtractDomainBrick(name=\"extract\")\n\nemail_pipeline = validate | normalize | extract\n\n# Test with various inputs\ntest_emails = [\"  John.Doe@Example.COM  \", \"alice@wonderland.io\", \"BOB@CORP.NET\"]\n\nprint(\"Email Processing Pipeline:\")\nfor email in test_emails:\n    try:\n        domain = await email_pipeline.invoke(email)\n        print(f\"  {email:30} -&gt; {domain}\")\n    except ValueError as e:\n        print(f\"  {email:30} -&gt; ERROR: {e}\")\n\nEmail Processing Pipeline:\n    John.Doe@Example.COM         -&gt; example.com\n  alice@wonderland.io            -&gt; wonderland.io\n  BOB@CORP.NET                   -&gt; corp.net\n\n\n\n\nFrom dependency_injection.py - Smart Caching\n\nfrom typing import Optional\nfrom nanobricks import NanobrickBase\nfrom nanobricks.dependencies import (\n    StandardDeps,\n    MockDatabase,\n    MockCache,\n    MockLogger,\n    DependencyContainer,\n)\n\n\n# Smart loader with caching from dependency_injection.py\nclass SmartUserLoader(NanobrickBase[str, dict, StandardDeps]):\n    \"\"\"Loads user data with intelligent caching.\"\"\"\n\n    async def invoke(\n        self, user_id: str, *, deps: Optional[StandardDeps] = None\n    ) -&gt; dict:\n        if not deps:\n            raise ValueError(\"Dependencies required\")\n\n        # Check cache first\n        cache_key = f\"user:{user_id}\"\n        if \"cache\" in deps:\n            cached_data = await deps[\"cache\"].get(cache_key)\n            if cached_data:\n                if \"logger\" in deps:\n                    deps[\"logger\"].debug(f\"Cache hit for {cache_key}\")\n                return cached_data\n\n        # Load from database\n        if \"db\" not in deps:\n            raise ValueError(\"Database required\")\n\n        results = await deps[\"db\"].query(\n            \"SELECT * FROM users WHERE id = ?\", {\"id\": user_id}\n        )\n\n        if not results:\n            raise ValueError(f\"User {user_id} not found\")\n\n        user_data = results[0]\n\n        # Store in cache for next time\n        if \"cache\" in deps:\n            await deps[\"cache\"].set(cache_key, user_data, ttl=300)\n            if \"logger\" in deps:\n                deps[\"logger\"].debug(f\"Cached data for {cache_key}\")\n\n        return user_data\n\n\n# Set up dependencies\nmock_db = MockDatabase(\n    {\n        \"SELECT * FROM users WHERE id = ?\": [\n            {\"id\": \"123\", \"name\": \"Alice\", \"email\": \"alice@example.com\"}\n        ]\n    }\n)\n\ndeps = DependencyContainer(db=mock_db, cache=MockCache(), logger=MockLogger())\n\nloader = SmartUserLoader()\n\n# First call - hits database\nprint(\"First call (DB):\")\nuser1 = await loader.invoke(\"123\", deps=deps.to_dict())\nprint(f\"  User: {user1}\")\nprint(f\"  DB queries: {len(mock_db.queries)}\")\n\n# Second call - hits cache\nmock_db.queries.clear()\nprint(\"\\nSecond call (Cache):\")\nuser2 = await loader.invoke(\"123\", deps=deps.to_dict())\nprint(f\"  User: {user2}\")\nprint(f\"  DB queries: {len(mock_db.queries)} (cache hit!)\")\n\nFirst call (DB):\n  User: {'id': '123', 'name': 'Alice', 'email': 'alice@example.com'}\n  DB queries: 1\n\nSecond call (Cache):\n  User: {'id': '123', 'name': 'Alice', 'email': 'alice@example.com'}\n  DB queries: 0 (cache hit!)\n\n\n\n\nStep 7: Deployment Skills\nStatus: Complete! ✅\nDeploy your nanobricks anywhere with automatic containerization and orchestration!\n\nDocker Skill\nAutomatically containerize your nanobricks with zero configuration:\n\nfrom nanobricks import NanobrickSimple\nfrom nanobricks.skills.deployment import SkillDocker, DockerConfig\n\n# Define a simple brick for the example\nclass MyDataProcessor(NanobrickSimple[dict, dict]):\n    \"\"\"Process data with validation.\"\"\"\n    \n    def __init__(self):\n        super().__init__(name=\"mydataprocessor\", version=\"1.0.0\")\n    \n    async def invoke(self, input: dict, *, deps=None) -&gt; dict:\n        # Process the data\n        return {\"processed\": True, \"count\": len(input)}\n\n# Simple containerization\ndocker_skill = SkillDocker()\nbrick = MyDataProcessor()\nenhanced = docker_skill.enhance(brick)\n\n# Generate Dockerfile\ndockerfile = docker_skill.generate_dockerfile(brick)\nprint(\"Generated Dockerfile:\")\nprint(dockerfile)\n\n# Output:\n# FROM python:3.13-slim\n# WORKDIR /app\n# COPY requirements.txt .\n# RUN pip install -r requirements.txt\n# COPY . .\n# EXPOSE 8000\n# CMD [\"python\", \"-m\", \"nanobricks.run\", \"mydataprocessor\"]\n\nGenerated Dockerfile:\nFROM python:3.13-slim\n\nWORKDIR /app\n\n# Copy application code\nCOPY . .\n\nCMD [\"python\", \"-m\", \"nanobricks\", \"run\", \"mydataprocessor\"]\n\n\nWith custom configuration:\n\n# Continue from previous cell - redefine for this cell\nfrom nanobricks import NanobrickSimple\nfrom nanobricks.skills.deployment import SkillDocker, DockerConfig\n\nclass MyDataProcessor(NanobrickSimple[dict, dict]):\n    \"\"\"Process data with validation.\"\"\"\n    \n    def __init__(self):\n        super().__init__(name=\"mydataprocessor\", version=\"1.0.0\")\n    \n    async def invoke(self, input: dict, *, deps=None) -&gt; dict:\n        return {\"processed\": True, \"count\": len(input)}\n\nbrick = MyDataProcessor()\n\n# Advanced Docker configuration\ndocker_config = DockerConfig(\n    base_image=\"python:3.13-alpine\",\n    expose_ports=[8080],\n    environment={\"API_KEY\": \"${API_KEY}\", \"ENV\": \"production\"},\n    labels={\"app\": \"nanobricks\", \"version\": \"1.0.0\"},\n    healthcheck={\n        \"test\": [\"CMD\", \"curl\", \"-f\", \"http://localhost:8080/health\"],\n        \"interval\": \"30s\",\n        \"timeout\": \"3s\",\n        \"retries\": 3,\n    },\n    system_packages=[\"curl\"],  # For healthcheck\n)\n\ndocker_skill = SkillDocker(docker_config)\n\n# Generate docker-compose.yml\n# Note: generate_compose expects a dict of services\ncompose = docker_skill.generate_compose(\n    services={\"myapp\": brick},\n    networks=[\"app-network\"],\n)\nprint(\"Docker Compose configuration generated!\")\nprint(\"Compose file includes services, networks, and configuration.\")\n\nDocker Compose configuration generated!\nCompose file includes services, networks, and configuration.\n\n\n\n\nKubernetes Skill\nDeploy to Kubernetes with automatic manifest generation:\n\nfrom nanobricks.skills.deployment import SkillKubernetes, KubernetesConfig\n\n# Basic Kubernetes deployment\nk8s_config = KubernetesConfig(\n    namespace=\"production\",\n    replicas=3,\n    resources={\n        \"requests\": {\"cpu\": \"100m\", \"memory\": \"128Mi\"},\n        \"limits\": {\"cpu\": \"500m\", \"memory\": \"512Mi\"},\n    },\n    labels={\"app\": \"nanobricks\", \"tier\": \"backend\"},\n)\n\nk8s_skill = SkillKubernetes(k8s_config)\n# brick = MyAPIBrick()\n\n# Generate all Kubernetes manifests\n# manifests = k8s_skill.generate_manifests(brick, \"myapp:v1.0.0\")\n\n# Includes:\n# - Deployment with health checks and resource limits\n# - Service for load balancing\n# - HorizontalPodAutoscaler for auto-scaling\n# - ConfigMap for configuration\n\nGenerate Helm charts for production deployments:\n\n# Generate complete Helm chart\nhelm_chart = k8s_skill.generate_helm_chart(brick, \"myapp:v1.0.0\")\n\n# Creates:\n# - Chart.yaml with metadata\n# - values.yaml with configurable parameters\n# - Templates for all Kubernetes resources\n# - Support for multiple environments\n\nprint(\"Helm chart structure:\")\nfor filename, content in helm_chart.items():\n    print(f\"  {filename}\")\n\nHelm chart structure:\n  Chart.yaml\n  values.yaml\n  templates/deployment.yaml\n  templates/service.yaml\n\n\n\n\nComplete Deployment Pipeline\nCombine Docker and Kubernetes for full deployment automation:\n\nfrom nanobricks import NanobrickSimple, skill\nfrom nanobricks.skills.deployment import SkillDocker, SkillKubernetes\n\n\n@skill(\"logging\")\n@skill(\"api\", port=8080)\nclass ProductionBrick(NanobrickSimple[dict, dict]):\n    \"\"\"A production-ready nanobrick.\"\"\"\n\n    async def invoke(self, input: dict, *, deps=None) -&gt; dict:\n        # Your business logic here\n        return {\"processed\": True, \"items\": len(input)}\n\n\n# Create deployment pipeline\nbrick = ProductionBrick()\n\n# 1. Containerize\ndocker = SkillDocker()\ndockerfile = docker.generate_dockerfile(brick)\ncompose = docker.generate_docker_compose(brick, \"myapp:latest\")\n\n# 2. Deploy to Kubernetes\nk8s = SkillKubernetes(\n    KubernetesConfig(\n        namespace=\"production\",\n        replicas=5,\n        autoscaling=True,\n        min_replicas=3,\n        max_replicas=10,\n        target_cpu_utilization=70,\n    )\n)\n\nmanifests = k8s.generate_manifests(brick, \"myapp:latest\")\nhelm_chart = k8s.generate_helm_chart(brick, \"myapp:latest\")\n\nprint(\"✅ Deployment pipeline ready!\")\nprint(f\"   - Dockerfile: {len(dockerfile)} lines\")\nprint(f\"   - Kubernetes manifests: {len(manifests)} resources\")\nprint(f\"   - Helm chart: {len(helm_chart)} files\")\n\n\n\n\nStep 8: Performance Optimizations\nStatus: Complete! ✅\nMake your nanobricks blazing fast with built-in performance optimizations!\n\nCaching\nAdd intelligent caching to expensive operations:\n\nfrom nanobricks.performance import with_cache\n\n\n# Create an expensive brick\nclass DatabaseLookupBrick(NanobrickSimple[str, dict]):\n    async def invoke(self, key: str, *, deps=None) -&gt; dict:\n        # Simulate expensive database query\n        await asyncio.sleep(0.5)\n        return {\"key\": key, \"data\": f\"Data for {key}\"}\n\n\n# Add caching\ndb_brick = DatabaseLookupBrick()\ncached_db = with_cache(db_brick, max_size=100, ttl=300)  # 5 min TTL\n\n# First call - slow (cache miss)\nresult1 = await cached_db.invoke(\"user123\")  # Takes 0.5s\n\n# Second call - fast (cache hit)\nresult2 = await cached_db.invoke(\"user123\")  # Instant!\n\n# Check cache stats\nprint(cached_db.cache_info())\n# {'size': 1, 'max_size': 100, 'ttl': 300, ...}\n\n{'size': 1, 'error_size': 0, 'max_size': 100, 'ttl': 300, 'hits': 0, 'misses': 0}\n\n\n\n\nBatching\nProcess multiple items efficiently:\n\nfrom nanobricks.performance import with_batching\n\n\n# Create a brick that benefits from batching\nclass APICallBrick(NanobrickSimple[str, dict]):\n    async def invoke(self, item: str, *, deps=None) -&gt; dict:\n        # Simulate API call\n        await asyncio.sleep(0.1)\n        return {\"id\": item, \"processed\": True}\n\n\n# Add batching\napi_brick = APICallBrick()\nbatched_api = with_batching(api_brick, batch_size=10)\n\n# Process multiple items efficiently\nitems = [f\"item_{i}\" for i in range(20)]\nresults = await batched_api.invoke(items)\nprint(results)\n# Processes in batches\n\n[{'id': 'item_0', 'processed': True}, {'id': 'item_1', 'processed': True}, {'id': 'item_2', 'processed': True}, {'id': 'item_3', 'processed': True}, {'id': 'item_4', 'processed': True}, {'id': 'item_5', 'processed': True}, {'id': 'item_6', 'processed': True}, {'id': 'item_7', 'processed': True}, {'id': 'item_8', 'processed': True}, {'id': 'item_9', 'processed': True}, {'id': 'item_10', 'processed': True}, {'id': 'item_11', 'processed': True}, {'id': 'item_12', 'processed': True}, {'id': 'item_13', 'processed': True}, {'id': 'item_14', 'processed': True}, {'id': 'item_15', 'processed': True}, {'id': 'item_16', 'processed': True}, {'id': 'item_17', 'processed': True}, {'id': 'item_18', 'processed': True}, {'id': 'item_19', 'processed': True}]\n\n\n\n\nPipeline Fusion\nOptimize long pipelines by fusing operations:\n\nfrom nanobricks import Pipeline\nfrom nanobricks.performance import fuse_pipeline\n\n# Create a pipeline\nstep1 = TransformBrick()\nstep2 = ValidateBrick()\nstep3 = EnrichBrick()\nstep4 = SaveBrick()\n\n# Regular pipeline\npipeline = Pipeline(step1, step2, step3, step4)\n\n# Fused pipeline - reduces async overhead\nfused = fuse_pipeline(pipeline)\n\n# Both work the same, but fused is faster\nresult = await fused.invoke(data)\n\n\n\nMemory Pooling\nReuse objects to reduce allocation overhead:\n\nfrom nanobricks.performance import MemoryPool\n\n\n# Create a pool of reusable processors\nclass DataProcessor:\n    def __init__(self):\n        self.buffer = bytearray(1024)\n        self.results = []\n\n    def reset(self):\n        self.results.clear()\n\n    def process(self, data):\n        # Process using pre-allocated buffer\n        self.results.append(len(data))\n        return self.results\n\n\n# Create pool\npool = MemoryPool(DataProcessor, size=5)\n\n# Use pooled objects\nprocessor = pool.acquire()\nresult = processor.process(\"hello\")\nprint(result)\npool.release(processor)  # Returns to pool for reuse\n\n[5]\n\n\n\n\nCombined Optimizations\nStack optimizations for maximum performance:\n\nfrom nanobricks import NanobrickSimple\nfrom nanobricks.performance import with_cache, fuse_pipeline\n\n# Create a complex pipeline\nvalidator = DataValidator()\nenricher = DataEnricher()\ntransformer = DataTransformer()\n\n# Add caching to expensive operations\ncached_enricher = with_cache(enricher, max_size=1000)\n\n# Create and fuse pipeline\npipeline = validator | cached_enricher | transformer\nfused = fuse_pipeline([validator, cached_enricher, transformer])\n\n# Ultra-fast processing with caching and fusion\nresults = []\nfor item in large_dataset:\n    result = await fused.invoke(item)  # Cached + fused!\n    results.append(result)",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Nanobricks for Humans"
    ]
  },
  {
    "objectID": "human.html#built-in-transformers",
    "href": "human.html#built-in-transformers",
    "title": "Nanobricks for Humans",
    "section": "Built-in Transformers",
    "text": "Built-in Transformers\n\nCSV Transformers\nWork with CSV data seamlessly:\n\nfrom nanobricks.transformers import CSVParser, CSVSerializer\n\n# Parse CSV text\nparser = CSVParser(delimiter=\",\", strip_values=True)\ndata = await parser.invoke(\"\"\"name,age,city\nAlice,30,New York\nBob,25,London\"\"\")\n# Result: [{\"name\": \"Alice\", \"age\": \"30\", \"city\": \"New York\"}, ...]\n\n# Serialize back to CSV\nserializer = CSVSerializer(columns=[\"name\", \"city\"])  # Select columns\ncsv_text = await serializer.invoke(data)\n\n\n\nText Normalization\nClean and normalize text data:\n\nfrom nanobricks.transformers import TextNormalizer\n\n# Configure normalization\nnormalizer = TextNormalizer(\n    lowercase=True,\n    remove_punctuation=True,\n    expand_contractions=True,\n    remove_urls=True,\n    custom_replacements={\n        \"CEO\": \"Chief Executive Officer\",\n        \"AI\": \"Artificial Intelligence\"\n    }\n)\n\ntext = \"The CEO said: Don't miss our AI update at https://example.com!\"\nclean = await normalizer.invoke(text)\n# Result: \"the chief executive officer said do not miss our artificial intelligence update\"\n\n\n\nSmart Type Conversion\nConvert between types intelligently:\n\nfrom nanobricks.transformers import SmartTypeConverter, BulkTypeConverter\n\n# Single value conversion\nto_date = SmartTypeConverter(target_type=date, date_format=\"%Y-%m-%d\")\nresult = await to_date.invoke(\"2024-01-15\")  # Returns date object\n\n# Bulk conversion with error handling\nto_int = BulkTypeConverter(\n    target_type=int,\n    skip_errors=True,\n    error_value=0,\n    report_errors=True\n)\n\nvalues = [\"42\", \"100\", \"invalid\", \"75\"]\nresult = await to_int.invoke(values)\n# Result: {\"results\": [42, 100, 0, 75], \"error_count\": 1, ...}\n\n\n\nDynamic Type Conversion\nConvert dictionaries with type hints:\n\nfrom nanobricks.transformers import DynamicTypeConverter\n\n# Configure type mappings\nconverter = DynamicTypeConverter(\n    type_map={\"age\": int, \"active\": bool},\n    infer_types=True  # Auto-detect numbers and booleans\n)\n\ndata = {\n    \"name\": \"Alice\",\n    \"age\": \"30\",        # Will convert to int\n    \"salary\": \"50000.50\",  # Will infer as float\n    \"active\": \"yes\"     # Will convert to bool\n}\n\nresult = await converter.invoke(data)\n# Result: {\"name\": \"Alice\", \"age\": 30, \"salary\": 50000.5, \"active\": True}\n\n\n\nDataFrame Transformers\nWork with pandas DataFrames for powerful data analysis:\n\nfrom nanobricks.transformers import (\n    DataFrameFilter,\n    DataFrameGroupBy,\n    DataFrameMerge,\n    DataFrameReshape\n)\n\n# Filter data\nsales_data = [\n    {\"product\": \"Laptop\", \"price\": 999.99, \"quantity\": 5, \"region\": \"North\"},\n    {\"product\": \"Mouse\", \"price\": 29.99, \"quantity\": 15, \"region\": \"South\"},\n    {\"product\": \"Laptop\", \"price\": 999.99, \"quantity\": 3, \"region\": \"South\"},\n]\n\n# Filter high-value sales\nfilter = DataFrameFilter(\n    column=\"price\",\n    value=100,\n    op=\"&gt;\"\n)\nexpensive_items = await filter.invoke(sales_data)\n\n# Group and aggregate\ngroupby = DataFrameGroupBy(\n    by=\"product\",\n    agg={\n        \"quantity\": \"sum\",\n        \"price\": [\"mean\", \"count\"]\n    }\n)\nproduct_summary = await groupby.invoke(sales_data)\n\n# Pivot data\npivot = DataFrameReshape(\n    reshape_type=\"pivot\",\n    index=\"product\",\n    columns=\"region\",\n    values=\"quantity\",\n    aggfunc=\"sum\"\n)\npivot_table = await pivot.invoke(sales_data)\n\n\nAdvanced DataFrame Operations\n\n# Complex filtering with queries\ncomplex_filter = DataFrameFilter(\n    query=\"price &gt; 50 and region == 'North' and quantity &gt;= 5\"\n)\nfiltered = await complex_filter.invoke(sales_data)\n\n# Merge with additional data\nproduct_info = pd.DataFrame([\n    {\"product\": \"Laptop\", \"category\": \"Electronics\", \"cost\": 700},\n    {\"product\": \"Mouse\", \"category\": \"Electronics\", \"cost\": 15}\n])\n\nmerge = DataFrameMerge(\n    other=product_info,\n    how=\"left\",\n    on=\"product\"\n)\nenriched_data = await merge.invoke(sales_data)\n\n# Time series operations (requires datetime index)\nfrom nanobricks.transformers import DataFrameTimeSeriesOperator\n\n# Rolling average\nrolling = DataFrameTimeSeriesOperator(\n    operation=\"rolling\",\n    window=7,\n    func=\"mean\"\n)\nsmoothed_data = await rolling.invoke(time_series_df)\n\n# Resample to weekly\nresample = DataFrameTimeSeriesOperator(\n    operation=\"resample\",\n    rule=\"W\",  # Weekly\n    func=\"sum\"\n)\nweekly_data = await resample.invoke(time_series_df)",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Nanobricks for Humans"
    ]
  },
  {
    "objectID": "human.html#developer-tools",
    "href": "human.html#developer-tools",
    "title": "Nanobricks for Humans",
    "section": "Developer Tools",
    "text": "Developer Tools\n\nProject Scaffolding\nCreate new nanobrick projects in seconds:\n# Create a new project\nnanobrick new my-awesome-brick \\\n  --description \"An awesome data processor\" \\\n  --author \"Your Name\" \\\n  --email \"you@example.com\"\n\n# Project structure created:\nmy-awesome-brick/\n├── pyproject.toml          # Project metadata & dependencies\n├── nanobrick.toml          # Nanobrick configuration\n├── README.md               # Documentation\n├── .gitignore              # Git ignore patterns\n├── .vscode/\n│   └── settings.json       # VS Code configuration\n├── src/\n│   └── my_awesome_brick/\n│       ├── __init__.py\n│       └── core.py         # Your brick implementation\n└── tests/\n    ├── __init__.py\n    └── test_my_awesome_brick.py\nThe generated brick is ready to use:\n\nfrom my_awesome_brick import MyAwesomeBrick\n\n# Use your new brick\nbrick = MyAwesomeBrick(prefix=\"AWESOME\")\nresult = await brick.invoke(\"hello\")\n# Result: \"AWESOME: hello processed\"\n\n\n\nDocumentation Generation\nAutomatically generate documentation from your nanobricks:\n\nfrom nanobricks.docs import DocumentationGenerator\n\n# Generate docs for your bricks\ngenerator = DocumentationGenerator(output_dir=\"docs/api\")\n\n# Document individual bricks\nfrom my_bricks import DataProcessor, Validator, Transformer\n\ngenerator.write_docs([DataProcessor, Validator, Transformer])\n\n# Creates:\n# docs/api/\n#   ├── index.md              # Overview with links\n#   ├── dataprocessor.md      # Full docs for DataProcessor\n#   ├── validator.md          # Full docs for Validator\n#   └── transformer.md        # Full docs for Transformer\n\nGenerated documentation includes:\n\nType signatures with input/output/dependencies\nConstructor parameters\nMethod documentation\nUsage examples extracted from docstrings\nSupported skills\nComposition examples\n\nExample generated documentation:\n# DataProcessor\n\n`my_bricks.DataProcessor`\n\nA brick that processes data with validation and transformation.\n\n## Type Signature\n\n- **Input**: `dict`\n- **Output**: `dict`\n- **Dependencies**: `DatabaseConfig`\n\n## Constructor\n\n```python\nDataProcessor(validate=True, transform=True)\nInitialize data processor with options…",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Nanobricks for Humans"
    ]
  },
  {
    "objectID": "human.html#examples",
    "href": "human.html#examples",
    "title": "Nanobricks for Humans",
    "section": "Examples",
    "text": "Examples\n&gt;&gt;&gt; processor = DataProcessor()\n&gt;&gt;&gt; result = await processor.invoke({\"name\": \"Alice\", \"age\": 30})\n&gt;&gt;&gt; print(result)\n{\"name\": \"ALICE\", \"age\": 30, \"valid\": True}",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Nanobricks for Humans"
    ]
  },
  {
    "objectID": "human.html#supported-skills",
    "href": "human.html#supported-skills",
    "title": "Nanobricks for Humans",
    "section": "Supported Skills",
    "text": "Supported Skills\n\nSkill Framework\nLogging\nAPI\nObservability\n\n\n### Developer Experience Tools\n\n#### Pipeline Debugger\n\nDebug your pipelines step-by-step:\n\n::: {#aebdfde4 .cell execution_count=44}\n``` {.python .cell-code}\nfrom nanobricks.devtools import BrickDebugger\n\n# Create debugger\ndebugger = BrickDebugger(\n    capture_input=True,\n    capture_output=True,\n    capture_errors=True,\n    save_to_file=\"debug.json\"\n)\n\n# Wrap bricks with debugging\nprocessor = debugger.wrap_brick(DataProcessor())\nvalidator = debugger.wrap_brick(Validator())\n\n# Create debugged pipeline\npipeline = processor | validator\n\n# Run and see what happens\nresult = await pipeline.invoke({\"name\": \"Alice\", \"age\": 30})\n\n# Print execution trace\ndebugger.print_trace()\n# Output:\n# 📊 Execution Trace:\n# ============================================================\n# \n# [14:32:15.123] ▶️  DataProcessor\n#     Input: {'name': 'Alice', 'age': 30}\n# [14:32:15.234] ✅ DataProcessor (111.00ms)\n#     Output: {'name': 'ALICE', 'age': 30, 'processed': True}\n# \n# [14:32:15.235] ▶️  Validator\n#     Input: {'name': 'ALICE', 'age': 30, 'processed': True}\n# [14:32:15.267] ✅ Validator (32.00ms)\n#     Output: {'name': 'ALICE', 'age': 30, 'processed': True, 'valid': True}\n:::\n\nPerformance Profiler\nProfile your bricks to find bottlenecks:\n\nfrom nanobricks.devtools import BrickProfiler, profile_brick\n\n# Create profiler\nprofiler = BrickProfiler(measure_memory=True)\n\n# Profile a pipeline\nslow_brick = profiler.wrap_brick(SlowProcessor())\nfast_brick = profiler.wrap_brick(FastValidator())\npipeline = slow_brick | fast_brick\n\n# Run multiple times\nfor i in range(100):\n    await pipeline.invoke(f\"data_{i}\")\n\n# Print performance stats\nprofiler.print_stats()\n# Output:\n# ⏱️  Performance Profile:\n# ================================================================================\n# Brick                          Calls   Total(ms)    Avg(ms)    Min(ms)    Max(ms)\n# --------------------------------------------------------------------------------\n# SlowProcessor                    100     8543.21      85.43      82.10      95.33\n# FastValidator                    100      234.56       2.35       2.01       3.44\n# \n# 💾 Memory Usage:\n# --------------------------------------------------------------------------------\n# SlowProcessor                  Delta:   +12.34 MB\n\n# Find bottlenecks\nbottlenecks = profiler.get_bottlenecks(threshold_pct=50)\nprint(f\"Bottlenecks: {bottlenecks}\")  # ['SlowProcessor']\n\n\n\nPipeline Visualizer\nVisualize your pipeline structure:\n\nfrom nanobricks.devtools import visualize_pipeline\n\n# Create a complex pipeline\nloader = DataLoader()\nprocessor = DataProcessor()\nvalidator = Validator()\nsaver = DataSaver()\n\npipeline = loader | processor | validator | saver\n\n# ASCII visualization\nprint(visualize_pipeline(pipeline))\n# Output:\n# Pipeline Flow:\n# ==================================================\n# \n#      INPUT\n#        │\n#        ▼\n# ┌──────────────────────┐\n# │      DataLoader      │\n# │     DataLoader       │\n# │       v1.0.0         │\n# └──────────────────────┘\n#        │\n#        ▼\n# ┌──────────────────────┐\n# │    DataProcessor     │\n# │    DataProcessor     │\n# │       v1.0.0         │\n# └──────────────────────┘\n#        │\n#        ▼\n# ┌──────────────────────┐\n# │      Validator       │\n# │      Validator       │\n# │       v1.0.0         │\n# └──────────────────────┘\n#        │\n#        ▼\n# ┌──────────────────────┐\n# │      DataSaver       │\n# │      DataSaver       │\n# │       v1.0.0         │\n# └──────────────────────┘\n#        │\n#        ▼\n#      OUTPUT\n\n# Generate Mermaid diagram\nmermaid = visualize_pipeline(pipeline, style=\"mermaid\", save_to=\"pipeline.mmd\")",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Nanobricks for Humans"
    ]
  },
  {
    "objectID": "human.html#ai-integration",
    "href": "human.html#ai-integration",
    "title": "Nanobricks for Humans",
    "section": "AI Integration",
    "text": "AI Integration\n\nAI Reasoning Skill\nEnhance your nanobricks with AI-powered intelligence:\n\nfrom nanobricks.skills import create_ai_skill\n\n# Create AI skill with cost controls\nai_skill = create_ai_skill(\n    max_cost_per_invoke=0.05,  # $0.05 limit per call\n    enable_reasoning_trace=True,\n    enable_memory=True,\n)\n\n# Enhance any brick with AI\nvalidator = DataValidator()\nai_validator = validator.with_skill(ai_skill)\n\n# The AI will:\n# - Understand complex validation rules from context\n# - Learn from past validations\n# - Provide intelligent error recovery\n# - Track costs to prevent overruns\n\nresult = await ai_validator.invoke(complex_data)\n\n# Check AI reasoning\nif hasattr(ai_validator, 'get_reasoning_trace'):\n    trace = ai_validator.get_reasoning_trace()\n    for step in trace:\n        print(f\"{step.timestamp}: {step.thought}\")\n\n# Monitor costs\nprint(f\"AI costs: {ai_skill.get_cost_report()}\")\n\n\n\nMCP Server Integration\nExpose your nanobricks as tools for LLMs via Model Context Protocol:\n\nfrom nanobricks.skills import create_mcp_server, MCPToolConfig\n\n# Configure tool exposure\nsummarizer = TextSummarizer()\nanalyzer = DataAnalyzer()\n\n# Create MCP server\nmcp_server = create_mcp_server(\n    bricks=[summarizer, analyzer],\n    server_name=\"my-tools\",\n    configs={\n        \"TextSummarizer\": MCPToolConfig(\n            name=\"summarize\",\n            description=\"Summarizes text intelligently\",\n            example_inputs=[{\"text\": \"Long article...\"}],\n        )\n    }\n)\n\n# Run server for LLM access\nawait mcp_server.run_server()\n\n# Your LLM can now use these tools!\n\n\n\nAgent Communication\nBuild multi-agent systems with nanobricks:\n\nfrom nanobricks.agent import create_agent\n\n# Create agents from bricks\nprocessor = create_agent(DataProcessor(), name=\"Processor Agent\")\nvalidator = create_agent(DataValidator(), name=\"Validator Agent\") \nanalyzer = create_agent(DataAnalyzer(), name=\"Analyzer Agent\")\n\n# Agents can discover each other\ndiscovered = await processor.discover_agents(capability=\"validation\")\n\n# Send messages between agents\nawait processor.send_message(\n    to_agent=validator.id,\n    message_type=MessageType.REQUEST,\n    content={\"validate\": data}\n)\n\n# Request processing from another agent\nresult = await processor.request_processing(\n    target_agent=analyzer.id,\n    input=processed_data,\n    timeout=30.0\n)\n\n# Broadcast to all agents\nawait processor.broadcast(\n    MessageType.ANNOUNCE,\n    {\"status\": \"processing complete\"}\n)\n\n\n\nAdaptive Behavior\nCreate self-tuning nanobricks that learn and adapt:\n\nfrom nanobricks.adaptive import create_adaptive_brick\nfrom nanobricks.adaptive.policies import ThresholdPolicy, MLPolicy\n\n# Simple threshold-based adaptation\npolicy = ThresholdPolicy(\n    latency_threshold_ms=100,\n    error_rate_threshold=0.05\n)\n\nprocessor = DataProcessor()\nadaptive_processor = create_adaptive_brick(processor, policy)\n\n# The brick will automatically:\n# - Monitor performance metrics\n# - Detect when thresholds are exceeded\n# - Adjust parameters (batch size, concurrency, etc.)\n# - Retry with different strategies on errors\n\n# Use ML-based adaptation for complex scenarios\nml_policy = MLPolicy(model_path=\"adaptation_model.pkl\")\nsmart_processor = create_adaptive_brick(processor, ml_policy)\n\n# Check adaptation status\nmetrics = adaptive_processor.get_metrics_summary()\nprint(f\"Success rate: {metrics['success_rate']:.2%}\")\nprint(f\"Current adaptations: {metrics['current_adaptations']}\")\n\n\n\nAI-Powered Examples\n\nIntelligent Validator\n\n# A validator that understands context\nvalidator = IntelligentValidator(\n    context=\"user registration with email and phone\"\n)\n\n# Add AI for smarter validation\nai_validator = validator.with_skill(create_ai_skill())\n\nresult = await ai_validator.invoke({\n    \"name\": \"\",\n    \"email\": \"invalid-email\",\n    \"phone\": \"555\"\n})\n\n# AI provides intelligent feedback:\n# - Errors: [\"Invalid email format\"]\n# - Warnings: [\"Name is empty\", \"Phone too short\"]  \n# - Suggestions: [\"Use format: name@domain.com\", \"Include country code\"]\n\n\n\nConversational Pipeline Assistant\n\n# Create a conversational assistant for your pipeline\nassistant = ConversationalBrick(\n    system_prompt=\"Help users debug data pipelines\"\n)\n\n# Use in your pipeline for interactive debugging\npipeline = DataLoader() | processor | assistant | validator\n\n# The assistant can:\n# - Answer questions about the pipeline\n# - Explain errors in plain language\n# - Suggest optimizations\n# - Guide users through debugging",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Nanobricks for Humans"
    ]
  },
  {
    "objectID": "human.html#security-features",
    "href": "human.html#security-features",
    "title": "Nanobricks for Humans",
    "section": "Security Features",
    "text": "Security Features\nProtect your nanobricks with comprehensive security measures:\n\nInput Sanitization\nPrevent injection attacks with automatic sanitization:\n\nfrom nanobricks.security import InputSanitizer\n\n# Protect against XSS and SQL injection\nsanitizer = InputSanitizer(\n    UserService(),\n    html_escape=True,\n    sql_escape=True,\n    max_length=1000,\n    allowed_chars=\"a-zA-Z0-9 .-_\",\n)\n\n# Malicious input gets sanitized\nresult = await sanitizer.invoke('&lt;script&gt;alert(\"xss\")&lt;/script&gt;')\n# Result: '&lt;script&gt;alert(&quot;xss&quot;)&lt;/script&gt;'\n\n\n\nRate Limiting\nPrevent abuse with configurable rate limits:\n\nfrom nanobricks.security import RateLimiter\n\n# Limit requests per user\nlimiter = RateLimiter(\n    ApiService(),\n    max_requests=100,\n    window_seconds=60,\n    burst_size=10,  # Max burst within short period\n    key_func=lambda input, deps: deps[\"user_id\"],\n)\n\n# Exceeding limits raises an error\ntry:\n    await limiter.invoke(request, deps={\"user_id\": \"alice\"})\nexcept ValueError as e:\n    print(f\"Rate limited: {e}\")\n\n\n\nPermission-Based Access Control\nFine-grained permissions and role-based access:\n\nfrom nanobricks.security import PermissionGuard, Permission, SecurityContext\n\n# Require specific permissions\nadmin_only = PermissionGuard(\n    DeleteService(),\n    required_permissions={Permission.DELETE, Permission.ADMIN},\n    required_roles={\"administrator\"},\n)\n\n# Create security context\ncontext = SecurityContext(\n    user_id=\"alice\",\n    permissions={Permission.READ, Permission.WRITE},\n    roles={\"user\"},\n)\n\n# Access denied without proper permissions\ntry:\n    await admin_only.invoke(request, deps={\"security_context\": context})\nexcept PermissionError:\n    print(\"Access denied\")\n\n\n\nEncryption\nProtect sensitive data at rest and in transit:\n\nfrom nanobricks.security import EncryptionBrick\n\n# Encrypt specific fields\nencrypted = EncryptionBrick(\n    UserDataStore(),\n    fields_to_encrypt=[\"password\", \"ssn\", \"credit_card\"],\n    encrypt_output=True,\n)\n\n# Sensitive fields are automatically encrypted\nresult = await encrypted.invoke({\n    \"username\": \"alice\",\n    \"password\": \"secret123\",  # Will be encrypted\n    \"email\": \"alice@example.com\",  # Not encrypted\n})\n\n\n\nAudit Logging\nTrack all operations for compliance and security:\n\nfrom nanobricks.security import AuditLogger\n\n# Log all operations\naudited = AuditLogger(\n    PaymentService(),\n    log_input=True,\n    log_output=False,  # Don't log sensitive output\n    hash_sensitive_data=True,\n)\n\n# All operations are logged\nawait audited.invoke(payment_request)\n\n# Query audit log\nentries = audited.get_audit_log(\n    user_id=\"alice\",\n    start_time=yesterday,\n    success_only=False,\n)\n\n\n\nLayered Security\nApply multiple security layers at once:\n\nfrom nanobricks.security import secure_nanobrick\n\n# Apply all security features\nsecure_service = secure_nanobrick(\n    CriticalService(),\n    sanitize=True,\n    rate_limit=50,\n    permissions={Permission.EXECUTE},\n    encrypt=True,\n    audit=True,\n)",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Nanobricks for Humans"
    ]
  },
  {
    "objectID": "human.html#production-features",
    "href": "human.html#production-features",
    "title": "Nanobricks for Humans",
    "section": "Production Features",
    "text": "Production Features\nMake your nanobricks production-ready with reliability patterns:\n\nCircuit Breakers\nPrevent cascading failures with automatic circuit breaking:\n\nfrom nanobricks.production import CircuitBreaker\n\n# Protect unreliable services\nbreaker = CircuitBreaker(\n    ExternalApiService(),\n    failure_threshold=5,      # Open after 5 failures\n    success_threshold=2,      # Close after 2 successes\n    timeout_seconds=60,       # Reset attempt after 60s\n    fallback=lambda x, d: cached_response,  # Fallback when open\n)\n\n# Circuit opens automatically on failures\nresult = await breaker.invoke(request)\n\n# Check circuit status\nprint(f\"Circuit state: {breaker.state}\")  # CLOSED, OPEN, or HALF_OPEN\nprint(f\"Stats: {breaker.stats}\")\n\n\n\nBulkhead Isolation\nIsolate resources to prevent total system failure:\n\nfrom nanobricks.production import Bulkhead\n\n# Limit concurrent executions\nbulkhead = Bulkhead(\n    ResourceIntensiveService(),\n    max_concurrent=5,        # Max 5 concurrent\n    max_queue_size=10,       # Queue up to 10 more\n    timeout_seconds=30,      # Timeout waiting for slot\n)\n\n# Requests beyond limits are rejected\ntry:\n    results = await asyncio.gather(*[\n        bulkhead.invoke(req) for req in many_requests\n    ])\nexcept RuntimeError as e:\n    print(f\"Bulkhead full: {e}\")\n\n# Monitor bulkhead status\nprint(f\"Active: {bulkhead.active_count}\")\nprint(f\"Stats: {bulkhead.stats}\")\n\n\n\nHealth Checks\nMonitor service health automatically:\n\nfrom nanobricks.production import HealthCheck\n\n# Add health monitoring\nmonitored = HealthCheck(\n    CriticalService(),\n    check_interval_seconds=30,\n    failure_threshold=3,\n)\n\n# Start background health checks\nawait monitored.start_health_checks()\n\n# Use the service normally\nresult = await monitored.invoke(request)\n\n# Check health status\nhealth = await monitored.check_health()\nprint(f\"Status: {health.status}\")  # HEALTHY, DEGRADED, or UNHEALTHY\nprint(f\"Details: {health.details}\")\n\n# Custom health check logic\ndef custom_check():\n    # Check database, disk space, etc.\n    if database_connected and disk_space_ok:\n        return HealthCheckResult(HealthStatus.HEALTHY)\n    else:\n        return HealthCheckResult(HealthStatus.UNHEALTHY)\n\ncustom_monitored = HealthCheck(service, custom_check=custom_check)\n\n\n\nGraceful Shutdown\nClean shutdown with proper resource cleanup:\n\nfrom nanobricks.production import GracefulShutdown\n\n# Create shutdown manager\nshutdown = GracefulShutdown(timeout_seconds=30)\n\n# Register resources to manage\nshutdown.register_brick(database_service)\nshutdown.register_brick(cache_service)\n\n# Register background tasks\nshutdown.register_task(monitoring_task)\nshutdown.register_task(cleanup_task)\n\n# Custom shutdown handlers\nasync def save_state():\n    await state_store.save()\n    print(\"State saved\")\n\nshutdown.register_handler(save_state)\n\n# Install signal handlers (SIGTERM, SIGINT)\nshutdown.install_signal_handlers()\n\n# Wait for shutdown signal\nawait shutdown.wait_for_shutdown()\n\n# Or trigger manually\nawait shutdown.shutdown()\n\n\n\nAll-in-One Production Setup\nApply all production features at once:\n\nfrom nanobricks.production import with_production_features\n\n# Make any brick production-ready\nproduction_service = with_production_features(\n    MyService(),\n    circuit_breaker=True,\n    bulkhead=10,           # Max 10 concurrent\n    health_check=True,\n    failure_threshold=5,   # Circuit breaker setting\n    timeout_seconds=60,    # Circuit reset timeout\n)\n\n# Use normally - all protection is transparent\nresult = await production_service.invoke(request)\n\n# Features work together:\n# - Circuit breaker prevents cascading failures\n# - Bulkhead limits resource usage\n# - Health checks monitor service status",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Nanobricks for Humans"
    ]
  },
  {
    "objectID": "human.html#performance-optimization",
    "href": "human.html#performance-optimization",
    "title": "Nanobricks for Humans",
    "section": "Performance Optimization",
    "text": "Performance Optimization\nOptimize your nanobricks for maximum performance:\n\nCaching\nCache expensive operations with TTL and LRU eviction:\n\nfrom nanobricks.performance import with_cache\n\n# Add caching to any brick\nfibonacci = FibonacciBrick()\ncached_fib = with_cache(\n    fibonacci,\n    max_size=1000,      # Max cache entries\n    ttl=300,            # 5 minute TTL\n)\n\n# First call computes\nresult1 = await cached_fib.invoke(20)  # Takes time\n\n# Second call is instant\nresult2 = await cached_fib.invoke(20)  # From cache!\n\n# Check cache statistics\nstats = cached_fib.cache_info()\nprint(f\"Cache hits: {stats['hits']}\")\nprint(f\"Cache size: {stats['size']}\")\n\n# Clear cache if needed\ncached_fib.clear_cache()\n\n\n\nConnection Pooling\nEfficiently manage external resources:\n\nfrom nanobricks.performance import ConnectionPool\n\n# Create connection pool\nasync def create_db_connection():\n    # Simulate expensive connection\n    await asyncio.sleep(0.1)\n    return {\"id\": random.randint(1000, 9999), \"connected\": True}\n\npool = ConnectionPool(\n    create_db_connection,\n    min_size=2,         # Keep 2 connections ready\n    max_size=10,        # Max 10 connections\n    acquire_timeout=5.0,\n)\n\n# Use connections efficiently\nasync with pool.acquire() as conn:\n    # Connection is automatically returned to pool\n    result = await db_brick.invoke(query, deps={\"connection\": conn})\n\n# Monitor pool health\nstats = pool.get_stats()\nprint(f\"Active connections: {stats['in_use']}\")\nprint(f\"Available: {stats['available']}\")\n\n# Cleanup\nawait pool.close()\n\n\n\nPipeline Fusion\nOptimize pipeline execution by fusing operations:\n\nfrom nanobricks.performance import FusedPipeline\n\n# Regular pipeline\nupper = TransformBrick(str.upper)\nstrip = TransformBrick(str.strip)\nreverse = TransformBrick(lambda x: x[::-1])\n\n# Create fused pipeline for better performance\nfused = FusedPipeline([upper, strip, reverse])\n\n# Benchmark comparison\nimport time\n\n# Regular execution\nstart = time.time()\nfor _ in range(10000):\n    result = await upper.invoke(\"  hello  \")\n    result = await strip.invoke(result)\n    result = await reverse.invoke(result)\nregular_time = time.time() - start\n\n# Fused execution - reduces async overhead\nstart = time.time()\nfor _ in range(10000):\n    result = await fused.invoke(\"  hello  \")\nfused_time = time.time() - start\n\nprint(f\"Speedup: {regular_time / fused_time:.2f}x\")\n\n\n\nPerformance Profiling\nProfile your bricks to identify bottlenecks:\n\nfrom nanobricks.performance import ProfilerBrick\n\n# Wrap any brick with profiling\nprofiled = ProfilerBrick(\n    SlowService(),\n    enable_trace=True,\n    log_slow_calls=100.0,  # Log calls &gt; 100ms\n)\n\n# Use normally\nawait profiled.invoke(request)\n\n# Get performance metrics\nmetrics = profiled.get_metrics()\nprint(f\"Average time: {metrics.avg_time_ms:.2f}ms\")\nprint(f\"Min/Max: {metrics.min_time_ms:.2f}ms / {metrics.max_time_ms:.2f}ms\")\nprint(f\"Total calls: {metrics.total_calls}\")\n\n# Get detailed traces\ntraces = profiled.get_traces()\nfor trace in traces:\n    print(f\"{trace['brick']}: {trace['duration_ms']:.2f}ms\")\n\n\n\nBenchmarking\nAccurately benchmark your nanobricks:\n\nfrom nanobricks.performance import Benchmark\n\n# Create benchmark\nbench = Benchmark(\"data_processing\")\n\n# Measure performance\nresult = await bench.measure(\n    lambda: processor.invoke(test_data),\n    iterations=1000,\n    warmup=10,\n)\n\nprint(f\"Operations/sec: {result.ops_per_sec:.2f}\")\nprint(f\"Average latency: {result.avg_time * 1000:.2f}ms\")\nprint(f\"Min/Max: {result.min_time * 1000:.2f}ms / {result.max_time * 1000:.2f}ms\")\n\n# Compare optimizations\noriginal_result = await bench.measure(original_processor.invoke)\noptimized_result = await bench.measure(optimized_processor.invoke)\n\ncomparison = bench.compare(optimized_result)\nprint(f\"Speedup: {comparison['speedup']:.2f}x\")\n\n\n\nSystem Monitoring\nMonitor system resources:\n\nfrom nanobricks.performance import get_system_metrics\n\n# Get current system state\nmetrics = get_system_metrics()\n\nprint(f\"CPU Usage: {metrics['cpu']['percent']}%\")\nprint(f\"Memory: {metrics['memory']['percent']:.1f}% used\")\nprint(f\"Disk I/O: {metrics['disk_io'].get('read_bytes', 0) / 1e9:.2f} GB read\")\nprint(f\"Network: {metrics['network_io'].get('bytes_sent', 0) / 1e6:.2f} MB sent\")\n\n# Use for adaptive behavior\nif metrics['cpu']['percent'] &gt; 80:\n    # Switch to low-resource mode\n    processor.set_batch_size(10)\n\n\n\nAll-in-One Optimization\nApply multiple optimizations at once:\n\nfrom nanobricks.performance import with_performance_optimizations\n\n# Apply all optimizations\noptimized = with_performance_optimizations(\n    DataProcessor(),\n    cache=True,           # Enable caching\n    cache_size=1000,      # Cache size\n    cache_ttl=300,        # 5 minute TTL\n    profile=True,         # Enable profiling\n    profile_trace=True,   # Store traces\n    log_slow_calls=50.0,  # Log &gt; 50ms calls\n)\n\n# Use normally - optimizations are transparent\nresult = await optimized.invoke(data)\n\n# Check performance\nmetrics = optimized.get_metrics()\nprint(f\"Cache hit rate: {metrics.cache_hits / metrics.total_calls:.1%}\")",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Nanobricks for Humans"
    ]
  },
  {
    "objectID": "human.html#coming-next",
    "href": "human.html#coming-next",
    "title": "Nanobricks for Humans",
    "section": "Coming Next",
    "text": "Coming Next\n✅ Configuration system (TOML) - DONE!\n✅ Skill framework - DONE!\n✅ First built-in skills (logging, API, CLI) - DONE!\n✅ Deployment skills (Docker, Kubernetes) - DONE!\n✅ Performance optimizations (caching, batching, fusion) - DONE!\n✅ Additional transformers (CSV, text, type conversion) - DONE!\n✅ Project scaffolding & documentation generation - DONE!\n✅ Developer experience tools (debugger, visualizer, profiler) - DONE!\n✅ AI Integration (MCP, agents, adaptive behavior) - DONE!\n✅ Security features (sanitization, rate limiting, permissions, encryption) - DONE!\n✅ Production features (circuit breakers, bulkheads, health checks) - DONE!\n✅ Performance profiling and optimization - DONE!\n✅ Package registry and discovery - DONE!",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Nanobricks for Humans"
    ]
  },
  {
    "objectID": "human.html#package-registry",
    "href": "human.html#package-registry",
    "title": "Nanobricks for Humans",
    "section": "Package Registry",
    "text": "Package Registry\nShare and discover nanobricks with the built-in package registry:\n\nCreating Packages\nPackage your nanobricks for distribution:\n\nfrom nanobricks import create_package_from_brick\n\n# Create a package from your brick\nprocessor = TextProcessor()\npackage = create_package_from_brick(\n    processor,\n    name=\"text-processor\",\n    version=\"1.0.0\",\n    author=\"Your Name\",\n    description=\"Powerful text processing nanobrick\",\n    keywords=[\"text\", \"nlp\", \"processing\"],\n    homepage=\"https://github.com/yourname/text-processor\",\n)\n\n# Save as archive\narchive = package.to_archive()\nwith open(\"text-processor-1.0.0.nbp\", \"wb\") as f:\n    f.write(archive)\n\nprint(f\"Package created: {len(archive) / 1024:.2f} KB\")\n\n\n\nPublishing Packages\nShare your nanobricks with the community:\n\nfrom nanobricks import get_registry\n\n# Get registry client\nregistry = get_registry()\n\n# Publish your package\nsuccess = await registry.publish(package, api_key=\"your-api-key\")\nif success:\n    print(f\"✓ Published {package.metadata.name} v{package.metadata.version}\")\n\n\n\nSearching Packages\nFind nanobricks for your needs:\n\n# Search by keyword\nresults = await registry.search(\"validation\", limit=10)\nfor result in results:\n    print(f\"{result.name} v{result.version}: {result.description}\")\n    print(f\"  Downloads: {result.downloads}\")\n\n# Search by author\nresults = await registry.search(\"\", author=\"Alice Developer\")\n\n# Get detailed info\ninfo = await registry.info(\"data-validator\")\nprint(f\"Latest version: {info.latest_version}\")\nprint(f\"Total downloads: {info.total_downloads}\")\n\n\n\nInstalling Packages\nInstall nanobricks with version management:\n\n# Install latest stable version\npackage = await registry.install(\"data-validator\")\n\n# Install specific version\npackage = await registry.install(\"data-validator\", version_spec=\"==2.1.0\")\n\n# Install with version constraints\npackage = await registry.install(\n    \"data-validator\",\n    version_spec=\"^2.0.0\",  # Compatible with 2.x.x\n)\n\n# List installed packages\ninstalled = await registry.list_installed()\nfor pkg in installed:\n    print(f\"{pkg.metadata.name} v{pkg.metadata.version}\")\n\n\n\nVersion Management\nSemantic versioning with constraints:\n\nfrom nanobricks.registry import Version, VersionRange\n\n# Parse versions\nv1 = Version.parse(\"1.2.3\")\nv2 = Version.parse(\"2.0.0-beta.1\")\n\n# Compare versions\nprint(v1 &lt; v2)  # True\n\n# Version ranges\nrange = VersionRange.parse(\"^1.0.0\")  # Compatible with 1.x.x\nprint(range.contains(v1))  # True\n\n# Resolve dependencies\nfrom nanobricks.registry import resolve_dependencies\n\ndependencies = {\n    \"nanobrick-core\": \"^1.0.0\",\n    \"nanobrick-utils\": \"&gt;=1.0.0,&lt;2.0.0\",\n    \"nanobrick-types\": \"~1.2.0\",  # ~1.2.x\n}\n\navailable = {\n    \"nanobrick-core\": [Version.parse(\"1.0.0\"), Version.parse(\"1.5.0\")],\n    \"nanobrick-utils\": [Version.parse(\"1.2.0\"), Version.parse(\"2.0.0\")],\n    \"nanobrick-types\": [Version.parse(\"1.2.3\"), Version.parse(\"1.3.0\")],\n}\n\nresolved = resolve_dependencies(dependencies, available)\n# Results: core=1.5.0, utils=1.2.0, types=1.2.3\n\n\n\nPackage Metadata\nRich metadata for discoverability:\n\nfrom nanobricks.registry import PackageMetadata\n\nmetadata = PackageMetadata(\n    name=\"my-nanobrick\",\n    version=\"1.0.0\",\n    description=\"Does amazing things\",\n    author=\"Your Name\",\n    email=\"you@example.com\",\n    license=\"MIT\",\n    homepage=\"https://github.com/you/my-nanobrick\",\n    keywords=[\"awesome\", \"nanobrick\"],\n    dependencies={\n        \"nanobricks\": \"&gt;=1.0.0\",\n        \"requests\": \"^2.0.0\",\n    },\n    # Nanobrick-specific metadata\n    skills=[\"api\", \"cli\"],\n    input_type=\"dict[str, Any]\",\n    output_type=\"ProcessedData\",\n)\n\n🔜 Community marketplace and visual builder\n\nLast Updated: Registry Ready - Share and Discover Amazing Nanobricks!",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Nanobricks for Humans"
    ]
  },
  {
    "objectID": "human-overview.html",
    "href": "human-overview.html",
    "title": "Nanobricks Documentation",
    "section": "",
    "text": "Nanobricks is a Python SDK that lets you build production-ready systems from small, composable components. Think of it as “Lego blocks for Python developers” - simple pieces that combine to create anything."
  },
  {
    "objectID": "human-overview.html#welcome-to-nanobricks",
    "href": "human-overview.html#welcome-to-nanobricks",
    "title": "Nanobricks Documentation",
    "section": "",
    "text": "Nanobricks is a Python SDK that lets you build production-ready systems from small, composable components. Think of it as “Lego blocks for Python developers” - simple pieces that combine to create anything."
  },
  {
    "objectID": "human-overview.html#what-makes-nanobricks-different",
    "href": "human-overview.html#what-makes-nanobricks-different",
    "title": "Nanobricks Documentation",
    "section": "What Makes Nanobricks Different?",
    "text": "What Makes Nanobricks Different?\n\n🧱 Atomic Components\nEvery nanobrick does ONE thing well. No bloated classes, no complex inheritance - just simple, focused components.\n\n\n🔗 Universal Composition\nAll nanobricks share the same interface. Combine them with the pipe operator (|) to build complex systems from simple parts.\n\n\n🚀 Progressive Enhancement\nStart simple, add capabilities as needed. Skills like logging, API endpoints, and monitoring can be added without changing your code.\n\n\n🏭 Production Ready\nBuilt-in support for deployment (Docker, Kubernetes), observability, security, and performance optimization.\n\n\n🔍 Type Safe\nFull type inference through pipelines. Your IDE knows the types at every step."
  },
  {
    "objectID": "human-overview.html#quick-example",
    "href": "human-overview.html#quick-example",
    "title": "Nanobricks Documentation",
    "section": "Quick Example",
    "text": "Quick Example\n\nfrom nanobricks import NanobrickSimple, skill\n\n# Define a nanobrick\n@skill(\"logging\")\n@skill(\"api\", port=8080)\nclass DataProcessor(NanobrickSimple[dict, dict]):\n    async def invoke(self, data: dict, *, deps=None) -&gt; dict:\n        return {\"processed\": True, \"items\": len(data)}\n\n# Compose a pipeline\npipeline = (\n    ValidateData()\n    | DataProcessor()\n    | SaveToDatabase()\n).with_skill(\"retry\", max_attempts=3)\n\n# Use it\nresult = await pipeline.invoke({\"users\": [1, 2, 3]})"
  },
  {
    "objectID": "human-overview.html#documentation-guide",
    "href": "human-overview.html#documentation-guide",
    "title": "Nanobricks Documentation",
    "section": "Documentation Guide",
    "text": "Documentation Guide\n\n🚀 Quickstart\n10 minutes to your first nanobrick - Installation - Creating your first brick - Basic composition - Adding skills\n\n\n📚 Tutorial\nComplete guide to nanobricks - Core concepts explained - Step-by-step examples - Testing strategies - Configuration system\n\n\n🏗️ SDK Guide\nBuilding production systems - Architecture patterns (Repository, Service Layer, CQRS) - Real-world examples (REST APIs, Data Pipelines, Microservices) - Best practices - Deployment strategies\n\n\n🎨 Design Patterns\nAdvanced composition patterns - Branching and conditionals - Parallel execution - Error handling - Event-driven architecture\n\n\n🏭 Production Guide\nDeploy and scale with confidence - Security features - Performance optimization - Monitoring and observability - High availability patterns\n\n\n📖 API Reference\nComplete API documentation - All classes and methods - Built-in components - Skills reference - Type definitions"
  },
  {
    "objectID": "human-overview.html#the-nanobricks-philosophy",
    "href": "human-overview.html#the-nanobricks-philosophy",
    "title": "Nanobricks Documentation",
    "section": "The Nanobricks Philosophy",
    "text": "The Nanobricks Philosophy\n\n1. Simplicity First\nIf a component does more than one thing, it should be two components.\n\n\n2. Composition Over Configuration\nBuild behavior by combining bricks, not by tweaking parameters.\n\n\n3. Explicit Over Implicit\nNo hidden magic. You can see exactly what each brick does.\n\n\n4. Type Safety Throughout\nStrong typing from input to output, through entire pipelines.\n\n\n5. Progressive Enhancement\nStart with basic functionality, add production features as needed."
  },
  {
    "objectID": "human-overview.html#use-cases",
    "href": "human-overview.html#use-cases",
    "title": "Nanobricks Documentation",
    "section": "Use Cases",
    "text": "Use Cases\n\nBuild a REST API\n\napi = create_rest_api(\n    endpoints=[UserEndpoints(), OrderEndpoints()],\n    middleware=[RateLimiter(), Authenticator()],\n    skills=[\"logging\", \"monitoring\", \"docker\"]\n)\n\n\n\nCreate a Data Pipeline\n\npipeline = (\n    S3Loader(bucket=\"raw-data\")\n    | CSVParser()\n    | DataValidator(schema)\n    | Transformer()\n    | DatabaseWriter()\n).with_skill(\"monitoring\")\n\n\n\nDesign a Microservice\n\nservice = create_microservice(\n    name=\"payment-processor\",\n    handlers={\n        \"POST /payments\": ProcessPayment(),\n        \"payment.requested\": PaymentHandler()\n    },\n    skills=[\"api\", \"kubernetes\", \"observability\"]\n)"
  },
  {
    "objectID": "human-overview.html#getting-help",
    "href": "human-overview.html#getting-help",
    "title": "Nanobricks Documentation",
    "section": "Getting Help",
    "text": "Getting Help\n\n💬 Community\n\nGitHub Discussions\nDiscord Server\nStack Overflow\n\n\n\n📚 Resources\n\nExample Repository\nVideo Tutorials\nBlog\n\n\n\n🐛 Issues\n\nBug Reports\nFeature Requests"
  },
  {
    "objectID": "human-overview.html#ready-to-build",
    "href": "human-overview.html#ready-to-build",
    "title": "Nanobricks Documentation",
    "section": "Ready to Build?",
    "text": "Ready to Build?\nStart with the Quickstart Guide and build your first nanobrick in 10 minutes!\n\nNanobricks - Build it right, build it once."
  },
  {
    "objectID": "design-philosophy.html",
    "href": "design-philosophy.html",
    "title": "Design Philosophy",
    "section": "",
    "text": "Nanobricks are the code equivalent of “antifragile nanobots” — atomic, self-sufficient components that gain strength from stress and compose organically into complex systems.\n“Both super complex and super simple” - this guides our design:\n\nSimple rules: Minimal nanobrick interface\nComplex emergence: Rich behaviors from composition\nOrganic growth: Start small, evolve naturally\nAntifragility: Learn and improve from stress",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Design Philosophy"
    ]
  },
  {
    "objectID": "design-philosophy.html#the-nature-metaphor",
    "href": "design-philosophy.html#the-nature-metaphor",
    "title": "Design Philosophy",
    "section": "",
    "text": "Nanobricks are the code equivalent of “antifragile nanobots” — atomic, self-sufficient components that gain strength from stress and compose organically into complex systems.\n“Both super complex and super simple” - this guides our design:\n\nSimple rules: Minimal nanobrick interface\nComplex emergence: Rich behaviors from composition\nOrganic growth: Start small, evolve naturally\nAntifragility: Learn and improve from stress",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Design Philosophy"
    ]
  },
  {
    "objectID": "design-philosophy.html#the-five-pillars",
    "href": "design-philosophy.html#the-five-pillars",
    "title": "Design Philosophy",
    "section": "The Five Pillars",
    "text": "The Five Pillars\n\n1. Simple\n\nDesigned for clarity and straightforward implementation\nEasy for both humans and AIs to reason about\nSingle responsibility principle at the atomic level\nMinimal cognitive overhead\n\n\n\n2. Standardized\n\nConsistent interfaces — the “Lego Connector Mechanism” for code\nPredictable behavior patterns\nUniversal protocols for:\n\nInput/Output contracts\nConfiguration management\nError handling\nLifecycle hooks\n\n\n\n\n3. Composable\n\nSeamless integration patterns\nPipeline-ready (can be chained/piped together)\nSupports multiple composition patterns:\n\nSequential (A → B → C)\nParallel (A + B + C)\nNested (A(B(C)))\nHybrid workflows\n\n\n\n\n4. Batteries Included\nEach nanobrick ships with modular, self-contained interfaces:\n\nAPI Layer (FastAPI) — RESTful endpoints auto-generated\nCLI Layer (Typer) — Command-line interface out of the box\nFrontend Layer (Streamlit) — UI components (app/page/tab/subtab)\nData Layer (SQLModel) — Database interaction when needed\n\n\n\n5. Scaffoldable\n\nInstant end-to-end functionality\nRails-inspired convention over configuration\nTask-powered automation\nAI-friendly patterns for guided implementation\nProgressive enhancement model",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Design Philosophy"
    ]
  },
  {
    "objectID": "design-philosophy.html#key-design-patterns",
    "href": "design-philosophy.html#key-design-patterns",
    "title": "Design Philosophy",
    "section": "Key Design Patterns",
    "text": "Key Design Patterns\n\nFrom Successful Frameworks\nBased on analysis of successful frameworks like LangChain and PydanticAI, nanobricks leverages:\n\nRunnable Interface Pattern\n\nStandardized methods: invoke(), batch(), stream()\nEnables uniform interaction across all components\n\nComposition Pattern with Pipe Operator\n\nUse | operator for intuitive chaining\nDeclarative pipeline construction\n\nDependency Injection\n\nContext-based dependency passing\nEnhances testability and flexibility\n\nDecorator Pattern\n\nClean, pythonic API for component configuration\nSkills and capabilities as decorators\n\nGeneric Programming\n\nType-safe interfaces using Python generics\nCompile-time type checking",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Design Philosophy"
    ]
  },
  {
    "objectID": "design-philosophy.html#antifragility-mechanisms",
    "href": "design-philosophy.html#antifragility-mechanisms",
    "title": "Design Philosophy",
    "section": "Antifragility Mechanisms",
    "text": "Antifragility Mechanisms\n\n1. Self-Healing\n\nAutomatic retry with exponential backoff\nGraceful degradation\nCircuit breaker patterns\n\n\n\n2. Adaptation\n\nRuntime configuration updates\nDynamic scaling based on load\nLearning from failures\n\n\n\n3. Evolution\n\nVersion migration support\nBackward compatibility guarantees\nProgressive enhancement",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Design Philosophy"
    ]
  },
  {
    "objectID": "design-philosophy.html#type-safety-strategy",
    "href": "design-philosophy.html#type-safety-strategy",
    "title": "Design Philosophy",
    "section": "Type Safety Strategy",
    "text": "Type Safety Strategy\n\nThe Challenge\nWhen composing nanobricks with pipes (A | B | C), we need to ensure type safety where the output of A matches the input of B.\n\n\nThe Solution\nA hybrid approach combining:\n\nStatic Analysis: Python generics + mypy for development-time safety\nRuntime Validation: Beartype for production runtime validation\n\nProtocol Enforcement: ABC + Protocol hybrid for interface guarantees\n\nThis provides the best of both worlds without over-engineering.",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Design Philosophy"
    ]
  },
  {
    "objectID": "design-philosophy.html#ai-integration-philosophy",
    "href": "design-philosophy.html#ai-integration-philosophy",
    "title": "Design Philosophy",
    "section": "AI Integration Philosophy",
    "text": "AI Integration Philosophy\n\nMulti-Protocol Strategy\nNanobricks support multiple AI protocols as optional skills:\n\nMCP (Model Context Protocol) - Primary\n\n“USB-C for AI” - standardized tool/resource exposure\nClient-server architecture perfect for nanobricks\nVendor-agnostic LLM support\n\nA2A (Agent-to-Agent) - Secondary\n\nFor nanobrick-to-nanobrick communication\nPreserves agent opacity (privacy)\nEnables multi-agent workflows\n\nAG-UI (Agent User Interaction) - UI Layer\n\nEvent-driven protocol\nReal-time state streaming\nHuman-in-the-loop support\n\n\nThis approach keeps AI capabilities optional while providing a clear path for enhancement.",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Design Philosophy"
    ]
  },
  {
    "objectID": "design-philosophy.html#implementation-philosophy",
    "href": "design-philosophy.html#implementation-philosophy",
    "title": "Design Philosophy",
    "section": "Implementation Philosophy",
    "text": "Implementation Philosophy\n\nStart Simple, Grow Organically\n\nDefine minimal viable interfaces\nBuild essential composition operators\nAdd skills progressively\nLet patterns emerge from usage\n\n\n\nConvention Over Configuration\n\nSensible defaults for everything\nOverride only when needed\nClear naming conventions\nPredictable project structure\n\n\n\nDeveloper Experience First\n\nFast feedback loops\nClear error messages\nExcellent documentation\nAI-friendly patterns\n\nThis philosophy prevents over-engineering while enabling powerful capabilities.",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Design Philosophy"
    ]
  },
  {
    "objectID": "multi-project-development.html",
    "href": "multi-project-development.html",
    "title": "Multi-Project Development",
    "section": "",
    "text": "When developing a project that uses Nanobricks, you often need to:\n\nMake changes to Nanobricks itself\nTest those changes in your project immediately\nAccess both codebases for reference\n\nOur task-based linking system makes this seamless by creating symbolic links between projects.",
    "crumbs": [
      "Home",
      "Building Systems",
      "Multi-Project Development"
    ]
  },
  {
    "objectID": "multi-project-development.html#overview",
    "href": "multi-project-development.html#overview",
    "title": "Multi-Project Development",
    "section": "",
    "text": "When developing a project that uses Nanobricks, you often need to:\n\nMake changes to Nanobricks itself\nTest those changes in your project immediately\nAccess both codebases for reference\n\nOur task-based linking system makes this seamless by creating symbolic links between projects.",
    "crumbs": [
      "Home",
      "Building Systems",
      "Multi-Project Development"
    ]
  },
  {
    "objectID": "multi-project-development.html#setting-up-links",
    "href": "multi-project-development.html#setting-up-links",
    "title": "Multi-Project Development",
    "section": "Setting Up Links",
    "text": "Setting Up Links\n\nFrom Nanobricks Directory\nLink a project that uses Nanobricks:\n# Link your project\ntask dev:link:project PATH=/path/to/your/project\n\n# Or with relative path\ntask dev:link:project PATH=../nano-scorm\n\n# List all linked projects\ntask dev:list:linked\n\n# Remove a link\ntask dev:unlink:project NAME=nano-scorm\n\n\nFrom Your Project Directory\nProjects created with dist:new:project:uv already include linking tasks:\n# Link back to Nanobricks (path already configured)\ntask link:nanobricks\n\n# Update after Nanobricks changes\ntask sync:nanobricks\n\n# Remove the link\ntask unlink:nanobricks",
    "crumbs": [
      "Home",
      "Building Systems",
      "Multi-Project Development"
    ]
  },
  {
    "objectID": "multi-project-development.html#workflow-example",
    "href": "multi-project-development.html#workflow-example",
    "title": "Multi-Project Development",
    "section": "Workflow Example",
    "text": "Workflow Example\n\n1. Create Your Project\ntask dist:new:project:uv NAME=my-app DIR=/path/to/workspace\n\n\n2. Link the Projects\nFrom Nanobricks directory:\ntask dev:link:project PATH=/path/to/workspace/my-app\n\n\n3. Work in Both Directories\n\nIn Nanobricks: Access your project at linked-projects/my-app/\nIn your project: Access Nanobricks at linked-nanobricks/\n\n\n\n4. Sync Changes\nAfter making Nanobricks changes, from your project directory:\ntask sync:nanobricks",
    "crumbs": [
      "Home",
      "Building Systems",
      "Multi-Project Development"
    ]
  },
  {
    "objectID": "multi-project-development.html#benefits",
    "href": "multi-project-development.html#benefits",
    "title": "Multi-Project Development",
    "section": "Benefits",
    "text": "Benefits\n\n\n\n\n\n\nKey Advantages\n\n\n\n\nSingle IDE Session: Work on both codebases without switching directories\nImmediate Testing: Changes to Nanobricks are immediately available\nEasy Navigation: Browse both codebases seamlessly\nClean Git: Links are gitignored, keeping repositories clean",
    "crumbs": [
      "Home",
      "Building Systems",
      "Multi-Project Development"
    ]
  },
  {
    "objectID": "multi-project-development.html#directory-structure",
    "href": "multi-project-development.html#directory-structure",
    "title": "Multi-Project Development",
    "section": "Directory Structure",
    "text": "Directory Structure\nAfter linking, you’ll see:\n\n\nDirectory Structure\n\nnanobricks/\n├── src/nanobricks/        # Framework source\n├── linked-projects/       # Your linked projects\n│   └── my-app -&gt; /path/to/my-app\n└── ...\n\nmy-app/\n├── src/my_app/           # Your app source\n├── linked-nanobricks -&gt; /path/to/nanobricks\n└── ...",
    "crumbs": [
      "Home",
      "Building Systems",
      "Multi-Project Development"
    ]
  },
  {
    "objectID": "multi-project-development.html#tips-and-best-practices",
    "href": "multi-project-development.html#tips-and-best-practices",
    "title": "Multi-Project Development",
    "section": "Tips and Best Practices",
    "text": "Tips and Best Practices\n\nUse Relative Paths\nWhen possible, use relative paths for portability:\ntask dev:link:project PATH=../my-app\n\n\nMultiple Projects\nLink multiple projects to test Nanobricks changes across different use cases:\ntask dev:link:project PATH=../project-a\ntask dev:link:project PATH=../project-b\ntask dev:list:linked\n\n\nIDE Integration\nMost IDEs will follow symbolic links, giving you full code intelligence across both codebases.\n\n\nVersion Control\nBoth linked-projects/ and linked-nanobricks are automatically added to .gitignore.",
    "crumbs": [
      "Home",
      "Building Systems",
      "Multi-Project Development"
    ]
  },
  {
    "objectID": "multi-project-development.html#troubleshooting",
    "href": "multi-project-development.html#troubleshooting",
    "title": "Multi-Project Development",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\n\n\n\n\n\nCommon Issues\n\n\n\n\n“Project directory not found”\n\nUse full paths or paths relative to the Nanobricks directory\nTilde (~) expansion might not work - use full paths instead\n\n\n\n“Link already exists”\n\nThe task will automatically remove old links and create new ones\nThis is normal and expected behavior\n\n\n\nChanges not reflected\n\nRun task sync:nanobricks from your project after Nanobricks changes\nThis runs uv sync to update the installation",
    "crumbs": [
      "Home",
      "Building Systems",
      "Multi-Project Development"
    ]
  },
  {
    "objectID": "multi-project-development.html#example-developing-a-feature",
    "href": "multi-project-development.html#example-developing-a-feature",
    "title": "Multi-Project Development",
    "section": "Example: Developing a Feature",
    "text": "Example: Developing a Feature\nHere’s a complete workflow for developing a new Nanobricks feature:\n# 1. Create test project\ntask dist:new:project:uv NAME=test-feature DIR=../\n\n# 2. Link projects (from Nanobricks)\ntask dev:link:project PATH=../test-feature\n\n# 3. Develop new feature in Nanobricks\n# Edit files in src/nanobricks/...\n\n# 4. Test in your project\ncd linked-projects/test-feature\ntask sync:nanobricks\ntask dev:test\n\n# 5. Iterate until complete",
    "crumbs": [
      "Home",
      "Building Systems",
      "Multi-Project Development"
    ]
  },
  {
    "objectID": "multi-project-development.html#integration-with-claude-code",
    "href": "multi-project-development.html#integration-with-claude-code",
    "title": "Multi-Project Development",
    "section": "Integration with Claude Code",
    "text": "Integration with Claude Code\nThis linking approach works particularly well with AI assistants like Claude Code that are limited to a single root directory. By using symbolic links, the AI can access and work with both codebases seamlessly.",
    "crumbs": [
      "Home",
      "Building Systems",
      "Multi-Project Development"
    ]
  },
  {
    "objectID": "multi-project-development.html#next-steps",
    "href": "multi-project-development.html#next-steps",
    "title": "Multi-Project Development",
    "section": "Next Steps",
    "text": "Next Steps\n\nLearn about Distribution & Deployment for packaging your projects\nExplore SDK Guide for building applications with Nanobricks\nCheck Production Examples for real-world patterns",
    "crumbs": [
      "Home",
      "Building Systems",
      "Multi-Project Development"
    ]
  },
  {
    "objectID": "task-system.html",
    "href": "task-system.html",
    "title": "Task System & Commands",
    "section": "",
    "text": "Nanobricks uses Task for all build automation and development workflows. We follow a consistent component:action[:target] naming structure that makes commands intuitive and discoverable.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Task System & Commands"
    ]
  },
  {
    "objectID": "task-system.html#overview",
    "href": "task-system.html#overview",
    "title": "Task System & Commands",
    "section": "",
    "text": "Nanobricks uses Task for all build automation and development workflows. We follow a consistent component:action[:target] naming structure that makes commands intuitive and discoverable.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Task System & Commands"
    ]
  },
  {
    "objectID": "task-system.html#naming-convention",
    "href": "task-system.html#naming-convention",
    "title": "Task System & Commands",
    "section": "Naming Convention",
    "text": "Naming Convention\nAll tasks follow this pattern:\ncomponent:action[:target]\nWhere: - component - The system/module being operated on (e.g., docs, dev, package) - action - The verb/operation (e.g., create, build, test, link) - target - Optional specific target (e.g., local, wheel, pypi, unit)\n\nBenefits\n\nIntuitive - “I want to work with X and do Y to it”\nDiscoverable - task --list groups related tasks naturally\nExtensible - Easy to add new actions or targets\nConsistent - One pattern throughout all Taskfiles",
    "crumbs": [
      "Home",
      "Getting Started",
      "Task System & Commands"
    ]
  },
  {
    "objectID": "task-system.html#common-tasks-reference",
    "href": "task-system.html#common-tasks-reference",
    "title": "Task System & Commands",
    "section": "Common Tasks Reference",
    "text": "Common Tasks Reference\n\nDocumentation Management\ntask docs:render      # Build Quarto documentation\ntask docs:preview     # Live preview with hot reload\ntask docs:publish     # Deploy to GitHub Pages\ntask docs:clean       # Remove generated docs\ntask docs:open        # Open in browser\n\n\nDevelopment Workflow\ntask dev:setup              # Initial environment setup\ntask dev:test               # Run all tests\ntask dev:test:unit          # Unit tests only\ntask dev:test:integration   # Integration tests only\ntask dev:test:coverage      # Tests with coverage report\ntask dev:lint               # Run linters (ruff + mypy)\ntask dev:format             # Format code\ntask dev:typecheck          # Strict type checking\ntask dev:all                # Run all checks\n\n\nProject Creation\n\n\n\n\n\n\nRecommended: Use uv\n\n\n\nThe project:create:uv task is the preferred method as it automatically sets up Python 3.13, creates a virtual environment, and installs dependencies.\n\n\n# Create new project with uv (recommended)\ntask dist:project:create:uv NAME=my-app DIR=~/projects\n\n# Manual project creation\ntask dist:project:create NAME=my-app DIR=~/projects\n\n\nMulti-Project Development\nFrom the Nanobricks directory:\ntask dev:project:link PATH=../my-app    # Link a project\ntask dev:project:list                   # List all linked projects\ntask dev:project:unlink NAME=my-app     # Remove a link\nFrom your project directory:\ntask link:nanobricks      # Link to Nanobricks source\ntask sync:nanobricks      # Update after Nanobricks changes\ntask unlink:nanobricks    # Remove the link\n\n\nBuilding & Distribution\ntask dist:package:build           # Build wheel and sdist\ntask dist:package:clean           # Clean build artifacts\ntask dist:package:verify          # Verify installation\n\n# Install in another project\ntask dist:package:install:local PROJECT_PATH=/path/to/project\ntask dist:package:install:wheel PROJECT_PATH=/path/to/project\n\n# Publishing\ntask dist:publish:test            # Validate package (dry run)\ntask dist:publish:pypi            # Publish to PyPI\n\n\nComponent Scaffolding\ntask nanobrick:create NAME=my_component    # Create new nanobrick\n\n\nVersion Management\ntask version:bump             # Bump patch version\ntask version:bump PART=minor  # Bump minor version\ntask version:bump PART=major  # Bump major version",
    "crumbs": [
      "Home",
      "Getting Started",
      "Task System & Commands"
    ]
  },
  {
    "objectID": "task-system.html#quick-aliases",
    "href": "task-system.html#quick-aliases",
    "title": "Task System & Commands",
    "section": "Quick Aliases",
    "text": "Quick Aliases\nFor convenience, common tasks have short aliases:\ntask test     # Alias for dev:test\ntask lint     # Alias for dev:lint\ntask format   # Alias for dev:format",
    "crumbs": [
      "Home",
      "Getting Started",
      "Task System & Commands"
    ]
  },
  {
    "objectID": "task-system.html#task-discovery",
    "href": "task-system.html#task-discovery",
    "title": "Task System & Commands",
    "section": "Task Discovery",
    "text": "Task Discovery\nExplore available tasks:\ntask --list           # Show all public tasks\ntask --list-all       # Include internal tasks\ntask --summary [task] # Show task description",
    "crumbs": [
      "Home",
      "Getting Started",
      "Task System & Commands"
    ]
  },
  {
    "objectID": "task-system.html#project-tasks",
    "href": "task-system.html#project-tasks",
    "title": "Task System & Commands",
    "section": "Project Tasks",
    "text": "Project Tasks\nWhen you create a new project with Nanobricks, it includes these tasks:\ntask dev:test         # Run project tests\ntask dev:lint         # Lint project code\ntask dev:format       # Format project code\ntask link:nanobricks  # Link to Nanobricks source\ntask sync:nanobricks  # Update Nanobricks dependency",
    "crumbs": [
      "Home",
      "Getting Started",
      "Task System & Commands"
    ]
  },
  {
    "objectID": "task-system.html#task-organization",
    "href": "task-system.html#task-organization",
    "title": "Task System & Commands",
    "section": "Task Organization",
    "text": "Task Organization\nTasks are organized across multiple Taskfiles:\n\nTaskfile.yml - Core development tasks\nTaskfile.dist.yml - Distribution and project creation\nTaskfile.dev.yml - Development utilities and helpers\n\nAll are included in the main Taskfile, so you access them with their namespace prefix:\ntask dev:test              # From main Taskfile\ntask dist:package:build    # From Taskfile.dist.yml\ntask dev:project:link      # From Taskfile.dev.yml",
    "crumbs": [
      "Home",
      "Getting Started",
      "Task System & Commands"
    ]
  },
  {
    "objectID": "task-system.html#advanced-features",
    "href": "task-system.html#advanced-features",
    "title": "Task System & Commands",
    "section": "Advanced Features",
    "text": "Advanced Features\n\nWatch Mode\nFor rapid development:\ntask dev:watch:test    # Auto-run tests on file changes\ntask dev:watch:docs    # Auto-rebuild docs (same as preview)\n\n\nQuick Checks\ntask dev:check:all     # Run all checks (lint, typecheck, test)\ntask dev:check:quick   # Quick lint check only\n\n\nLocal Repository\nCreate a local package repository:\ntask dist:repo:create:local\n# Then install with:\nuv pip install nanobricks --find-links file://~/.nanobricks-repo",
    "crumbs": [
      "Home",
      "Getting Started",
      "Task System & Commands"
    ]
  },
  {
    "objectID": "task-system.html#best-practices",
    "href": "task-system.html#best-practices",
    "title": "Task System & Commands",
    "section": "Best Practices",
    "text": "Best Practices\n\nUse uv for project creation - It handles Python version and dependencies automatically\nLink projects during development - Makes testing changes immediate\nRun checks before commits - Use task dev:check:all\nUse watch mode - For TDD workflow with task dev:watch:test",
    "crumbs": [
      "Home",
      "Getting Started",
      "Task System & Commands"
    ]
  },
  {
    "objectID": "task-system.html#extension-pattern",
    "href": "task-system.html#extension-pattern",
    "title": "Task System & Commands",
    "section": "Extension Pattern",
    "text": "Extension Pattern\nWhen adding new functionality, maintain the component:action pattern:\n# Good examples\ncache:clear:\ncache:warm:\ncache:invalidate:redis:\nbenchmark:run:\nbenchmark:compare:\n\n# Avoid these patterns\nclear-cache:\nwarm_cache:\ninvalidateRedisCache:\nrun-benchmarks:\nThis ensures consistency and discoverability as the project grows.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Task System & Commands"
    ]
  },
  {
    "objectID": "task-system.html#troubleshooting",
    "href": "task-system.html#troubleshooting",
    "title": "Task System & Commands",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\n\n\n\n\n\nCommon Issues\n\n\n\n\n“Please specify DIR”\nWhen creating projects, always specify the parent directory to avoid creating projects inside Nanobricks:\n# Good\ntask dist:project:create:uv NAME=my-app DIR=~/projects\n\n# Will fail\ntask dist:project:create:uv NAME=my-app\n\n\nVariable expansion in generated Taskfiles\nIf task link:nanobricks fails with empty variables in your project, regenerate the project with the latest Nanobricks version.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Task System & Commands"
    ]
  },
  {
    "objectID": "task-system.html#next-steps",
    "href": "task-system.html#next-steps",
    "title": "Task System & Commands",
    "section": "Next Steps",
    "text": "Next Steps\n\nReview the Quickstart Guide for a hands-on introduction\nExplore Multi-Project Development for advanced workflows\nSee Distribution Guide for packaging and deployment",
    "crumbs": [
      "Home",
      "Getting Started",
      "Task System & Commands"
    ]
  },
  {
    "objectID": "roadmap.html#introduction",
    "href": "roadmap.html#introduction",
    "title": "Nanobricks Roadmap",
    "section": "",
    "text": "This roadmap synthesizes feedback from real-world usage of nanobricks (particularly from the nano-scorm project) and outlines our path forward. The framework’s core abstractions have proven solid, but we need to reduce friction and provide clearer guidance for common patterns.",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#current-state-v0.1.0",
    "href": "roadmap.html#current-state-v0.1.0",
    "title": "Nanobricks Roadmap",
    "section": "Current State: v0.1.0",
    "text": "Current State: v0.1.0\n\nWhat’s Working Well\n\nCore Protocol: The NanobrickSimple[Input, Output] abstraction is intuitive and powerful\nSkills System: The @skill decorator pattern for cross-cutting concerns is a major hit\nType Safety: Generic typing provides excellent IDE support and catch errors early\nAsync-First: Modern design that works well with contemporary Python patterns\n\n\n\nKey Achievements\n\nSuccessfully used to build production-grade applications (nano-scorm)\n~55% code reduction compared to manual implementation\n70% reduction in API boilerplate through skills system\nProven composition patterns for complex workflows",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#immediate-priorities",
    "href": "roadmap.html#immediate-priorities",
    "title": "Nanobricks Roadmap",
    "section": "Immediate Priorities",
    "text": "Immediate Priorities\nBased on real-world feedback, we’re taking a pragmatic approach with patch releases for backwards-compatible improvements before major feature additions.",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#version-0.1.1-documentation-examples-january-2025",
    "href": "roadmap.html#version-0.1.1-documentation-examples-january-2025",
    "title": "Nanobricks Roadmap",
    "section": "Version 0.1.1: Documentation & Examples (January 2025)",
    "text": "Version 0.1.1: Documentation & Examples (January 2025)\nTheme: Zero code changes, maximum clarity\n\nPriority Documentation\n\nCookbook Examples: Step-by-step guides for common patterns\nDependency Injection Guide: Clear documentation on using the deps parameter\nError Handling Patterns: Best practices for exceptions vs Result types\nTesting Patterns: How to test individual bricks and pipelines\nArchitecture Decision Guide: When to use composition vs inheritance",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#version-0.1.2-type-improvements-january-2025",
    "href": "roadmap.html#version-0.1.2-type-improvements-january-2025",
    "title": "Nanobricks Roadmap",
    "section": "Version 0.1.2: Type Improvements (January 2025)",
    "text": "Version 0.1.2: Type Improvements (January 2025)\nTheme: Backwards-compatible enhancements to reduce friction\n\nType System Enhancements\n\nType Adapter Utilities: Helper functions to adapt between mismatched types for pipe operator\nGeneric Result Type: Optional Result[T, E] import for standardized error handling\nBetter Error Messages: Improved type mismatch errors for pipe operator\nType Conversion Helpers: Common adapters for string↔︎dict, list↔︎tuple, etc.",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#version-0.2.0-composition-revolution-february-2025",
    "href": "roadmap.html#version-0.2.0-composition-revolution-february-2025",
    "title": "Nanobricks Roadmap",
    "section": "Version 0.2.0: Composition Revolution (February 2025)",
    "text": "Version 0.2.0: Composition Revolution (February 2025)\nTheme: Address the #1 pain point - strict pipe operator type alignment\n\nCore Enhancements\n\nPipeline Builder API: Fluent interface for complex compositions\npipeline = (\n    Pipeline.start_with(Parser())\n    .branch(Validator(), Analyzer())\n    .merge_with(CombineResults())\n    .build()\n)\nParallel/Branch Support: Built-in support for fan-out/fan-in patterns\nComposition Debugger: Visualize pipeline flow and type mismatches\nFlexible Pipe Operator: More permissive type matching with runtime validation",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#version-0.2.1-standard-patterns-library-march-2025",
    "href": "roadmap.html#version-0.2.1-standard-patterns-library-march-2025",
    "title": "Nanobricks Roadmap",
    "section": "Version 0.2.1: Standard Patterns Library (March 2025)",
    "text": "Version 0.2.1: Standard Patterns Library (March 2025)\nTheme: Common bricks to accelerate development\n\nBatteries Included\n\nCommon Validators: Email, URL, phone, regex, range validators\nCommon Transformers: JSON adapters, type converters, normalizers\nFile Processing Bricks: CSV, JSON, XML, YAML readers/writers\nHTTP Client Brick: Basic requests wrapper with retry logic\nData Manipulation: Filter, map, reduce, aggregate bricks",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#version-0.3.0-developer-experience-q2-2025",
    "href": "roadmap.html#version-0.3.0-developer-experience-q2-2025",
    "title": "Nanobricks Roadmap",
    "section": "Version 0.3.0: Developer Experience (Q2 2025)",
    "text": "Version 0.3.0: Developer Experience (Q2 2025)\nTheme: Superior tooling and development workflow\n\nCLI Enhancements\n\nInteractive Scaffolding: nanobrick new --interactive\nProject Templates: Full application templates (API service, data processor, etc.)\nSkill Generation: nanobrick add-skill logging to retrofit existing bricks\n\n\n\nDevelopment Tools\n\nPipeline Visualizer: Generate diagrams from code\nPerformance Profiler: Identify bottlenecks in pipelines\nType Stub Generator: Better IDE support for dynamic features\nVS Code Extension: Snippets, navigation, and debugging support\n\n\n\nDependency Injection\n\nClear Patterns: Documentation and examples for the deps parameter\nService Registry: Built-in IoC container for shared resources\nConfiguration Injection: Standardized config management\nTesting Utilities: Mock injectors for unit tests",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#version-0.4.0-standard-library-q3-2025",
    "href": "roadmap.html#version-0.4.0-standard-library-q3-2025",
    "title": "Nanobricks Roadmap",
    "section": "Version 0.4.0: Standard Library (Q3 2025)",
    "text": "Version 0.4.0: Standard Library (Q3 2025)\nTheme: Batteries-included ecosystem\n\nCommon Bricks Library\n\nData Validation: Email, phone, URL, schema validators\nHTTP Operations: Request/response handling, retries, rate limiting\nDatabase Access: Connection pooling, query builders, ORMs\nFile Processing: CSV, JSON, XML, Excel readers/writers\nMessage Queues: Kafka, RabbitMQ, Redis pub/sub\nCaching: In-memory, Redis, distributed caches\n\n\n\nIntegration Patterns\n\nThird-party Libraries: Adapters for popular packages\nFramework Bridges: Integration with FastAPI, Django, Flask\nCloud Services: AWS, GCP, Azure service bricks",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#version-0.5.0-advanced-features-q4-2025",
    "href": "roadmap.html#version-0.5.0-advanced-features-q4-2025",
    "title": "Nanobricks Roadmap",
    "section": "Version 0.5.0: Advanced Features (Q4 2025)",
    "text": "Version 0.5.0: Advanced Features (Q4 2025)\nTheme: Production-scale capabilities\n\nStreaming Support\nclass StreamProcessor(NanobrickStream[bytes, ProcessedItem]):\n    async def invoke_stream(\n        self, \n        input_stream: AsyncIterator[bytes]\n    ) -&gt; AsyncIterator[ProcessedItem]:\n        async for chunk in input_stream:\n            yield await self.process_chunk(chunk)\n\n\nState Management\n\nStateful Bricks: Clear patterns for maintaining state\nState Persistence: Save/restore capabilities\nDistributed State: Coordination across instances\n\n\n\nResource Lifecycle\n\nSetup/Teardown Hooks: Manage expensive resources\nConnection Pooling: Built-in resource pools\nGraceful Shutdown: Proper cleanup on termination",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#version-1.0.0-production-ready-q1-2026",
    "href": "roadmap.html#version-1.0.0-production-ready-q1-2026",
    "title": "Nanobricks Roadmap",
    "section": "Version 1.0.0: Production Ready (Q1 2026)",
    "text": "Version 1.0.0: Production Ready (Q1 2026)\nTheme: Enterprise-grade stability\n\nPerformance Optimizations\n\nZero-Copy Operations: Efficient data passing\nParallel Execution: Automatic parallelization where possible\nMemory Management: Streaming and chunking for large datasets\nProfiling Tools: Built-in performance monitoring\n\n\n\nObservability\n\nDistributed Tracing: Full OpenTelemetry integration\nMetrics Collection: Prometheus-compatible metrics\nHealth Checks: Standardized health/readiness probes\nDebugging Tools: Enhanced error messages and stack traces\n\n\n\nEnterprise Features\n\nSecurity: Sandboxing, permission models, audit trails\nCompliance: GDPR, SOC2, HIPAA helpers\nMulti-tenancy: Isolation and resource limits\nHigh Availability: Clustering and failover support",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#future-vision-beyond-1.0",
    "href": "roadmap.html#future-vision-beyond-1.0",
    "title": "Nanobricks Roadmap",
    "section": "Future Vision (Beyond 1.0)",
    "text": "Future Vision (Beyond 1.0)\n\nAI-Native Framework\n\nLLM Integration: First-class support for AI models\nPrompt Management: Version and test prompts\nRAG Pipelines: Built-in retrieval-augmented generation\nAgent Frameworks: Multi-agent orchestration\n\n\n\nDistributed Computing\n\nCluster Mode: Scale across multiple machines\nWorkflow Engine: Complex DAG execution\nEvent Sourcing: Built-in event store\nCQRS Support: Command/query separation\n\n\n\nLanguage Expansion\n\nType Hints: Full runtime validation with beartype\nRust Extensions: Performance-critical components\nWebAssembly: Run bricks in browsers\nCross-Language: Polyglot brick composition",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#community-ecosystem",
    "href": "roadmap.html#community-ecosystem",
    "title": "Nanobricks Roadmap",
    "section": "Community & Ecosystem",
    "text": "Community & Ecosystem\n\nGovernance\n\nRFC Process: Community-driven feature development\nStability Guarantee: Clear deprecation policies\nSecurity Process: Vulnerability disclosure and fixes\nRelease Cadence: Predictable release schedule\n\n\n\nEcosystem Growth\n\nPlugin Registry: Discover and share bricks\nCertification Program: Quality standards for third-party bricks\nCorporate Sponsors: Sustainable development funding\nAcademic Partnerships: Research and education",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#migration-strategy",
    "href": "roadmap.html#migration-strategy",
    "title": "Nanobricks Roadmap",
    "section": "Migration Strategy",
    "text": "Migration Strategy\n\nVersion Compatibility\n\nSemantic Versioning: Strict adherence to SemVer\nDeprecation Warnings: Two minor versions before removal\nMigration Tools: Automated upgrades where possible\nCompatibility Layer: Support for older brick versions\n\n\n\nAdoption Path\n\nEarly Adopters (now): Feedback and iteration\nProduction Users (v0.3+): Stable core with good tooling\n\nEnterprise (v0.5+): Advanced features and support\nMainstream (v1.0+): Mature ecosystem",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks Roadmap"
    ]
  },
  {
    "objectID": "roadmap.html#getting-involved",
    "href": "roadmap.html#getting-involved",
    "title": "Nanobricks Roadmap",
    "section": "Getting Involved",
    "text": "Getting Involved\n\nGitHub Discussions: Share ideas and feedback\nRFC Process: Propose new features\nContributing: Code, documentation, examples\nSponsorship: Support sustainable development\n\n\nThis roadmap is a living document. We’ll update it based on community feedback and real-world usage patterns. Your input shapes the future of nanobricks!",
    "crumbs": [
      "Home",
      "Evolution",
      "Nanobricks Roadmap"
    ]
  },
  {
    "objectID": "cookbook/dependency-injection.html",
    "href": "cookbook/dependency-injection.html",
    "title": "Dependency Injection",
    "section": "",
    "text": "Your nanobricks need access to shared resources like: - Database connections - Configuration settings - External services (APIs, caches, message queues) - Loggers and monitoring tools - Authentication/authorization context\nYou want to inject these dependencies without: - Hard-coding them into bricks - Creating global variables - Breaking testability - Coupling bricks to specific implementations",
    "crumbs": [
      "Home",
      "Cookbook",
      "Dependency Injection"
    ]
  },
  {
    "objectID": "cookbook/dependency-injection.html#problem",
    "href": "cookbook/dependency-injection.html#problem",
    "title": "Dependency Injection",
    "section": "",
    "text": "Your nanobricks need access to shared resources like: - Database connections - Configuration settings - External services (APIs, caches, message queues) - Loggers and monitoring tools - Authentication/authorization context\nYou want to inject these dependencies without: - Hard-coding them into bricks - Creating global variables - Breaking testability - Coupling bricks to specific implementations",
    "crumbs": [
      "Home",
      "Cookbook",
      "Dependency Injection"
    ]
  },
  {
    "objectID": "cookbook/dependency-injection.html#solution",
    "href": "cookbook/dependency-injection.html#solution",
    "title": "Dependency Injection",
    "section": "Solution",
    "text": "Solution\nUse the deps parameter in the invoke method to pass dependencies:\nasync def invoke(self, input: T, *, deps: Optional[Dict[str, Any]] = None) -&gt; U:\n    db = deps.get(\"db\") if deps else None\n    config = deps.get(\"config\", default_config) if deps else default_config\n    # Use injected dependencies",
    "crumbs": [
      "Home",
      "Cookbook",
      "Dependency Injection"
    ]
  },
  {
    "objectID": "cookbook/dependency-injection.html#core-concepts",
    "href": "cookbook/dependency-injection.html#core-concepts",
    "title": "Dependency Injection",
    "section": "Core Concepts",
    "text": "Core Concepts\n\nThe deps Parameter\nEvery nanobrick’s invoke method accepts an optional deps parameter:\nclass MyBrick(NanobrickSimple[Input, Output]):\n    async def invoke(\n        self, \n        input: Input, \n        *, \n        deps: Optional[Dict[str, Any]] = None\n    ) -&gt; Output:\n        # Access dependencies from deps dict\n        pass\n\n\nDependency Flow in Pipelines\nWhen using the pipe operator, deps flow through all stages:\n# deps are automatically passed to each stage\npipeline = BrickA() | BrickB() | BrickC()\nresult = await pipeline.invoke(input, deps={\"db\": db_conn})",
    "crumbs": [
      "Home",
      "Cookbook",
      "Dependency Injection"
    ]
  },
  {
    "objectID": "cookbook/dependency-injection.html#common-patterns",
    "href": "cookbook/dependency-injection.html#common-patterns",
    "title": "Dependency Injection",
    "section": "Common Patterns",
    "text": "Common Patterns\n\nPattern 1: Database Connections\nclass DatabaseBrick(NanobrickSimple[Query, Results]):\n    async def invoke(self, query: Query, *, deps=None) -&gt; Results:\n        if not deps or \"db\" not in deps:\n            raise ValueError(\"Database connection required in deps['db']\")\n        \n        db = deps[\"db\"]\n        return await db.execute(query)\n\n# Usage\ndeps = {\"db\": database_connection}\nresults = await brick.invoke(query, deps=deps)\n\n\nPattern 2: Configuration Objects\n@dataclass\nclass Config:\n    api_key: str\n    timeout: int = 30\n    retry_count: int = 3\n\nclass APIBrick(NanobrickSimple[Request, Response]):\n    async def invoke(self, request: Request, *, deps=None) -&gt; Response:\n        config = Config()  # Default\n        if deps and \"config\" in deps:\n            config = deps[\"config\"]\n        \n        # Use configuration\n        return await make_api_call(\n            request, \n            api_key=config.api_key,\n            timeout=config.timeout\n        )\n\n\nPattern 3: Multiple Services\nclass NotificationBrick(NanobrickSimple[Message, bool]):\n    async def invoke(self, message: Message, *, deps=None) -&gt; bool:\n        if not deps:\n            raise ValueError(\"Dependencies required\")\n        \n        # Extract multiple services\n        email_service = deps.get(\"email_service\")\n        sms_service = deps.get(\"sms_service\")\n        logger = deps.get(\"logger\")\n        \n        # Use services as needed\n        if message.type == \"email\" and email_service:\n            result = await email_service.send(message)\n        elif message.type == \"sms\" and sms_service:\n            result = await sms_service.send(message)\n        else:\n            raise ValueError(f\"No service for {message.type}\")\n        \n        # Optional logging\n        if logger:\n            logger.info(f\"Sent {message.type} notification\")\n        \n        return result\n\n\nPattern 4: Environment-Specific Dependencies\ndef create_dependencies(environment: str) -&gt; Dict[str, Any]:\n    \"\"\"Create environment-specific dependencies.\"\"\"\n    if environment == \"production\":\n        return {\n            \"db\": ProductionDatabase(),\n            \"cache\": RedisCache(\"prod.redis.com\"),\n            \"config\": load_config(\"prod.yaml\")\n        }\n    elif environment == \"development\":\n        return {\n            \"db\": SQLiteDatabase(\":memory:\"),\n            \"cache\": InMemoryCache(),\n            \"config\": load_config(\"dev.yaml\")\n        }\n    else:\n        raise ValueError(f\"Unknown environment: {environment}\")\n\n# Use appropriate deps for environment\ndeps = create_dependencies(os.getenv(\"ENVIRONMENT\", \"development\"))\nresult = await pipeline.invoke(data, deps=deps)",
    "crumbs": [
      "Home",
      "Cookbook",
      "Dependency Injection"
    ]
  },
  {
    "objectID": "cookbook/dependency-injection.html#advanced-patterns",
    "href": "cookbook/dependency-injection.html#advanced-patterns",
    "title": "Dependency Injection",
    "section": "Advanced Patterns",
    "text": "Advanced Patterns\n\nDependency Container\nCreate a container class to manage complex dependencies:\nclass DependencyContainer:\n    \"\"\"Manages application dependencies with lazy initialization.\"\"\"\n    \n    def __init__(self, config_path: str):\n        self.config = load_config(config_path)\n        self._db = None\n        self._cache = None\n        self._services = {}\n    \n    @property\n    def db(self):\n        \"\"\"Lazy database connection.\"\"\"\n        if not self._db:\n            self._db = create_db_connection(self.config.database)\n        return self._db\n    \n    @property\n    def cache(self):\n        \"\"\"Lazy cache initialization.\"\"\"\n        if not self._cache:\n            self._cache = create_cache(self.config.cache)\n        return self._cache\n    \n    def get_service(self, name: str):\n        \"\"\"Get or create named service.\"\"\"\n        if name not in self._services:\n            self._services[name] = create_service(name, self.config)\n        return self._services[name]\n    \n    def as_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert to deps dictionary.\"\"\"\n        return {\n            \"db\": self.db,\n            \"cache\": self.cache,\n            \"config\": self.config,\n            \"container\": self  # Include self for dynamic service access\n        }\n\n# Usage\ncontainer = DependencyContainer(\"config/prod.yaml\")\nresult = await pipeline.invoke(data, deps=container.as_dict())\n\n\nDependency Injection in Pipelines\nWhen building complex pipelines, you might want different deps for different stages:\nclass SmartPipeline(NanobrickSimple[Input, Output]):\n    def __init__(self):\n        self.stage1 = Stage1Brick()\n        self.stage2 = Stage2Brick()\n        self.stage3 = Stage3Brick()\n    \n    async def invoke(self, input: Input, *, deps=None) -&gt; Output:\n        # Stage 1 needs database\n        stage1_deps = {\"db\": deps.get(\"db\")} if deps else None\n        result1 = await self.stage1.invoke(input, deps=stage1_deps)\n        \n        # Stage 2 needs cache and config\n        stage2_deps = {\n            \"cache\": deps.get(\"cache\"),\n            \"config\": deps.get(\"config\")\n        } if deps else None\n        result2 = await self.stage2.invoke(result1, deps=stage2_deps)\n        \n        # Stage 3 needs everything\n        result3 = await self.stage3.invoke(result2, deps=deps)\n        \n        return result3",
    "crumbs": [
      "Home",
      "Cookbook",
      "Dependency Injection"
    ]
  },
  {
    "objectID": "cookbook/dependency-injection.html#testing-with-dependencies",
    "href": "cookbook/dependency-injection.html#testing-with-dependencies",
    "title": "Dependency Injection",
    "section": "Testing with Dependencies",
    "text": "Testing with Dependencies\nDependency injection makes testing much easier:\n# test_my_brick.py\nimport pytest\nfrom unittest.mock import AsyncMock, Mock\n\nclass TestDatabaseBrick:\n    @pytest.mark.asyncio\n    async def test_with_mock_database(self):\n        # Create mock dependencies\n        mock_db = AsyncMock()\n        mock_db.execute.return_value = [{\"id\": 1, \"name\": \"Test\"}]\n        \n        mock_logger = Mock()\n        \n        deps = {\n            \"db\": mock_db,\n            \"logger\": mock_logger\n        }\n        \n        # Test the brick\n        brick = DatabaseBrick()\n        result = await brick.invoke(\"SELECT * FROM users\", deps=deps)\n        \n        # Verify behavior\n        assert len(result) == 1\n        mock_db.execute.assert_called_once_with(\"SELECT * FROM users\")\n        \n    @pytest.mark.asyncio\n    async def test_missing_dependencies(self):\n        brick = DatabaseBrick()\n        \n        # Should raise when deps missing\n        with pytest.raises(ValueError, match=\"Database connection required\"):\n            await brick.invoke(\"SELECT *\", deps={})",
    "crumbs": [
      "Home",
      "Cookbook",
      "Dependency Injection"
    ]
  },
  {
    "objectID": "cookbook/dependency-injection.html#best-practices",
    "href": "cookbook/dependency-injection.html#best-practices",
    "title": "Dependency Injection",
    "section": "Best Practices",
    "text": "Best Practices\n\n1. Document Required Dependencies\nAlways document what dependencies your brick expects:\nclass MyBrick(NanobrickSimple[Input, Output]):\n    \"\"\"\n    Processes data using external services.\n    \n    Required dependencies:\n        - db: Database connection\n        - config: Configuration object\n        \n    Optional dependencies:\n        - logger: Logger instance\n        - cache: Cache service\n    \"\"\"\n\n\n2. Provide Sensible Defaults\nWhen possible, provide defaults for optional dependencies:\nasync def invoke(self, input: Input, *, deps=None) -&gt; Output:\n    # Required dependency - fail fast\n    if not deps or \"db\" not in deps:\n        raise ValueError(\"db is required in deps\")\n    \n    # Optional with default\n    config = deps.get(\"config\", self.default_config) if deps else self.default_config\n    logger = deps.get(\"logger\", null_logger) if deps else null_logger\n\n\n3. Type Your Dependencies\nConsider using TypedDict for better type safety:\nfrom typing import TypedDict, Optional\n\nclass BrickDeps(TypedDict, total=False):\n    db: DatabaseConnection\n    config: Config\n    logger: Logger\n    cache: Optional[Cache]\n\nclass TypedBrick(NanobrickSimple[Input, Output]):\n    async def invoke(self, input: Input, *, deps: Optional[BrickDeps] = None) -&gt; Output:\n        # Now you get type hints for deps!\n        if deps:\n            db = deps.get(\"db\")  # Type: Optional[DatabaseConnection]\n\n\n4. Avoid Over-Injection\nDon’t inject things that should be: - Constructor parameters (brick configuration) - Part of the input data - Internal implementation details\n# Good - injecting shared resource\ndeps = {\"db\": database_connection}\n\n# Bad - this should be input data\ndeps = {\"user_id\": 123}  # Should be part of input\n\n# Bad - this should be constructor param\ndeps = {\"batch_size\": 100}  # Should be MyBrick(batch_size=100)",
    "crumbs": [
      "Home",
      "Cookbook",
      "Dependency Injection"
    ]
  },
  {
    "objectID": "cookbook/dependency-injection.html#common-pitfalls",
    "href": "cookbook/dependency-injection.html#common-pitfalls",
    "title": "Dependency Injection",
    "section": "Common Pitfalls",
    "text": "Common Pitfalls\n\nModifying Deps\nDon’t modify the deps dictionary:\n# Bad - modifies shared deps\nasync def invoke(self, input: Input, *, deps=None) -&gt; Output:\n    if deps:\n        deps[\"processed\"] = True  # Don't do this!\n    \n# Good - create new dict if needed\nasync def invoke(self, input: Input, *, deps=None) -&gt; Output:\n    my_deps = dict(deps) if deps else {}\n    my_deps[\"processed\"] = True  # Safe\n\n\nTight Coupling\nAvoid coupling to specific implementations:\n# Bad - coupled to specific class\nif isinstance(deps.get(\"db\"), PostgresDatabase):\n    # PostgreSQL-specific code\n\n# Good - depend on interface/protocol\ndb = deps.get(\"db\")\nif hasattr(db, \"execute\"):\n    # Works with any database that has execute method",
    "crumbs": [
      "Home",
      "Cookbook",
      "Dependency Injection"
    ]
  },
  {
    "objectID": "cookbook/dependency-injection.html#see-also",
    "href": "cookbook/dependency-injection.html#see-also",
    "title": "Dependency Injection",
    "section": "See Also",
    "text": "See Also\n\nTesting Bricks - Testing with mock dependencies\nBasic Pipeline - How deps flow through pipelines\nError Handling - Handling missing dependencies\nComposition Patterns - Advanced dependency patterns",
    "crumbs": [
      "Home",
      "Cookbook",
      "Dependency Injection"
    ]
  },
  {
    "objectID": "cookbook/index.html",
    "href": "cookbook/index.html",
    "title": "Nanobricks Cookbook",
    "section": "",
    "text": "Welcome to the Nanobricks Cookbook! This collection of practical examples and patterns will help you get the most out of the framework. Each recipe addresses a specific use case or pattern that emerged from real-world usage.",
    "crumbs": [
      "Home",
      "Cookbook",
      "Nanobricks Cookbook"
    ]
  },
  {
    "objectID": "cookbook/index.html#introduction",
    "href": "cookbook/index.html#introduction",
    "title": "Nanobricks Cookbook",
    "section": "",
    "text": "Welcome to the Nanobricks Cookbook! This collection of practical examples and patterns will help you get the most out of the framework. Each recipe addresses a specific use case or pattern that emerged from real-world usage.",
    "crumbs": [
      "Home",
      "Cookbook",
      "Nanobricks Cookbook"
    ]
  },
  {
    "objectID": "cookbook/index.html#quick-navigation",
    "href": "cookbook/index.html#quick-navigation",
    "title": "Nanobricks Cookbook",
    "section": "Quick Navigation",
    "text": "Quick Navigation\n\n🚀 Getting Started\n\nBasic Pipeline - Your first nanobrick pipeline\nManual Composition - When the pipe operator isn’t enough\n\n\n\n🔧 Core Patterns\n\nDependency Injection - Using the deps parameter effectively\nError Handling - Exceptions vs validation results\nTesting Bricks - Unit and integration testing strategies\n\n\n\n🎯 Advanced Patterns\n\nBranching Pipelines - Parallel and conditional flows\nSkill Patterns - Creating and applying skills\nComposition vs Inheritance - Architecture decisions",
    "crumbs": [
      "Home",
      "Cookbook",
      "Nanobricks Cookbook"
    ]
  },
  {
    "objectID": "cookbook/index.html#how-to-use-this-cookbook",
    "href": "cookbook/index.html#how-to-use-this-cookbook",
    "title": "Nanobricks Cookbook",
    "section": "How to Use This Cookbook",
    "text": "How to Use This Cookbook\nEach recipe follows a consistent format:\n\nProblem: What challenge are we solving?\nSolution: The recommended approach\nExample: Complete, runnable code\nDiscussion: Why this approach works\nVariations: Alternative patterns\nSee Also: Related recipes",
    "crumbs": [
      "Home",
      "Cookbook",
      "Nanobricks Cookbook"
    ]
  },
  {
    "objectID": "cookbook/index.html#code-examples",
    "href": "cookbook/index.html#code-examples",
    "title": "Nanobricks Cookbook",
    "section": "Code Examples",
    "text": "Code Examples\nAll examples in this cookbook are: - ✅ Tested with the current version of nanobricks - ✅ Type-checked with mypy - ✅ Available in the examples/cookbook/ directory - ✅ Designed to be copy-paste ready",
    "crumbs": [
      "Home",
      "Cookbook",
      "Nanobricks Cookbook"
    ]
  },
  {
    "objectID": "cookbook/index.html#common-questions",
    "href": "cookbook/index.html#common-questions",
    "title": "Nanobricks Cookbook",
    "section": "Common Questions",
    "text": "Common Questions\n\n“When should I use the pipe operator vs manual composition?”\nThe pipe operator (|) is great when: - Types align perfectly between stages - You want a clean, linear flow - Each stage has a single input/output\nUse manual composition when: - You need to branch or merge data flows - Types don’t align naturally - You need complex error handling - You want more control over the flow\n\n\n“How do I handle errors in my pipelines?”\nWe recommend two patterns: 1. Let exceptions bubble - Good for unexpected errors 2. Return validation results - Good for expected failures\nSee the Error Handling recipe for details.\n\n\n“How do I test my nanobricks?”\nTesting strategies depend on your brick’s complexity: - Unit tests for individual bricks - Integration tests for pipelines - Mock dependencies for isolated testing\nSee the Testing Bricks recipe for patterns.",
    "crumbs": [
      "Home",
      "Cookbook",
      "Nanobricks Cookbook"
    ]
  },
  {
    "objectID": "cookbook/index.html#contributing",
    "href": "cookbook/index.html#contributing",
    "title": "Nanobricks Cookbook",
    "section": "Contributing",
    "text": "Contributing\nFound a pattern that others might benefit from? We welcome cookbook contributions!\n\nCreate an example in examples/cookbook/\nDocument it in docs/quarto/cookbook/\nEnsure it follows our format\nSubmit a pull request",
    "crumbs": [
      "Home",
      "Cookbook",
      "Nanobricks Cookbook"
    ]
  },
  {
    "objectID": "cookbook/index.html#need-help",
    "href": "cookbook/index.html#need-help",
    "title": "Nanobricks Cookbook",
    "section": "Need Help?",
    "text": "Need Help?\n\n📚 Check the SDK Guide for comprehensive documentation\n💬 Ask questions in GitHub Discussions\n🐛 Report issues on GitHub\n🚀 See the Roadmap for upcoming features\n\n\nHappy cooking with nanobricks! 🧱",
    "crumbs": [
      "Home",
      "Cookbook",
      "Nanobricks Cookbook"
    ]
  },
  {
    "objectID": "cookbook/basic-pipeline.html",
    "href": "cookbook/basic-pipeline.html",
    "title": "Basic Pipeline",
    "section": "",
    "text": "You want to process data through multiple stages, where each stage transforms the data in a specific way. For example, cleaning text, tokenizing it, and then analyzing it.",
    "crumbs": [
      "Home",
      "Cookbook",
      "Basic Pipeline"
    ]
  },
  {
    "objectID": "cookbook/basic-pipeline.html#problem",
    "href": "cookbook/basic-pipeline.html#problem",
    "title": "Basic Pipeline",
    "section": "",
    "text": "You want to process data through multiple stages, where each stage transforms the data in a specific way. For example, cleaning text, tokenizing it, and then analyzing it.",
    "crumbs": [
      "Home",
      "Cookbook",
      "Basic Pipeline"
    ]
  },
  {
    "objectID": "cookbook/basic-pipeline.html#solution",
    "href": "cookbook/basic-pipeline.html#solution",
    "title": "Basic Pipeline",
    "section": "Solution",
    "text": "Solution\nUse the pipe operator (|) to compose nanobricks when the output type of one stage matches the input type of the next stage.\npipeline = TextCleaner() | WordTokenizer() | WordCounter()\nresult = await pipeline.invoke(input_text)",
    "crumbs": [
      "Home",
      "Cookbook",
      "Basic Pipeline"
    ]
  },
  {
    "objectID": "cookbook/basic-pipeline.html#complete-example",
    "href": "cookbook/basic-pipeline.html#complete-example",
    "title": "Basic Pipeline",
    "section": "Complete Example",
    "text": "Complete Example\nLet’s build a text analysis pipeline that: 1. Cleans messy text 2. Splits it into words 3. Counts word frequencies\nimport asyncio\nfrom typing import List, Dict\nfrom nanobricks import NanobrickSimple\n\n\nclass TextCleaner(NanobrickSimple[str, str]):\n    \"\"\"Removes extra whitespace and normalizes text.\"\"\"\n    \n    name = \"text_cleaner\"\n    version = \"1.0.0\"\n    \n    async def invoke(self, input: str, *, deps=None) -&gt; str:\n        return \" \".join(input.split())\n\n\nclass WordTokenizer(NanobrickSimple[str, List[str]]):\n    \"\"\"Splits text into individual words.\"\"\"\n    \n    name = \"word_tokenizer\"\n    version = \"1.0.0\"\n    \n    async def invoke(self, input: str, *, deps=None) -&gt; List[str]:\n        return input.lower().split()\n\n\nclass WordCounter(NanobrickSimple[List[str], Dict[str, int]]):\n    \"\"\"Counts word frequencies.\"\"\"\n    \n    name = \"word_counter\"\n    version = \"1.0.0\"\n    \n    async def invoke(self, input: List[str], *, deps=None) -&gt; Dict[str, int]:\n        word_freq = {}\n        for word in input:\n            word_freq[word] = word_freq.get(word, 0) + 1\n        return word_freq\n\n\n# Compose the pipeline\npipeline = TextCleaner() | WordTokenizer() | WordCounter()\n\n# Use it\nasync def analyze_text(text: str):\n    return await pipeline.invoke(text)",
    "crumbs": [
      "Home",
      "Cookbook",
      "Basic Pipeline"
    ]
  },
  {
    "objectID": "cookbook/basic-pipeline.html#how-it-works",
    "href": "cookbook/basic-pipeline.html#how-it-works",
    "title": "Basic Pipeline",
    "section": "How It Works",
    "text": "How It Works\n\nType Flow\nThe pipe operator works because the types align perfectly:\nTextCleaner:    str → str\nWordTokenizer:  str → List[str]\nWordCounter:    List[str] → Dict[str, int]\n\nPipeline:       str → Dict[str, int]\n\n\nBehind the Scenes\nWhen you use |, nanobricks: 1. Creates a composite brick that chains the invoke calls 2. Passes the output of each stage as input to the next 3. Maintains async compatibility throughout 4. Combines metadata (names, versions) from all stages",
    "crumbs": [
      "Home",
      "Cookbook",
      "Basic Pipeline"
    ]
  },
  {
    "objectID": "cookbook/basic-pipeline.html#common-patterns",
    "href": "cookbook/basic-pipeline.html#common-patterns",
    "title": "Basic Pipeline",
    "section": "Common Patterns",
    "text": "Common Patterns\n\nPattern 1: Linear Processing\nPerfect for straightforward transformations:\n# Data processing pipeline\npipeline = (\n    CSVParser() |      # str → List[Dict]\n    DataValidator() |  # List[Dict] → List[Dict]\n    DataTransformer()  # List[Dict] → DataFrame\n)\n\n\nPattern 2: Reusable Pipelines\nCreate factory functions for common pipelines:\ndef create_etl_pipeline(config: dict):\n    return (\n        Extractor(config[\"source\"]) |\n        Transformer(config[\"rules\"]) |\n        Loader(config[\"destination\"])\n    )\n\n# Use with different configurations\nsales_pipeline = create_etl_pipeline(sales_config)\ninventory_pipeline = create_etl_pipeline(inventory_config)\n\n\nPattern 3: Partial Pipelines\nBuild pipelines incrementally:\n# Start with cleaning\nbase_pipeline = TextCleaner() | TextNormalizer()\n\n# Add analysis for one use case\nsentiment_pipeline = base_pipeline | SentimentAnalyzer()\n\n# Add different analysis for another\nsummary_pipeline = base_pipeline | TextSummarizer()",
    "crumbs": [
      "Home",
      "Cookbook",
      "Basic Pipeline"
    ]
  },
  {
    "objectID": "cookbook/basic-pipeline.html#when-to-use-this-pattern",
    "href": "cookbook/basic-pipeline.html#when-to-use-this-pattern",
    "title": "Basic Pipeline",
    "section": "When to Use This Pattern",
    "text": "When to Use This Pattern\n✅ Use pipe operator when: - Types naturally align between stages - You want clean, readable code - The flow is linear (no branching) - Each stage has single input/output\n❌ Don’t use when: - You need complex branching logic - Types don’t align without conversion - You need error recovery between stages - You need access to intermediate results",
    "crumbs": [
      "Home",
      "Cookbook",
      "Basic Pipeline"
    ]
  },
  {
    "objectID": "cookbook/basic-pipeline.html#troubleshooting",
    "href": "cookbook/basic-pipeline.html#troubleshooting",
    "title": "Basic Pipeline",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nType Mismatch Errors\nIf you get type errors when composing:\n# This won't work - types don't align\nbad_pipeline = TextCleaner() | WordCounter()  # str → str | List[str] → Dict\nSolution: Add an adapter or use manual composition (see Manual Composition)\n\n\nAccessing Intermediate Results\nIf you need intermediate values:\n# Instead of a pipeline, use individual bricks\ncleaner = TextCleaner()\ntokenizer = WordTokenizer()\n\ncleaned = await cleaner.invoke(raw_text)\nprint(f\"After cleaning: {cleaned}\")  # Intermediate result\n\ntokens = await tokenizer.invoke(cleaned)\nprint(f\"Tokens: {tokens}\")  # Another intermediate",
    "crumbs": [
      "Home",
      "Cookbook",
      "Basic Pipeline"
    ]
  },
  {
    "objectID": "cookbook/basic-pipeline.html#performance-considerations",
    "href": "cookbook/basic-pipeline.html#performance-considerations",
    "title": "Basic Pipeline",
    "section": "Performance Considerations",
    "text": "Performance Considerations\nThe pipe operator has minimal overhead: - Each stage is called sequentially (no unnecessary parallelism) - No intermediate storage unless needed - Async operations are properly chained\nFor parallel processing, see Branching Pipelines.",
    "crumbs": [
      "Home",
      "Cookbook",
      "Basic Pipeline"
    ]
  },
  {
    "objectID": "cookbook/basic-pipeline.html#try-it-yourself",
    "href": "cookbook/basic-pipeline.html#try-it-yourself",
    "title": "Basic Pipeline",
    "section": "Try It Yourself",
    "text": "Try It Yourself\n\nRun the example: python examples/cookbook/01_simple_pipeline.py\nModify the pipeline to add a stage that filters stop words\nCreate a pipeline for your own use case",
    "crumbs": [
      "Home",
      "Cookbook",
      "Basic Pipeline"
    ]
  },
  {
    "objectID": "cookbook/basic-pipeline.html#see-also",
    "href": "cookbook/basic-pipeline.html#see-also",
    "title": "Basic Pipeline",
    "section": "See Also",
    "text": "See Also\n\nManual Composition - When types don’t align\nError Handling - Adding error recovery\nTesting Bricks - Testing your pipelines\nBranching Pipelines - Non-linear flows",
    "crumbs": [
      "Home",
      "Cookbook",
      "Basic Pipeline"
    ]
  },
  {
    "objectID": "cookbook/error-handling.html",
    "href": "cookbook/error-handling.html",
    "title": "Error Handling Patterns",
    "section": "",
    "text": "Your nanobricks need to handle various error scenarios: - Invalid input data - Missing dependencies - External service failures - Business rule violations - Partial failures in batch processing\nYou need to decide between: - Raising exceptions (fail fast) - Returning error results (explicit handling) - Logging and continuing - Providing fallback behaviors"
  },
  {
    "objectID": "cookbook/error-handling.html#problem",
    "href": "cookbook/error-handling.html#problem",
    "title": "Error Handling Patterns",
    "section": "",
    "text": "Your nanobricks need to handle various error scenarios: - Invalid input data - Missing dependencies - External service failures - Business rule violations - Partial failures in batch processing\nYou need to decide between: - Raising exceptions (fail fast) - Returning error results (explicit handling) - Logging and continuing - Providing fallback behaviors"
  },
  {
    "objectID": "cookbook/error-handling.html#solution",
    "href": "cookbook/error-handling.html#solution",
    "title": "Error Handling Patterns",
    "section": "Solution",
    "text": "Solution\nChoose your error handling strategy based on the error type:\n\nExceptions for unexpected/unrecoverable errors\nResult types for expected/business errors\nFallbacks for resilient systems\nError boundaries for pipeline isolation"
  },
  {
    "objectID": "cookbook/error-handling.html#core-patterns",
    "href": "cookbook/error-handling.html#core-patterns",
    "title": "Error Handling Patterns",
    "section": "Core Patterns",
    "text": "Core Patterns\n\nPattern 1: Exception-Based (Fail Fast)\nBest for unexpected errors that should stop processing:\nclass StrictValidator(NanobrickSimple[Data, ValidData]):\n    async def invoke(self, data: Data, *, deps=None) -&gt; ValidData:\n        if not data.is_valid():\n            raise ValueError(f\"Invalid data: {data.errors}\")\n        \n        if data.size &gt; self.max_size:\n            raise ValueError(f\"Data too large: {data.size}\")\n        \n        return ValidData(data)\nWhen to use: - Data integrity violations - Programming errors - Missing required dependencies - Constraint violations\n\n\nPattern 2: Result-Based (Explicit Handling)\nBest for expected errors that callers should handle:\n@dataclass\nclass Result:\n    success: bool\n    data: Optional[Any] = None\n    error: Optional[str] = None\n    \nclass SafeProcessor(NanobrickSimple[Input, Result]):\n    async def invoke(self, input: Input, *, deps=None) -&gt; Result:\n        if not self.validate(input):\n            return Result(\n                success=False, \n                error=\"Validation failed\"\n            )\n        \n        try:\n            data = self.process(input)\n            return Result(success=True, data=data)\n        except ProcessingError as e:\n            return Result(success=False, error=str(e))\nWhen to use: - User input validation - Business rule failures - Partial batch processing - External API errors\n\n\nPattern 3: Typed Result Pattern\nMore sophisticated result types with type safety:\nfrom typing import Generic, TypeVar, Union\n\nT = TypeVar('T')\nE = TypeVar('E')\n\n@dataclass\nclass Ok(Generic[T]):\n    value: T\n\n@dataclass\nclass Err(Generic[E]):\n    error: E\n\nResult = Union[Ok[T], Err[E]]\n\nclass TypedProcessor(NanobrickSimple[str, Result[ProcessedData, str]]):\n    async def invoke(self, input: str, *, deps=None) -&gt; Result[ProcessedData, str]:\n        if not input:\n            return Err(\"Input cannot be empty\")\n        \n        try:\n            data = ProcessedData(input)\n            return Ok(data)\n        except Exception as e:\n            return Err(f\"Processing failed: {e}\")\n\n# Usage with pattern matching (Python 3.10+)\nresult = await processor.invoke(\"data\")\nmatch result:\n    case Ok(value):\n        print(f\"Success: {value}\")\n    case Err(error):\n        print(f\"Failed: {error}\")"
  },
  {
    "objectID": "cookbook/error-handling.html#advanced-patterns",
    "href": "cookbook/error-handling.html#advanced-patterns",
    "title": "Error Handling Patterns",
    "section": "Advanced Patterns",
    "text": "Advanced Patterns\n\nError Recovery with Fallbacks\nBuild resilient systems that try multiple approaches:\nclass ResilientFetcher(NanobrickSimple[Query, Data]):\n    def __init__(self):\n        self.primary_source = PrimaryAPI()\n        self.cache = CacheService()\n        self.fallback_source = SecondaryAPI()\n    \n    async def invoke(self, query: Query, *, deps=None) -&gt; Data:\n        logger = deps.get(\"logger\") if deps else None\n        \n        # Try primary source\n        try:\n            return await self.primary_source.fetch(query)\n        except Exception as e:\n            if logger:\n                logger.warning(f\"Primary source failed: {e}\")\n        \n        # Try cache\n        try:\n            cached = await self.cache.get(query.key)\n            if cached and not cached.is_stale():\n                return cached.data\n        except Exception as e:\n            if logger:\n                logger.warning(f\"Cache failed: {e}\")\n        \n        # Last resort - fallback source\n        try:\n            return await self.fallback_source.fetch(query)\n        except Exception as e:\n            # All options exhausted\n            raise ServiceUnavailable(\n                \"All data sources failed\",\n                attempts=[\n                    \"primary_api\", \n                    \"cache\", \n                    \"secondary_api\"\n                ]\n            )\n\n\nError Boundaries in Pipelines\nIsolate failures to prevent cascade:\nclass ErrorBoundary(NanobrickSimple[T, Union[T, ErrorInfo]]):\n    \"\"\"Wraps a brick to catch and handle its errors.\"\"\"\n    \n    def __init__(self, brick: NanobrickSimple[T, U], on_error=None):\n        self.brick = brick\n        self.on_error = on_error or self.default_error_handler\n    \n    async def invoke(self, input: T, *, deps=None) -&gt; Union[U, ErrorInfo]:\n        try:\n            return await self.brick.invoke(input, deps=deps)\n        except Exception as e:\n            return self.on_error(e, input, deps)\n    \n    def default_error_handler(self, error, input, deps):\n        return ErrorInfo(\n            error_type=type(error).__name__,\n            message=str(error),\n            input_data=str(input)[:100]  # Truncate for safety\n        )\n\n# Usage\npipeline = (\n    Parser() |\n    ErrorBoundary(RiskyTransformer()) |  # Isolate risky operation\n    Validator()\n)\n\n\nBatch Processing with Partial Failures\nHandle errors in batch operations:\n@dataclass\nclass BatchResult:\n    successful: List[ProcessedItem]\n    failed: List[FailedItem]\n    stats: Dict[str, int]\n\nclass BatchProcessor(NanobrickSimple[List[Item], BatchResult]):\n    async def invoke(self, items: List[Item], *, deps=None) -&gt; BatchResult:\n        successful = []\n        failed = []\n        \n        for item in items:\n            try:\n                processed = await self.process_item(item)\n                successful.append(processed)\n            except ValidationError as e:\n                failed.append(FailedItem(\n                    item=item,\n                    error=\"Validation failed\",\n                    details=str(e)\n                ))\n            except ProcessingError as e:\n                failed.append(FailedItem(\n                    item=item,\n                    error=\"Processing failed\",\n                    details=str(e),\n                    recoverable=True\n                ))\n            except Exception as e:\n                # Unexpected error - might want to stop\n                if self.fail_fast:\n                    raise\n                failed.append(FailedItem(\n                    item=item,\n                    error=\"Unexpected error\",\n                    details=str(e),\n                    recoverable=False\n                ))\n        \n        return BatchResult(\n            successful=successful,\n            failed=failed,\n            stats={\n                \"total\": len(items),\n                \"succeeded\": len(successful),\n                \"failed\": len(failed),\n                \"success_rate\": len(successful) / len(items)\n            }\n        )"
  },
  {
    "objectID": "cookbook/error-handling.html#custom-exception-types",
    "href": "cookbook/error-handling.html#custom-exception-types",
    "title": "Error Handling Patterns",
    "section": "Custom Exception Types",
    "text": "Custom Exception Types\nDefine domain-specific exceptions for better error handling:\nclass BrickException(Exception):\n    \"\"\"Base exception for all brick errors.\"\"\"\n    pass\n\nclass ValidationException(BrickException):\n    \"\"\"Input validation failed.\"\"\"\n    def __init__(self, message: str, field: str = None, value: Any = None):\n        super().__init__(message)\n        self.field = field\n        self.value = value\n\nclass DependencyException(BrickException):\n    \"\"\"Required dependency missing.\"\"\"\n    def __init__(self, missing: List[str]):\n        super().__init__(f\"Missing dependencies: {', '.join(missing)}\")\n        self.missing = missing\n\nclass ProcessingException(BrickException):\n    \"\"\"Processing logic failed.\"\"\"\n    def __init__(self, message: str, recoverable: bool = False):\n        super().__init__(message)\n        self.recoverable = recoverable\n\n# Usage\nclass StrictBrick(NanobrickSimple[Input, Output]):\n    async def invoke(self, input: Input, *, deps=None) -&gt; Output:\n        # Check dependencies\n        required_deps = [\"db\", \"cache\"]\n        missing = [d for d in required_deps if not deps or d not in deps]\n        if missing:\n            raise DependencyException(missing)\n        \n        # Validate input\n        if not input.id:\n            raise ValidationException(\n                \"ID is required\",\n                field=\"id\",\n                value=input.id\n            )\n        \n        # Process\n        try:\n            return self.process(input)\n        except RecoverableError as e:\n            raise ProcessingException(str(e), recoverable=True)\n        except FatalError as e:\n            raise ProcessingException(str(e), recoverable=False)"
  },
  {
    "objectID": "cookbook/error-handling.html#error-handling-in-pipelines",
    "href": "cookbook/error-handling.html#error-handling-in-pipelines",
    "title": "Error Handling Patterns",
    "section": "Error Handling in Pipelines",
    "text": "Error Handling in Pipelines\n\nSequential Error Propagation\nDefault behavior - first error stops the pipeline:\n# Any error stops the pipeline\npipeline = Parser() | Validator() | Processor()\n\ntry:\n    result = await pipeline.invoke(data)\nexcept ValidationException as e:\n    # Handle validation errors\n    logger.error(f\"Validation failed: {e}\")\nexcept Exception as e:\n    # Handle other errors\n    logger.error(f\"Pipeline failed: {e}\")\n\n\nParallel Error Collection\nCollect errors from parallel operations:\nclass ParallelProcessor(NanobrickSimple[Data, Results]):\n    async def invoke(self, data: Data, *, deps=None) -&gt; Results:\n        # Run operations in parallel\n        tasks = [\n            self.process_a(data),\n            self.process_b(data),\n            self.process_c(data)\n        ]\n        \n        results = []\n        errors = []\n        \n        # Gather results, catching exceptions\n        outcomes = await asyncio.gather(*tasks, return_exceptions=True)\n        \n        for outcome in outcomes:\n            if isinstance(outcome, Exception):\n                errors.append(outcome)\n            else:\n                results.append(outcome)\n        \n        if errors and self.all_must_succeed:\n            raise MultipleErrors(errors)\n        \n        return Results(\n            data=results,\n            errors=errors,\n            partial_success=len(errors) &gt; 0\n        )"
  },
  {
    "objectID": "cookbook/error-handling.html#best-practices",
    "href": "cookbook/error-handling.html#best-practices",
    "title": "Error Handling Patterns",
    "section": "Best Practices",
    "text": "Best Practices\n\n1. Be Consistent\nChoose a primary error handling strategy and stick to it:\n# If using exceptions, use them consistently\nclass ExceptionBased(NanobrickSimple[A, B]):\n    async def invoke(self, input: A, *, deps=None) -&gt; B:\n        if not valid(input):\n            raise ValidationException(\"Invalid input\")\n        return process(input)\n\n# If using results, use them consistently  \nclass ResultBased(NanobrickSimple[A, Result[B]]):\n    async def invoke(self, input: A, *, deps=None) -&gt; Result[B]:\n        if not valid(input):\n            return Result(success=False, error=\"Invalid input\")\n        return Result(success=True, data=process(input))\n\n\n2. Document Error Behavior\nAlways document what errors your brick can raise:\nclass DocumentedBrick(NanobrickSimple[Input, Output]):\n    \"\"\"\n    Processes input data.\n    \n    Raises:\n        ValidationException: If input is invalid\n        DependencyException: If required deps missing\n        ProcessingException: If processing fails\n    \n    Returns:\n        Processed output data\n    \"\"\"\n\n\n3. Provide Context\nInclude helpful context in error messages:\n# Bad\nraise ValueError(\"Invalid data\")\n\n# Good\nraise ValueError(\n    f\"Invalid data: expected positive integer, got {value} (type: {type(value)})\"\n)\n\n# Better\nraise ValidationException(\n    message=\"Value must be a positive integer\",\n    field=\"user_id\",\n    value=value,\n    expected_type=int,\n    constraints={\"min\": 1}\n)\n\n\n4. Handle Async Errors\nRemember to handle async-specific errors:\nclass AsyncAwareBrick(NanobrickSimple[Input, Output]):\n    async def invoke(self, input: Input, *, deps=None) -&gt; Output:\n        try:\n            return await self.async_operation(input)\n        except asyncio.TimeoutError:\n            raise ProcessingException(\"Operation timed out\", recoverable=True)\n        except asyncio.CancelledError:\n            # Clean up and re-raise\n            await self.cleanup()\n            raise"
  },
  {
    "objectID": "cookbook/error-handling.html#testing-error-handling",
    "href": "cookbook/error-handling.html#testing-error-handling",
    "title": "Error Handling Patterns",
    "section": "Testing Error Handling",
    "text": "Testing Error Handling\nWrite tests for both success and failure cases:\nimport pytest\n\nclass TestErrorHandling:\n    @pytest.mark.asyncio\n    async def test_validation_error(self):\n        brick = StrictValidator()\n        \n        with pytest.raises(ValidationException) as exc_info:\n            await brick.invoke(invalid_data)\n        \n        assert \"Invalid data\" in str(exc_info.value)\n        assert exc_info.value.field == \"expected_field\"\n    \n    @pytest.mark.asyncio\n    async def test_result_based_error(self):\n        brick = SafeProcessor()\n        \n        result = await brick.invoke(invalid_data)\n        \n        assert not result.success\n        assert result.error == \"Validation failed\"\n        assert result.data is None\n    \n    @pytest.mark.asyncio  \n    async def test_error_recovery(self):\n        brick = ResilientProcessor()\n        \n        # Mock primary failure\n        brick.primary.fail = True\n        \n        # Should still succeed via fallback\n        result = await brick.invoke(query)\n        assert result is not None"
  },
  {
    "objectID": "cookbook/error-handling.html#see-also",
    "href": "cookbook/error-handling.html#see-also",
    "title": "Error Handling Patterns",
    "section": "See Also",
    "text": "See Also\n\nTesting Bricks - Testing error scenarios\nDependency Injection - Handling missing dependencies\nBasic Pipeline - Error propagation in pipelines\nBranching Pipelines - Parallel error handling"
  },
  {
    "objectID": "cookbook/testing-bricks.html",
    "href": "cookbook/testing-bricks.html",
    "title": "Testing Patterns",
    "section": "",
    "text": "You need to test your nanobricks to ensure they: - Handle inputs correctly - Manage dependencies properly - Propagate errors as expected - Work correctly in pipelines - Perform well under load\nTesting async code and dependency injection adds complexity that requires specific patterns.",
    "crumbs": [
      "Home",
      "Cookbook",
      "Testing Patterns"
    ]
  },
  {
    "objectID": "cookbook/testing-bricks.html#problem",
    "href": "cookbook/testing-bricks.html#problem",
    "title": "Testing Patterns",
    "section": "",
    "text": "You need to test your nanobricks to ensure they: - Handle inputs correctly - Manage dependencies properly - Propagate errors as expected - Work correctly in pipelines - Perform well under load\nTesting async code and dependency injection adds complexity that requires specific patterns.",
    "crumbs": [
      "Home",
      "Cookbook",
      "Testing Patterns"
    ]
  },
  {
    "objectID": "cookbook/testing-bricks.html#solution",
    "href": "cookbook/testing-bricks.html#solution",
    "title": "Testing Patterns",
    "section": "Solution",
    "text": "Solution\nUse pytest with async support and strategic mocking:\nimport pytest\nfrom unittest.mock import Mock, AsyncMock\n\nclass TestMyBrick:\n    @pytest.mark.asyncio\n    async def test_basic_functionality(self):\n        brick = MyBrick()\n        result = await brick.invoke(input_data)\n        assert result == expected_output",
    "crumbs": [
      "Home",
      "Cookbook",
      "Testing Patterns"
    ]
  },
  {
    "objectID": "cookbook/testing-bricks.html#basic-testing-patterns",
    "href": "cookbook/testing-bricks.html#basic-testing-patterns",
    "title": "Testing Patterns",
    "section": "Basic Testing Patterns",
    "text": "Basic Testing Patterns\n\nTesting Individual Bricks\nStart with simple unit tests for each brick:\nclass TestTextProcessor:\n    @pytest.fixture\n    def brick(self):\n        \"\"\"Create a fresh brick instance for each test.\"\"\"\n        return TextProcessor()\n    \n    @pytest.mark.asyncio\n    async def test_basic_processing(self, brick):\n        result = await brick.invoke(\"hello world\")\n        assert result == \"HELLO WORLD\"\n    \n    @pytest.mark.asyncio\n    async def test_error_handling(self, brick):\n        with pytest.raises(ValueError, match=\"Text cannot be empty\"):\n            await brick.invoke(\"\")\n\n\nTesting with Dependencies\nMock dependencies to test in isolation:\nclass TestDatabaseBrick:\n    @pytest.fixture\n    def mock_db(self):\n        \"\"\"Create a mock database.\"\"\"\n        db = AsyncMock()\n        db.execute.return_value = [{\"id\": 1, \"name\": \"Test\"}]\n        return db\n    \n    @pytest.mark.asyncio\n    async def test_with_database(self, mock_db):\n        brick = DatabaseBrick()\n        deps = {\"db\": mock_db}\n        \n        result = await brick.invoke(\"SELECT * FROM users\", deps=deps)\n        \n        assert len(result) == 1\n        mock_db.execute.assert_called_once_with(\"SELECT * FROM users\")\n\n\nTesting Pipelines\nTest composed pipelines end-to-end:\nclass TestPipeline:\n    @pytest.fixture\n    def pipeline(self):\n        return Parser() | Validator() | Transformer()\n    \n    @pytest.mark.asyncio\n    async def test_full_pipeline(self, pipeline):\n        input_data = \"raw input\"\n        result = await pipeline.invoke(input_data)\n        \n        assert result.processed == True\n        assert result.valid == True",
    "crumbs": [
      "Home",
      "Cookbook",
      "Testing Patterns"
    ]
  },
  {
    "objectID": "cookbook/testing-bricks.html#advanced-testing-patterns",
    "href": "cookbook/testing-bricks.html#advanced-testing-patterns",
    "title": "Testing Patterns",
    "section": "Advanced Testing Patterns",
    "text": "Advanced Testing Patterns\n\nParametrized Tests\nTest multiple scenarios efficiently:\n@pytest.mark.parametrize(\"input,expected\", [\n    (\"hello\", \"HELLO\"),\n    (\"Hello World\", \"HELLO WORLD\"),\n    (\"  spaces  \", \"SPACES\"),\n    (\"123\", \"123\"),\n])\n@pytest.mark.asyncio\nasync def test_various_inputs(input, expected):\n    brick = TextProcessor()\n    result = await brick.invoke(input)\n    assert result == expected\n\n\nTesting Error Scenarios\nEnsure errors are handled correctly:\nclass TestErrorHandling:\n    @pytest.mark.asyncio\n    async def test_missing_dependencies(self):\n        brick = RequiresDependencies()\n        \n        # Test missing deps\n        with pytest.raises(ValueError, match=\"Database required\"):\n            await brick.invoke(\"query\")\n    \n    @pytest.mark.asyncio\n    async def test_partial_dependencies(self):\n        brick = RequiresDependencies()\n        deps = {\"logger\": Mock()}  # Missing required 'db'\n        \n        with pytest.raises(ValueError):\n            await brick.invoke(\"query\", deps=deps)\n\n\nTesting Async Behavior\nTest concurrent execution and timeouts:\nclass TestAsyncBehavior:\n    @pytest.mark.asyncio\n    async def test_concurrent_execution(self):\n        brick = ProcessorBrick()\n        \n        # Run multiple invocations concurrently\n        tasks = [\n            brick.invoke(f\"input-{i}\")\n            for i in range(10)\n        ]\n        \n        results = await asyncio.gather(*tasks)\n        assert len(results) == 10\n        assert all(r.startswith(\"PROCESSED\") for r in results)\n    \n    @pytest.mark.asyncio\n    async def test_timeout_handling(self):\n        brick = SlowBrick()\n        \n        with pytest.raises(asyncio.TimeoutError):\n            await asyncio.wait_for(\n                brick.invoke(\"input\"),\n                timeout=0.1\n            )",
    "crumbs": [
      "Home",
      "Cookbook",
      "Testing Patterns"
    ]
  },
  {
    "objectID": "cookbook/testing-bricks.html#mocking-strategies",
    "href": "cookbook/testing-bricks.html#mocking-strategies",
    "title": "Testing Patterns",
    "section": "Mocking Strategies",
    "text": "Mocking Strategies\n\nComplete Mock Replacement\nReplace entire bricks for testing:\n@pytest.mark.asyncio\nasync def test_with_mock_brick():\n    # Create a mock brick\n    mock_brick = AsyncMock()\n    mock_brick.invoke.return_value = {\"mocked\": True}\n    \n    # Use in pipeline\n    pipeline = RealBrick() | mock_brick | AnotherRealBrick()\n    \n    result = await pipeline.invoke(\"input\")\n    mock_brick.invoke.assert_called_once()\n\n\nPartial Mocking\nMock specific methods while keeping others:\n@pytest.mark.asyncio\nasync def test_partial_mock():\n    brick = ComplexBrick()\n    \n    # Mock only the external call\n    with patch.object(brick, '_call_external_api') as mock_api:\n        mock_api.return_value = {\"status\": \"ok\"}\n        \n        result = await brick.invoke(\"input\")\n        assert result[\"processed\"] == True\n        mock_api.assert_called_once()\n\n\nSpy Pattern\nMonitor calls without changing behavior:\n@pytest.mark.asyncio\nasync def test_spy_on_brick():\n    brick = MyBrick()\n    original_invoke = brick.invoke\n    \n    calls = []\n    async def spy_invoke(*args, **kwargs):\n        calls.append((args, kwargs))\n        return await original_invoke(*args, **kwargs)\n    \n    brick.invoke = spy_invoke\n    \n    # Use the brick\n    await brick.invoke(\"test\", deps={\"x\": 1})\n    \n    # Verify calls\n    assert len(calls) == 1\n    assert calls[0][0][0] == \"test\"\n    assert calls[0][1][\"deps\"][\"x\"] == 1",
    "crumbs": [
      "Home",
      "Cookbook",
      "Testing Patterns"
    ]
  },
  {
    "objectID": "cookbook/testing-bricks.html#test-fixtures",
    "href": "cookbook/testing-bricks.html#test-fixtures",
    "title": "Testing Patterns",
    "section": "Test Fixtures",
    "text": "Test Fixtures\n\nReusable Fixtures\nCreate fixtures for common test needs:\n@pytest.fixture\ndef mock_dependencies():\n    \"\"\"Standard mock dependencies.\"\"\"\n    return {\n        \"logger\": Mock(spec=Logger),\n        \"db\": AsyncMock(spec=Database),\n        \"cache\": AsyncMock(spec=Cache),\n        \"config\": {\"debug\": True, \"timeout\": 30}\n    }\n\n@pytest.fixture\nasync def real_dependencies():\n    \"\"\"Real dependencies for integration tests.\"\"\"\n    db = await create_test_database()\n    cache = InMemoryCache()\n    \n    yield {\n        \"db\": db,\n        \"cache\": cache,\n        \"logger\": TestLogger()\n    }\n    \n    # Cleanup\n    await db.close()\n    cache.clear()\n\n\nFixture Composition\nBuild complex fixtures from simpler ones:\n@pytest.fixture\ndef base_config():\n    return {\"environment\": \"test\"}\n\n@pytest.fixture\ndef db_config(base_config):\n    return {**base_config, \"db_url\": \"sqlite:///:memory:\"}\n\n@pytest.fixture\nasync def configured_brick(db_config, mock_dependencies):\n    brick = ConfigurableBrick(db_config)\n    await brick.initialize()\n    yield brick\n    await brick.cleanup()",
    "crumbs": [
      "Home",
      "Cookbook",
      "Testing Patterns"
    ]
  },
  {
    "objectID": "cookbook/testing-bricks.html#integration-testing",
    "href": "cookbook/testing-bricks.html#integration-testing",
    "title": "Testing Patterns",
    "section": "Integration Testing",
    "text": "Integration Testing\n\nTesting Complete Workflows\nTest realistic scenarios:\nclass TestIntegration:\n    @pytest.mark.asyncio\n    async def test_complete_workflow(self, mock_dependencies):\n        # Setup\n        parser = Parser()\n        validator = Validator()\n        processor = Processor()\n        storage = Storage()\n        \n        # Execute workflow\n        raw_data = '{\"user\": \"test\", \"action\": \"create\"}'\n        \n        parsed = await parser.invoke(raw_data, deps=mock_dependencies)\n        validated = await validator.invoke(parsed, deps=mock_dependencies)\n        processed = await processor.invoke(validated, deps=mock_dependencies)\n        stored = await storage.invoke(processed, deps=mock_dependencies)\n        \n        # Verify\n        assert stored[\"success\"] == True\n        assert mock_dependencies[\"db\"].save.called\n\n\nTesting with Real Dependencies\nSometimes you need real dependencies:\n@pytest.mark.integration\nclass TestWithRealDependencies:\n    @pytest.mark.asyncio\n    async def test_real_database_operations(self, real_dependencies):\n        brick = DatabaseBrick()\n        \n        # Insert test data\n        await brick.invoke(\n            \"INSERT INTO test VALUES (1, 'test')\",\n            deps=real_dependencies\n        )\n        \n        # Query and verify\n        results = await brick.invoke(\n            \"SELECT * FROM test\",\n            deps=real_dependencies\n        )\n        \n        assert len(results) == 1\n        assert results[0][\"id\"] == 1",
    "crumbs": [
      "Home",
      "Cookbook",
      "Testing Patterns"
    ]
  },
  {
    "objectID": "cookbook/testing-bricks.html#performance-testing",
    "href": "cookbook/testing-bricks.html#performance-testing",
    "title": "Testing Patterns",
    "section": "Performance Testing",
    "text": "Performance Testing\n\nBasic Performance Tests\nEnsure bricks meet performance requirements:\nimport time\n\nclass TestPerformance:\n    @pytest.mark.asyncio\n    async def test_processing_speed(self):\n        brick = FastProcessor()\n        input_data = \"x\" * 1000  # 1KB of data\n        \n        start = time.time()\n        result = await brick.invoke(input_data)\n        duration = time.time() - start\n        \n        assert duration &lt; 0.1  # Should process in under 100ms\n    \n    @pytest.mark.asyncio\n    async def test_throughput(self):\n        brick = BatchProcessor()\n        items = [f\"item-{i}\" for i in range(1000)]\n        \n        start = time.time()\n        results = await brick.invoke(items)\n        duration = time.time() - start\n        \n        throughput = len(items) / duration\n        assert throughput &gt; 100  # Should process &gt;100 items/second\n\n\nLoad Testing\nTest behavior under load:\n@pytest.mark.load\nclass TestUnderLoad:\n    @pytest.mark.asyncio\n    async def test_concurrent_load(self):\n        brick = ThreadSafeBrick()\n        \n        # Create many concurrent requests\n        async def make_request(i):\n            return await brick.invoke(f\"request-{i}\")\n        \n        tasks = [make_request(i) for i in range(100)]\n        results = await asyncio.gather(*tasks)\n        \n        # All should succeed\n        assert len(results) == 100\n        assert all(r[\"success\"] for r in results)",
    "crumbs": [
      "Home",
      "Cookbook",
      "Testing Patterns"
    ]
  },
  {
    "objectID": "cookbook/testing-bricks.html#test-organization",
    "href": "cookbook/testing-bricks.html#test-organization",
    "title": "Testing Patterns",
    "section": "Test Organization",
    "text": "Test Organization\n\nDirectory Structure\nOrganize tests to mirror your source:\ntests/\n├── unit/\n│   ├── test_transformers.py\n│   ├── test_validators.py\n│   └── test_processors.py\n├── integration/\n│   ├── test_pipelines.py\n│   └── test_workflows.py\n├── fixtures/\n│   ├── __init__.py\n│   └── common.py\n└── conftest.py\n\n\nShared Test Utilities\nCreate utilities for common testing needs:\n# tests/utils.py\nasync def create_test_brick(brick_class, **config):\n    \"\"\"Factory for creating configured test bricks.\"\"\"\n    brick = brick_class(**config)\n    await brick.initialize()\n    return brick\n\ndef assert_valid_result(result):\n    \"\"\"Common assertions for results.\"\"\"\n    assert result is not None\n    assert \"error\" not in result\n    assert result.get(\"success\", False)\n\nclass BrickTestCase:\n    \"\"\"Base class for brick tests.\"\"\"\n    \n    @pytest.fixture\n    async def brick(self):\n        \"\"\"Override in subclasses.\"\"\"\n        raise NotImplementedError\n    \n    @pytest.mark.asyncio\n    async def test_has_required_attributes(self, brick):\n        assert hasattr(brick, \"name\")\n        assert hasattr(brick, \"version\")\n        assert hasattr(brick, \"invoke\")",
    "crumbs": [
      "Home",
      "Cookbook",
      "Testing Patterns"
    ]
  },
  {
    "objectID": "cookbook/testing-bricks.html#best-practices",
    "href": "cookbook/testing-bricks.html#best-practices",
    "title": "Testing Patterns",
    "section": "Best Practices",
    "text": "Best Practices\n\n1. Test Isolation\nEach test should be independent:\n# Good - isolated test\n@pytest.mark.asyncio\nasync def test_processing():\n    brick = MyBrick()  # Fresh instance\n    result = await brick.invoke(\"test\")\n    assert result == \"expected\"\n\n# Bad - shared state\nprocessor = MyBrick()  # Shared instance\n\n@pytest.mark.asyncio\nasync def test_one():\n    await processor.invoke(\"test1\")  # Might affect test_two\n\n@pytest.mark.asyncio\nasync def test_two():\n    await processor.invoke(\"test2\")  # Depends on test_one\n\n\n2. Clear Test Names\nUse descriptive test names:\n# Good - describes what is being tested\nasync def test_empty_input_raises_validation_error()\nasync def test_concurrent_requests_are_handled_safely()\nasync def test_database_connection_timeout_is_respected()\n\n# Bad - unclear what is being tested\nasync def test_1()\nasync def test_error()\nasync def test_works()\n\n\n3. Arrange-Act-Assert\nStructure tests clearly:\n@pytest.mark.asyncio\nasync def test_user_creation():\n    # Arrange\n    brick = UserCreator()\n    user_data = {\"name\": \"Alice\", \"email\": \"alice@example.com\"}\n    mock_db = AsyncMock()\n    deps = {\"db\": mock_db}\n    \n    # Act\n    result = await brick.invoke(user_data, deps=deps)\n    \n    # Assert\n    assert result[\"created\"] == True\n    assert result[\"user_id\"] is not None\n    mock_db.insert.assert_called_once()\n\n\n4. Test Edge Cases\nDon’t just test the happy path:\n@pytest.mark.parametrize(\"input,should_fail\", [\n    (\"\", True),                    # Empty\n    (None, True),                  # None\n    (\"a\" * 1000, False),          # Very long\n    (\"unicode: 你好\", False),      # Unicode\n    (\"\\n\\t\\r\", True),             # Only whitespace\n    (\"valid input\", False),        # Normal case\n])\n@pytest.mark.asyncio\nasync def test_input_validation(input, should_fail):\n    brick = InputValidator()\n    \n    if should_fail:\n        with pytest.raises(ValueError):\n            await brick.invoke(input)\n    else:\n        result = await brick.invoke(input)\n        assert result is not None",
    "crumbs": [
      "Home",
      "Cookbook",
      "Testing Patterns"
    ]
  },
  {
    "objectID": "cookbook/testing-bricks.html#common-pitfalls",
    "href": "cookbook/testing-bricks.html#common-pitfalls",
    "title": "Testing Patterns",
    "section": "Common Pitfalls",
    "text": "Common Pitfalls\n\nForgetting await\nAlways await async calls in tests:\n# Wrong - test will pass even if brick fails!\n@pytest.mark.asyncio\nasync def test_wrong():\n    brick = MyBrick()\n    result = brick.invoke(\"test\")  # Missing await!\n    assert result == \"expected\"  # This compares a coroutine object\n\n# Correct\n@pytest.mark.asyncio\nasync def test_correct():\n    brick = MyBrick()\n    result = await brick.invoke(\"test\")\n    assert result == \"expected\"\n\n\nNot Cleaning Up\nAlways clean up resources:\n@pytest.fixture\nasync def database():\n    db = await create_database()\n    yield db\n    await db.close()  # Cleanup happens automatically\n\n# Or in the test\n@pytest.mark.asyncio\nasync def test_with_cleanup():\n    db = await create_database()\n    try:\n        # Test code\n        pass\n    finally:\n        await db.close()",
    "crumbs": [
      "Home",
      "Cookbook",
      "Testing Patterns"
    ]
  },
  {
    "objectID": "cookbook/testing-bricks.html#see-also",
    "href": "cookbook/testing-bricks.html#see-also",
    "title": "Testing Patterns",
    "section": "See Also",
    "text": "See Also\n\nError Handling - Testing error scenarios\nDependency Injection - Mocking dependencies\nBasic Pipeline - Testing composed pipelines\nexamples/cookbook/05_testing_example.py - Complete test examples",
    "crumbs": [
      "Home",
      "Cookbook",
      "Testing Patterns"
    ]
  },
  {
    "objectID": "cookbook/composition-patterns.html",
    "href": "cookbook/composition-patterns.html",
    "title": "Composition vs Inheritance",
    "section": "",
    "text": "You need to decide how to structure your nanobricks: - When should you compose existing bricks? - When should you inherit from a base brick? - When should you create a new brick from scratch? - How do you balance reusability with clarity?\nThese decisions significantly impact maintainability, testability, and flexibility.",
    "crumbs": [
      "Home",
      "Cookbook",
      "Composition vs Inheritance"
    ]
  },
  {
    "objectID": "cookbook/composition-patterns.html#problem",
    "href": "cookbook/composition-patterns.html#problem",
    "title": "Composition vs Inheritance",
    "section": "",
    "text": "You need to decide how to structure your nanobricks: - When should you compose existing bricks? - When should you inherit from a base brick? - When should you create a new brick from scratch? - How do you balance reusability with clarity?\nThese decisions significantly impact maintainability, testability, and flexibility.",
    "crumbs": [
      "Home",
      "Cookbook",
      "Composition vs Inheritance"
    ]
  },
  {
    "objectID": "cookbook/composition-patterns.html#solution",
    "href": "cookbook/composition-patterns.html#solution",
    "title": "Composition vs Inheritance",
    "section": "Solution",
    "text": "Solution\nFollow these guidelines:\n\nPrefer composition for combining functionality\nUse inheritance for specializing behavior\nCreate new bricks for atomic, reusable units\nMix approaches when it makes sense",
    "crumbs": [
      "Home",
      "Cookbook",
      "Composition vs Inheritance"
    ]
  },
  {
    "objectID": "cookbook/composition-patterns.html#decision-framework",
    "href": "cookbook/composition-patterns.html#decision-framework",
    "title": "Composition vs Inheritance",
    "section": "Decision Framework",
    "text": "Decision Framework\n\nWhen to Use Composition\nComposition is best when you’re combining distinct functionalities:\n# Good use of composition - combining distinct steps\nclass DataPipeline(NanobrickSimple[RawData, Report]):\n    def __init__(self):\n        self.validator = DataValidator()\n        self.transformer = DataTransformer()\n        self.analyzer = DataAnalyzer()\n        self.reporter = ReportGenerator()\n    \n    async def invoke(self, data: RawData, *, deps=None) -&gt; Report:\n        validated = await self.validator.invoke(data, deps=deps)\n        transformed = await self.transformer.invoke(validated, deps=deps)\n        analysis = await self.analyzer.invoke(transformed, deps=deps)\n        return await self.reporter.invoke(analysis, deps=deps)\nUse composition when: - ✅ Combining multiple distinct operations - ✅ Steps can be reused independently - ✅ You need flexibility to swap implementations - ✅ The workflow might change\n\n\nWhen to Use Inheritance\nInheritance is best when you’re specializing existing behavior:\n# Good use of inheritance - specializing behavior\nclass StrictValidator(Validator):\n    \"\"\"A validator with stricter rules than the base.\"\"\"\n    \n    async def invoke(self, data: Data, *, deps=None) -&gt; ValidationResult:\n        # First run parent validation\n        result = await super().invoke(data, deps=deps)\n        \n        if not result.is_valid:\n            return result\n        \n        # Add additional strict checks\n        if len(data.items) &lt; 10:\n            result.errors.append(\"Strict mode requires at least 10 items\")\n            result.is_valid = False\n        \n        return result\nUse inheritance when: - ✅ Extending existing behavior - ✅ Adding specialization to a general brick - ✅ Maintaining the same interface - ✅ Following Liskov Substitution Principle\n\n\nWhen to Use the Pipe Operator\nThe pipe operator is best for linear workflows:\n# Good use of pipe operator - linear flow with matching types\npipeline = (\n    CSVParser() |        # str -&gt; DataFrame\n    DataCleaner() |      # DataFrame -&gt; DataFrame\n    FeatureExtractor() | # DataFrame -&gt; Features\n    ModelPredictor()     # Features -&gt; Predictions\n)\nUse pipe operator when: - ✅ Types naturally align - ✅ Flow is strictly linear - ✅ No branching or conditions - ✅ Each stage has single input/output",
    "crumbs": [
      "Home",
      "Cookbook",
      "Composition vs Inheritance"
    ]
  },
  {
    "objectID": "cookbook/composition-patterns.html#common-patterns",
    "href": "cookbook/composition-patterns.html#common-patterns",
    "title": "Composition vs Inheritance",
    "section": "Common Patterns",
    "text": "Common Patterns\n\nPattern 1: Strategy Pattern with Composition\nUse composition to swap algorithms:\nclass DataProcessor(NanobrickSimple[Data, ProcessedData]):\n    def __init__(self, \n                 validator: Validator = None,\n                 transformer: Transformer = None):\n        self.validator = validator or DefaultValidator()\n        self.transformer = transformer or DefaultTransformer()\n    \n    async def invoke(self, data: Data, *, deps=None) -&gt; ProcessedData:\n        valid_data = await self.validator.invoke(data, deps=deps)\n        return await self.transformer.invoke(valid_data, deps=deps)\n\n# Different strategies\nproduction_processor = DataProcessor(\n    validator=StrictValidator(),\n    transformer=OptimizedTransformer()\n)\n\ndevelopment_processor = DataProcessor(\n    validator=LenientValidator(),\n    transformer=DebugTransformer()\n)\n\n\nPattern 2: Template Method with Inheritance\nUse inheritance for workflows with customization points:\nclass BaseETLPipeline(NanobrickSimple[Source, Result]):\n    \"\"\"Template for ETL pipelines.\"\"\"\n    \n    async def invoke(self, source: Source, *, deps=None) -&gt; Result:\n        # Template method defining the workflow\n        raw_data = await self.extract(source, deps)\n        transformed = await self.transform(raw_data, deps)\n        result = await self.load(transformed, deps)\n        await self.cleanup(deps)\n        return result\n    \n    async def extract(self, source: Source, deps) -&gt; RawData:\n        \"\"\"Override in subclasses.\"\"\"\n        raise NotImplementedError\n    \n    async def transform(self, data: RawData, deps) -&gt; TransformedData:\n        \"\"\"Override in subclasses.\"\"\"\n        raise NotImplementedError\n    \n    async def load(self, data: TransformedData, deps) -&gt; Result:\n        \"\"\"Override in subclasses.\"\"\"\n        raise NotImplementedError\n    \n    async def cleanup(self, deps):\n        \"\"\"Optional cleanup, override if needed.\"\"\"\n        pass\n\nclass SalesETLPipeline(BaseETLPipeline):\n    \"\"\"Specific implementation for sales data.\"\"\"\n    \n    async def extract(self, source: Source, deps) -&gt; RawData:\n        # Sales-specific extraction\n        return await fetch_sales_data(source)\n    \n    async def transform(self, data: RawData, deps) -&gt; TransformedData:\n        # Sales-specific transformations\n        return calculate_revenue_metrics(data)\n    \n    async def load(self, data: TransformedData, deps) -&gt; Result:\n        # Sales-specific loading\n        return await save_to_sales_warehouse(data)\n\n\nPattern 3: Decorator Pattern with Composition\nAdd functionality without modifying the original:\nclass LoggingBrick(NanobrickSimple[T, U]):\n    \"\"\"Adds logging to any brick.\"\"\"\n    \n    def __init__(self, brick: NanobrickSimple[T, U]):\n        self.brick = brick\n        self.name = f\"logged_{brick.name}\"\n        self.version = brick.version\n    \n    async def invoke(self, input: T, *, deps=None) -&gt; U:\n        logger = deps.get(\"logger\") if deps else print\n        \n        logger(f\"Starting {self.brick.name} with input type {type(input)}\")\n        start = time.time()\n        \n        try:\n            result = await self.brick.invoke(input, deps=deps)\n            logger(f\"Completed {self.brick.name} in {time.time()-start:.2f}s\")\n            return result\n        except Exception as e:\n            logger(f\"Error in {self.brick.name}: {e}\")\n            raise\n\n# Usage\nprocessor = DataProcessor()\nlogged_processor = LoggingBrick(processor)\n\n\nPattern 4: Mixin Pattern\nCombine inheritance with composition for shared functionality:\nclass CacheableMixin:\n    \"\"\"Mixin for adding caching capability.\"\"\"\n    \n    def __init__(self, *args, cache_ttl=300, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.cache_ttl = cache_ttl\n        self._cache = {}\n    \n    async def get_from_cache(self, key: str) -&gt; Optional[Any]:\n        if key in self._cache:\n            value, timestamp = self._cache[key]\n            if time.time() - timestamp &lt; self.cache_ttl:\n                return value\n        return None\n    \n    async def set_cache(self, key: str, value: Any):\n        self._cache[key] = (value, time.time())\n\nclass CachedProcessor(CacheableMixin, DataProcessor):\n    \"\"\"Data processor with caching.\"\"\"\n    \n    async def invoke(self, data: Data, *, deps=None) -&gt; ProcessedData:\n        # Check cache first\n        cache_key = hash(str(data))\n        cached = await self.get_from_cache(cache_key)\n        if cached:\n            return cached\n        \n        # Process if not cached\n        result = await super().invoke(data, deps=deps)\n        \n        # Cache the result\n        await self.set_cache(cache_key, result)\n        return result",
    "crumbs": [
      "Home",
      "Cookbook",
      "Composition vs Inheritance"
    ]
  },
  {
    "objectID": "cookbook/composition-patterns.html#decision-examples",
    "href": "cookbook/composition-patterns.html#decision-examples",
    "title": "Composition vs Inheritance",
    "section": "Decision Examples",
    "text": "Decision Examples\n\nExample 1: Building a Text Analysis System\n# Approach 1: Composition (Recommended)\nclass TextAnalysisPipeline(NanobrickSimple[str, AnalysisResult]):\n    \"\"\"Composes specialized bricks for text analysis.\"\"\"\n    \n    def __init__(self, language=\"en\"):\n        self.tokenizer = Tokenizer(language=language)\n        self.pos_tagger = POSTagger(language=language)\n        self.sentiment = SentimentAnalyzer()\n        self.summarizer = TextSummarizer()\n    \n    async def invoke(self, text: str, *, deps=None) -&gt; AnalysisResult:\n        tokens = await self.tokenizer.invoke(text, deps=deps)\n        tagged = await self.pos_tagger.invoke(tokens, deps=deps)\n        sentiment = await self.sentiment.invoke(text, deps=deps)\n        summary = await self.summarizer.invoke(text, deps=deps)\n        \n        return AnalysisResult(\n            tokens=tokens,\n            pos_tags=tagged,\n            sentiment=sentiment,\n            summary=summary\n        )\n\n# Approach 2: Inheritance (Not recommended for this case)\nclass TextAnalyzer(Tokenizer, POSTagger, SentimentAnalyzer):\n    # Multiple inheritance creates complexity\n    # Hard to understand which method comes from where\n    pass\n\n\nExample 2: Extending Validation\n# Approach 1: Inheritance (Recommended)\nclass EmailValidator(Validator):\n    \"\"\"Specializes validation for email addresses.\"\"\"\n    \n    async def invoke(self, email: str, *, deps=None) -&gt; ValidationResult:\n        # First run general validation\n        result = await super().invoke(email, deps=deps)\n        \n        # Add email-specific validation\n        if \"@\" not in email:\n            result.errors.append(\"Invalid email format\")\n        \n        return result\n\n# Approach 2: Composition (Over-engineered for this case)\nclass EmailValidator(NanobrickSimple[str, ValidationResult]):\n    def __init__(self):\n        self.base_validator = Validator()\n        self.email_checker = EmailFormatChecker()\n        self.domain_validator = DomainValidator()\n    # Too complex for a simple extension",
    "crumbs": [
      "Home",
      "Cookbook",
      "Composition vs Inheritance"
    ]
  },
  {
    "objectID": "cookbook/composition-patterns.html#anti-patterns-to-avoid",
    "href": "cookbook/composition-patterns.html#anti-patterns-to-avoid",
    "title": "Composition vs Inheritance",
    "section": "Anti-Patterns to Avoid",
    "text": "Anti-Patterns to Avoid\n\n1. Deep Inheritance Hierarchies\n# Bad - too many levels\nclass Processor(NanobrickBase): pass\nclass DataProcessor(Processor): pass\nclass StructuredDataProcessor(DataProcessor): pass\nclass JSONProcessor(StructuredDataProcessor): pass\nclass SecureJSONProcessor(JSONProcessor): pass\n\n# Good - flat hierarchy with composition\nclass JSONProcessor(NanobrickSimple[str, dict]):\n    def __init__(self, secure=False):\n        self.parser = JSONParser()\n        self.validator = SecureValidator() if secure else Validator()\n\n\n2. God Bricks\n# Bad - doing too much\nclass DoEverythingBrick(NanobrickSimple[Any, Any]):\n    async def invoke(self, input: Any, *, deps=None) -&gt; Any:\n        # 500 lines of code doing 10 different things\n        pass\n\n# Good - single responsibility\nclass Parser(NanobrickSimple[str, Data]): pass\nclass Validator(NanobrickSimple[Data, ValidData]): pass\nclass Processor(NanobrickSimple[ValidData, Result]): pass\n\n\n3. Inheritance for Code Reuse Only\n# Bad - inheriting just to reuse code\nclass ReportGenerator(DatabaseConnection):\n    # DatabaseConnection has connection pooling we want\n    # But ReportGenerator is not a DatabaseConnection!\n\n# Good - composition for code reuse\nclass ReportGenerator(NanobrickSimple[Query, Report]):\n    def __init__(self):\n        self.db = DatabaseConnection()  # Use, don't inherit",
    "crumbs": [
      "Home",
      "Cookbook",
      "Composition vs Inheritance"
    ]
  },
  {
    "objectID": "cookbook/composition-patterns.html#guidelines-summary",
    "href": "cookbook/composition-patterns.html#guidelines-summary",
    "title": "Composition vs Inheritance",
    "section": "Guidelines Summary",
    "text": "Guidelines Summary\n\nChoose Composition When:\n\nCombining multiple operations\nBuilding flexible pipelines\nOperations are independent\nYou need runtime flexibility\nFollowing “has-a” relationships\n\n\n\nChoose Inheritance When:\n\nSpecializing existing behavior\nExtending with minimal changes\nMaintaining interface compatibility\nFollowing “is-a” relationships\nCreating framework extension points\n\n\n\nChoose Pipe Operator When:\n\nBuilding linear pipelines\nTypes align naturally\nNo branching logic needed\nCreating reusable workflows\n\n\n\nMix Approaches When:\n\nDifferent parts have different needs\nOptimizing for both flexibility and simplicity\nBuilding complex systems with clear boundaries",
    "crumbs": [
      "Home",
      "Cookbook",
      "Composition vs Inheritance"
    ]
  },
  {
    "objectID": "cookbook/composition-patterns.html#testing-considerations",
    "href": "cookbook/composition-patterns.html#testing-considerations",
    "title": "Composition vs Inheritance",
    "section": "Testing Considerations",
    "text": "Testing Considerations\n\nTesting Composed Bricks\nclass TestComposedPipeline:\n    @pytest.mark.asyncio\n    async def test_with_mocked_components(self):\n        # Mock individual components\n        mock_validator = AsyncMock()\n        mock_processor = AsyncMock()\n        \n        pipeline = DataPipeline()\n        pipeline.validator = mock_validator\n        pipeline.processor = mock_processor\n        \n        # Test the composition logic\n        await pipeline.invoke(test_data)\n        \n        mock_validator.invoke.assert_called_once()\n        mock_processor.invoke.assert_called_once()\n\n\nTesting Inherited Bricks\nclass TestInheritedBrick:\n    @pytest.mark.asyncio\n    async def test_calls_parent_method(self):\n        brick = StrictValidator()\n        \n        # Spy on parent method\n        with patch.object(Validator, 'invoke', new_callable=AsyncMock) as mock_parent:\n            mock_parent.return_value = ValidationResult(is_valid=True)\n            \n            result = await brick.invoke(test_data)\n            \n            # Verify parent was called\n            mock_parent.assert_called_once()",
    "crumbs": [
      "Home",
      "Cookbook",
      "Composition vs Inheritance"
    ]
  },
  {
    "objectID": "cookbook/composition-patterns.html#see-also",
    "href": "cookbook/composition-patterns.html#see-also",
    "title": "Composition vs Inheritance",
    "section": "See Also",
    "text": "See Also\n\nBasic Pipeline - Using the pipe operator\nTesting Bricks - Testing strategies\nDependency Injection - Managing dependencies\nSDK Guide - Overall architecture patterns",
    "crumbs": [
      "Home",
      "Cookbook",
      "Composition vs Inheritance"
    ]
  }
]